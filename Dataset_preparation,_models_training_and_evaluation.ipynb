{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **5-HT1A-LiCAs: An Interpretable Machine Learning-based Tool for the Identification of Serotonin 5-HT1A Receptor Agonists and Antagonists**\n",
        "\n",
        "Sumbul Asif , Sara Sarfaraz *, Saba Zafar , Iqra Riaz , Saeed Abo Saeed , Mohamed E. Hasan , Katarzyna Wi≈Ñska  and Antoni Szumny *"
      ],
      "metadata": {
        "id": "ck4SabSwOw45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Calulation of Morgan Fingerprints uding RDKit:**"
      ],
      "metadata": {
        "id": "Qd-JLmhhHZ0b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset file required: Combined Dataset-Primary.xlsx"
      ],
      "metadata": {
        "id": "FdFWneQYMohT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCY2sndjHKE1"
      },
      "outputs": [],
      "source": [
        "!pip install rdkit\n",
        "from rdkit import Chem\n",
        "!pip install openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "\n",
        "# Read Excel file\n",
        "df = pd.read_excel('/content/Combined Dataset-Primary.xlsx')\n",
        "\n",
        "# Initialize Morgan fingerprint generator\n",
        "fpgen = AllChem.GetMorganGenerator(radius=2)\n",
        "\n",
        "print(\"Generating Morgan fingerprints...\")\n",
        "\n",
        "results = []\n",
        "failed_drugs = []\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    smiles = row['SMILE CODE']\n",
        "    mol = Chem.MolFromSmiles(str(smiles))\n",
        "\n",
        "    if mol is not None:\n",
        "        try:\n",
        "            fp = fpgen.GetFingerprint(mol)\n",
        "            fp_list = list(fp)\n",
        "            # Add all original row data plus fingerprints\n",
        "            new_row = row.to_dict()\n",
        "            for i, bit in enumerate(fp_list):\n",
        "                new_row[f'fp_bit_{i}'] = bit\n",
        "            results.append(new_row)\n",
        "        except Exception as e:\n",
        "            # If fingerprint generation fails after molecule creation\n",
        "            failed_drugs.append({\n",
        "                'index': idx,\n",
        "                'smiles': smiles,\n",
        "                'error': str(e)\n",
        "            })\n",
        "    else:\n",
        "        # If molecule creation fails\n",
        "        failed_drugs.append({\n",
        "            'index': idx,\n",
        "            'smiles': smiles,\n",
        "            'error': 'Invalid SMILES - cannot parse molecule'\n",
        "        })\n",
        "\n",
        "# Create new DataFrame with successful fingerprints\n",
        "if results:\n",
        "    result_df = pd.DataFrame(results)\n",
        "    result_df.to_excel('drugs_with_morgan_fingerprints.xlsx', index=False)\n",
        "    print(f\"‚úì Generated fingerprints for {len(result_df)} compounds\")\n",
        "    print(\"‚úì Saved to: drugs_with_morgan_fingerprints.xlsx\")\n",
        "else:\n",
        "    print(\"‚úó No valid fingerprints generated\")\n",
        "\n",
        "# Print failed drugs details\n",
        "if failed_drugs:\n",
        "    print(f\"\\n‚úó Fingerprint generation failed for {len(failed_drugs)} compounds:\")\n",
        "    print(\"=\" * 60)\n",
        "    for failed in failed_drugs:\n",
        "        print(f\"Row {failed['index'] + 2}: SMILES = '{failed['smiles']}' - {failed['error']}\")\n",
        "else:\n",
        "    print(\"‚úì All drugs successfully processed!\")\n",
        "\n",
        "print(f\"\\nSummary:\")\n",
        "print(f\"Total compounds processed: {len(df)}\")\n",
        "print(f\"Successful: {len(results)}\")\n",
        "print(f\"Failed: {len(failed_drugs)}\")"
      ],
      "metadata": {
        "id": "_yHa7sYAHu1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Calculation of 2D Descriptors using RDKit:**"
      ],
      "metadata": {
        "id": "LoX-gRLUHxXR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset required: Combined Dataset-Primary.xlsx"
      ],
      "metadata": {
        "id": "i1vd5w5PNI9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors, Lipinski, Crippen, MolSurf, rdMolDescriptors\n",
        "\n",
        "# Read Excel file\n",
        "df = pd.read_excel('/content/Combined Dataset-Primary.xlsx')\n",
        "\n",
        "print(\"Calculating ALL 2D molecular descriptors...\")\n",
        "\n",
        "successful_data = []\n",
        "failed_list = []\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    smiles = row['SMILE CODE']\n",
        "    mol = Chem.MolFromSmiles(str(smiles))\n",
        "\n",
        "    if mol is not None:\n",
        "        try:\n",
        "            desc_dict = {}\n",
        "\n",
        "            # Calculate ALL descriptors from RDKit's descriptor list\n",
        "            for desc_name, desc_func in Descriptors.descList:\n",
        "                try:\n",
        "                    desc_dict[desc_name] = desc_func(mol)\n",
        "                except:\n",
        "                    desc_dict[desc_name] = None\n",
        "\n",
        "            # Store original row data and descriptors\n",
        "            row_data = row.to_dict()\n",
        "            row_data.update(desc_dict)\n",
        "            successful_data.append(row_data)\n",
        "\n",
        "        except Exception as e:\n",
        "            failed_list.append((idx, smiles, str(e)))\n",
        "    else:\n",
        "        failed_list.append((idx, smiles, 'Invalid SMILES'))\n",
        "\n",
        "if successful_data:\n",
        "    # Create result DataFrame\n",
        "    result_df = pd.DataFrame(successful_data)\n",
        "\n",
        "    # Get the position of SMILE CODE column\n",
        "    original_cols = list(df.columns)\n",
        "    smile_idx = original_cols.index('SMILE CODE')\n",
        "\n",
        "    # Get descriptor columns (all columns not in original DataFrame)\n",
        "    desc_cols = [col for col in result_df.columns if col not in original_cols]\n",
        "\n",
        "    # Create new column order: original columns up to SMILE CODE, then descriptors, then remaining original columns\n",
        "    new_column_order = (original_cols[:smile_idx+1] +\n",
        "                       desc_cols +\n",
        "                       original_cols[smile_idx+1:])\n",
        "\n",
        "    # Reorder columns\n",
        "    result_df = result_df[new_column_order]\n",
        "\n",
        "    # Save to Excel\n",
        "    result_df.to_excel('drugs_with_all_2d_descriptors.xlsx', index=False)\n",
        "\n",
        "    print(f\"‚úì Generated {len(desc_cols)} 2D descriptors for {len(result_df)} compounds\")\n",
        "    print(\"‚úì Descriptors placed right after 'SMILE CODE' column\")\n",
        "    print(\"‚úì Saved to: drugs_with_all_2d_descriptors.xlsx\")\n",
        "\n",
        "    # Show descriptor count and names\n",
        "    print(f\"\\nTotal descriptors calculated: {len(desc_cols)}\")\n",
        "    print(f\"First 20 descriptors: {desc_cols[:20]}\")\n",
        "\n",
        "# Print failed compounds\n",
        "if failed_list:\n",
        "    print(f\"\\n‚úó Failed for {len(failed_list)} compounds:\")\n",
        "    for idx, smiles, error in failed_list:\n",
        "        print(f\"Row {idx+2}: '{smiles}' - {error}\")\n",
        "\n",
        "print(f\"\\nSummary: {len(successful_data)} successful, {len(failed_list)} failed\")"
      ],
      "metadata": {
        "id": "0FhiqYf_H5Sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Calculation of 3D Descriptors using RDKit:**"
      ],
      "metadata": {
        "id": "-Tm4J_3gIKEJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset required: Combined Dataset-primary.xlsx"
      ],
      "metadata": {
        "id": "N2UYqgxtNSSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem, rdMolDescriptors, Descriptors\n",
        "import numpy as np\n",
        "\n",
        "# Read Excel file\n",
        "df = pd.read_excel('/content/Combined Dataset-Primary.xlsx')\n",
        "\n",
        "print(\"Calculating 3D molecular descriptors...\")\n",
        "\n",
        "successful_data = []\n",
        "failed_list = []\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    smiles = row['SMILE CODE']\n",
        "    mol = Chem.MolFromSmiles(str(smiles))\n",
        "\n",
        "    if mol is not None:\n",
        "        try:\n",
        "            desc_dict = {}\n",
        "\n",
        "            # Generate 3D conformation\n",
        "            mol_3d = Chem.AddHs(mol)  # Add hydrogens for 3D\n",
        "            AllChem.EmbedMolecule(mol_3d, randomSeed=42)  # Generate 3D coordinates\n",
        "            AllChem.MMFFOptimizeMolecule(mol_3d)  # Energy minimization\n",
        "\n",
        "            # ===== 3D DESCRIPTORS =====\n",
        "\n",
        "            # PMI (Principal Moment of Inertia) descriptors\n",
        "            pmi1 = rdMolDescriptors.CalcPMI1(mol_3d)\n",
        "            pmi2 = rdMolDescriptors.CalcPMI2(mol_3d)\n",
        "            pmi3 = rdMolDescriptors.CalcPMI3(mol_3d)\n",
        "            desc_dict['PMI1'] = pmi1\n",
        "            desc_dict['PMI2'] = pmi2\n",
        "            desc_dict['PMI3'] = pmi3\n",
        "            desc_dict['NPR1'] = pmi1 / pmi3 if pmi3 != 0 else 0\n",
        "            desc_dict['NPR2'] = pmi2 / pmi3 if pmi3 != 0 else 0\n",
        "\n",
        "            # Radius of gyration\n",
        "            desc_dict['RadiusOfGyration'] = rdMolDescriptors.CalcRadiusOfGyration(mol_3d)\n",
        "\n",
        "            # Inertial shape factor\n",
        "            desc_dict['InertialShapeFactor'] = rdMolDescriptors.CalcInertialShapeFactor(mol_3d)\n",
        "\n",
        "            # Eccentricity\n",
        "            desc_dict['Eccentricity'] = rdMolDescriptors.CalcEccentricity(mol_3d)\n",
        "\n",
        "            # Asphericity\n",
        "            desc_dict['Asphericity'] = rdMolDescriptors.CalcAsphericity(mol_3d)\n",
        "\n",
        "            # Spherocity index\n",
        "            desc_dict['SpherocityIndex'] = rdMolDescriptors.CalcSpherocityIndex(mol_3d)\n",
        "\n",
        "            # Plane of best fit\n",
        "            desc_dict['PBF'] = rdMolDescriptors.CalcPBF(mol_3d)\n",
        "\n",
        "            # Remove unavailable descriptors\n",
        "            # desc_dict['Wiener3D'] = rdMolDescriptors.Calc3DWiener(mol_3d)  # Not available\n",
        "            # desc_dict['Balaban3D'] = rdMolDescriptors.Calc3DBalaban(mol_3d)  # Not available\n",
        "\n",
        "            # Hydrophobic surface area descriptors\n",
        "            desc_dict['FractionCSP3_3D'] = rdMolDescriptors.CalcFractionCSP3(mol_3d)\n",
        "\n",
        "            # Molecular surface area and volume\n",
        "            desc_dict['LabuteASA'] = rdMolDescriptors.CalcLabuteASA(mol_3d)\n",
        "            desc_dict['TPSA_3D'] = rdMolDescriptors.CalcTPSA(mol_3d)\n",
        "\n",
        "            # Get conformer and calculate geometric descriptors\n",
        "            if mol_3d.GetNumConformers() > 0:\n",
        "                conf = mol_3d.GetConformer()\n",
        "\n",
        "                # Calculate geometric center\n",
        "                coords = [list(conf.GetAtomPosition(i)) for i in range(mol_3d.GetNumAtoms())]\n",
        "                if coords:\n",
        "                    center = np.mean(coords, axis=0)\n",
        "                    desc_dict['GeometricCenter_X'] = center[0]\n",
        "                    desc_dict['GeometricCenter_Y'] = center[1]\n",
        "                    desc_dict['GeometricCenter_Z'] = center[2]\n",
        "\n",
        "                    # Calculate molecular dimensions\n",
        "                    coords_array = np.array(coords)\n",
        "                    min_coords = np.min(coords_array, axis=0)\n",
        "                    max_coords = np.max(coords_array, axis=0)\n",
        "                    dimensions = max_coords - min_coords\n",
        "                    desc_dict['MolecularLength'] = dimensions[0]\n",
        "                    desc_dict['MolecularWidth'] = dimensions[1]\n",
        "                    desc_dict['MolecularHeight'] = dimensions[2]\n",
        "                    desc_dict['MolecularVolume_Box'] = dimensions[0] * dimensions[1] * dimensions[2]\n",
        "\n",
        "            # Store original row data and descriptors\n",
        "            row_data = row.to_dict()\n",
        "            row_data.update(desc_dict)\n",
        "            successful_data.append(row_data)\n",
        "\n",
        "        except Exception as e:\n",
        "            failed_list.append((idx, smiles, str(e)))\n",
        "    else:\n",
        "        failed_list.append((idx, smiles, 'Invalid SMILES'))\n",
        "\n",
        "if successful_data:\n",
        "    # Create result DataFrame\n",
        "    result_df = pd.DataFrame(successful_data)\n",
        "\n",
        "    # Get the position of SMILE CODE column\n",
        "    original_cols = list(df.columns)\n",
        "    smile_idx = original_cols.index('SMILE CODE')\n",
        "\n",
        "    # Get descriptor columns (all columns not in original DataFrame)\n",
        "    desc_cols = [col for col in result_df.columns if col not in original_cols]\n",
        "\n",
        "    # Create new column order: original columns up to SMILE CODE, then descriptors, then remaining original columns\n",
        "    new_column_order = (original_cols[:smile_idx+1] +\n",
        "                       desc_cols +\n",
        "                       original_cols[smile_idx+1:])\n",
        "\n",
        "    # Reorder columns\n",
        "    result_df = result_df[new_column_order]\n",
        "\n",
        "    # Save to Excel\n",
        "    result_df.to_excel('drugs_with_3d_descriptors.xlsx', index=False)\n",
        "\n",
        "    print(f\"‚úì Generated {len(desc_cols)} 3D descriptors for {len(result_df)} compounds\")\n",
        "    print(\"‚úì Descriptors placed right after 'SMILE CODE' column\")\n",
        "    print(\"‚úì Saved to: drugs_with_3d_descriptors.xlsx\")\n",
        "\n",
        "    # Show descriptor names\n",
        "    print(f\"\\n3D descriptors calculated: {desc_cols}\")\n",
        "\n",
        "# Print failed compounds\n",
        "if failed_list:\n",
        "    print(f\"\\n‚úó Failed for {len(failed_list)} compounds:\")\n",
        "    for idx, smiles, error in failed_list:\n",
        "        print(f\"Row {idx+2}: '{smiles}' - {error}\")\n",
        "\n",
        "print(f\"\\nSummary: {len(successful_data)} successful, {len(failed_list)} failed\")"
      ],
      "metadata": {
        "id": "3kFnHsM7IPD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Some Settings to install Additional Fonts for Images:**"
      ],
      "metadata": {
        "id": "8BKZt5dTIXFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# GOOGLE COLAB FONT SETUP FOR PUBLICATION-QUALITY PLOTS\n",
        "# ============================================================================\n",
        "print(\"üîß Setting up fonts for publication-quality plots...\")\n",
        "\n",
        "# 1. Install the Liberation Sans font package (open-source, Arial-compatible)\n",
        "!sudo apt-get update -qq\n",
        "!sudo apt-get install -y fonts-liberation\n",
        "print(\"‚úÖ Font package installed.\")\n",
        "\n",
        "# 2. Clear matplotlib's font cache to force it to recognize new fonts\n",
        "import matplotlib\n",
        "!rm -rf ~/.cache/matplotlib\n",
        "\n",
        "# 3. Configure matplotlib to use Liberation Sans as the primary sans-serif font\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['font.family'] = 'sans-serif'\n",
        "plt.rcParams['font.sans-serif'] = ['Liberation Sans', 'DejaVu Sans', 'Verdana']\n",
        "plt.rcParams['axes.labelsize'] = 12\n",
        "plt.rcParams['axes.titlesize'] = 14\n",
        "\n",
        "# 4. Optional: Verify the font is available\n",
        "import matplotlib.font_manager as fm\n",
        "available_fonts = [f.name for f in fm.fontManager.ttflist]\n",
        "liberation_available = any('Liberation' in f for f in available_fonts)\n",
        "print(f\"‚úÖ Liberation Sans available: {liberation_available}\")\n",
        "\n",
        "print(\"‚úÖ Font setup complete. Your plots will now use Liberation Sans.\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# YOUR ORIGINAL PLOTTING CODE STARTS BELOW\n",
        "# ============================================================================\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ],
      "metadata": {
        "id": "vEYxR1G4IgCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Dataset Distribution Plot for Complete Dataset:**"
      ],
      "metadata": {
        "id": "vgCrN8_XIl6i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset required:Dataset for Machine Learning.xlsx"
      ],
      "metadata": {
        "id": "-2XMOcBoNdwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Read your data\n",
        "df = pd.read_excel('/content/Dataset for Machine Learning.xlsx')\n",
        "\n",
        "print(\"Dataset Overview:\")\n",
        "print(f\"Total compounds: {len(df)}\")\n",
        "\n",
        "# Count instances in each class\n",
        "class_counts = df['Ligand Type'].value_counts().sort_index()\n",
        "class_names = {0: 'Antagonist', 1: 'Agonist'}\n",
        "class_counts_named = class_counts.rename(index=class_names)\n",
        "\n",
        "print(f\"Agonists (1): {class_counts[1]}\")\n",
        "print(f\"Antagonists (0): {class_counts[0]}\")\n",
        "print(f\"Agonist/Antagonist ratio: {class_counts[1]/class_counts[0]:.2f}:1\")\n",
        "\n",
        "# Create a single figure with larger size\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Plot the single bar plot\n",
        "bars = plt.bar(class_counts_named.index, class_counts_named.values,\n",
        "               color=['#ff6b6b', '#4ecdc4'], alpha=0.8, edgecolor='black', linewidth=2)\n",
        "\n",
        "plt.title('Distribution of Agonists vs Antagonists', fontsize=20, fontweight='bold', pad=20)\n",
        "plt.ylabel('Number of Compounds', fontsize=18, fontweight='bold')\n",
        "plt.xlabel('Ligand Type', fontsize=18, fontweight='bold')\n",
        "\n",
        "# Increase tick label sizes\n",
        "plt.xticks(fontsize=16, fontweight='bold')\n",
        "plt.yticks(fontsize=16)\n",
        "\n",
        "# Add value labels on bars with larger font\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
        "             f'{int(height)}', ha='center', va='bottom',\n",
        "             fontweight='bold', fontsize=16)\n",
        "\n",
        "# Add grid for better readability\n",
        "plt.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "\n",
        "# Adjust layout and show plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print detailed statistics\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"DETAILED CLASS DISTRIBUTION STATISTICS\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Total compounds: {len(df)}\")\n",
        "print(f\"Antagonists (0): {class_counts[0]} ({class_counts[0]/len(df)*100:.1f}%)\")\n",
        "print(f\"Agonists (1): {class_counts[1]} ({class_counts[1]/len(df)*100:.1f}%)\")\n",
        "print(f\"Class ratio (Agonist:Antagonist): {class_counts[1]/class_counts[0]:.2f}:1\")\n",
        "\n",
        "# Check for class imbalance\n",
        "if class_counts[0] / class_counts[1] > 2 or class_counts[1] / class_counts[0] > 2:\n",
        "    print(\"\\n‚ö†Ô∏è  WARNING: Significant class imbalance detected!\")\n",
        "    print(\"   Consider using class weights in your ML model.\")\n",
        "else:\n",
        "    print(\"\\n‚úì Balanced dataset - good for machine learning!\")"
      ],
      "metadata": {
        "id": "BQHfgIGZIufm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Test-Train Split:**"
      ],
      "metadata": {
        "id": "eyGa_rxFIy-I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset required:Dataset for Machine Learning.xlsx"
      ],
      "metadata": {
        "id": "wOtQef-JNppE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Read your data\n",
        "df = pd.read_excel('/content/Dataset for Machine Learning.xlsx')\n",
        "\n",
        "print(\"Original Dataset Overview:\")\n",
        "print(f\"Total compounds: {len(df)}\")\n",
        "print(f\"Columns in dataset: {list(df.columns)}\")\n",
        "\n",
        "# Check class distribution\n",
        "class_counts = df['Ligand Type'].value_counts().sort_index()\n",
        "print(f\"Antagonists (0): {class_counts[0]} ({class_counts[0]/len(df)*100:.1f}%)\")\n",
        "print(f\"Agonists (1): {class_counts[1]} ({class_counts[1]/len(df)*100:.1f}%)\")\n",
        "\n",
        "# Prepare features and target - using exact column names from your dataset\n",
        "X = df.drop(['COMPOUND ID', 'SMILE CODE', 'Ligand Type'], axis=1)\n",
        "y = df['Ligand Type']\n",
        "\n",
        "# Split the data (80% train, 20% test) with stratification\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y  # This maintains the same class distribution in both sets\n",
        ")\n",
        "\n",
        "print(f\"\\nSplit Results:\")\n",
        "print(f\"Training set: {X_train.shape}\")\n",
        "print(f\"Testing set: {X_test.shape}\")\n",
        "\n",
        "# Create complete DataFrames for training and testing\n",
        "train_df = pd.DataFrame({\n",
        "    'COMPOUND ID': df.iloc[X_train.index]['COMPOUND ID'].values,\n",
        "    'SMILE CODE': df.iloc[X_train.index]['SMILE CODE'].values\n",
        "})\n",
        "train_df = pd.concat([train_df, X_train.reset_index(drop=True), y_train.reset_index(drop=True)], axis=1)\n",
        "\n",
        "test_df = pd.DataFrame({\n",
        "    'COMPOUND ID': df.iloc[X_test.index]['COMPOUND ID'].values,\n",
        "    'SMILE CODE': df.iloc[X_test.index]['SMILE CODE'].values\n",
        "})\n",
        "test_df = pd.concat([test_df, X_test.reset_index(drop=True), y_test.reset_index(drop=True)], axis=1)\n",
        "\n",
        "# Verify the splits\n",
        "print(\"\\nTraining Set Class Distribution:\")\n",
        "train_counts = train_df['Ligand Type'].value_counts().sort_index()\n",
        "print(f\"Antagonists (0): {train_counts[0]} ({train_counts[0]/len(train_df)*100:.1f}%)\")\n",
        "print(f\"Agonists (1): {train_counts[1]} ({train_counts[1]/len(train_df)*100:.1f}%)\")\n",
        "\n",
        "print(\"\\nTesting Set Class Distribution:\")\n",
        "test_counts = test_df['Ligand Type'].value_counts().sort_index()\n",
        "print(f\"Antagonists (0): {test_counts[0]} ({test_counts[0]/len(test_df)*100:.1f}%)\")\n",
        "print(f\"Agonists (1): {test_counts[1]} ({test_counts[1]/len(test_df)*100:.1f}%)\")\n",
        "\n",
        "# Save to Excel files\n",
        "train_df.to_excel('training_set.xlsx', index=False)\n",
        "test_df.to_excel('testing_set.xlsx', index=False)\n",
        "\n",
        "print(f\"\\n‚úì Files saved successfully!\")\n",
        "print(f\"Training set: training_set.xlsx ({len(train_df)} samples)\")\n",
        "print(f\"Testing set: testing_set.xlsx ({len(test_df)} samples)\")\n",
        "\n",
        "# Also save the feature matrices and targets separately for ML\n",
        "np.save('X_train.npy', X_train.values)\n",
        "np.save('X_test.npy', X_test.values)\n",
        "np.save('y_train.npy', y_train.values)\n",
        "np.save('y_test.npy', y_test.values)\n",
        "\n",
        "print(f\"‚úì Numpy arrays saved for ML modeling:\")\n",
        "print(f\"  X_train.npy, X_test.npy, y_train.npy, y_test.npy\")"
      ],
      "metadata": {
        "id": "naldeAlRI-90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Dataset Distribution in Training and Testing Data:**"
      ],
      "metadata": {
        "id": "-6N0BHacJEv7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset required:Dataset for Machine Learning.xlsx"
      ],
      "metadata": {
        "id": "rd3ZJ0rhNs6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Read data and split\n",
        "df = pd.read_excel('/content/Dataset for Machine Learning.xlsx')\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['Ligand Type'])\n",
        "\n",
        "# Calculate counts for the plot\n",
        "train_counts = train_df['Ligand Type'].value_counts().sort_index()\n",
        "test_counts = test_df['Ligand Type'].value_counts().sort_index()\n",
        "\n",
        "# Create the comparison plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "bar_width = 0.35\n",
        "x_pos = np.arange(2)\n",
        "\n",
        "plt.bar(x_pos - bar_width/2, train_counts.values, bar_width,\n",
        "        label='Training Set', color='#3498db', alpha=0.8, edgecolor='black')\n",
        "plt.bar(x_pos + bar_width/2, test_counts.values, bar_width,\n",
        "        label='Testing Set', color='#f39c12', alpha=0.8, edgecolor='black')\n",
        "\n",
        "plt.title('CLASS DISTRIBUTION COMPARISON: Training vs Testing Sets',\n",
        "          fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Ligand Type', fontsize=12)\n",
        "plt.ylabel('Number of Compounds', fontsize=12)\n",
        "plt.xticks(x_pos, ['Antagonist (0)', 'Agonist (1)'])\n",
        "plt.legend()\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add value labels on bars\n",
        "for i, v in enumerate(train_counts.values):\n",
        "    plt.text(i - bar_width/2, v + 0.5, str(v), ha='center', va='bottom', fontweight='bold')\n",
        "for i, v in enumerate(test_counts.values):\n",
        "    plt.text(i + bar_width/2, v + 0.5, str(v), ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pMtp6HUFJKLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Features Selectin using ReliefF**"
      ],
      "metadata": {
        "id": "n8701sOCJNsc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset required:Dataset for Machine Learning.xlsx"
      ],
      "metadata": {
        "id": "JNz_MIEGNxR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install skrebate"
      ],
      "metadata": {
        "id": "iUi4VZniJSXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from skrebate import ReliefF\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "\n",
        "# Set publication-quality parameters\n",
        "plt.rcParams['font.size'] = 12\n",
        "plt.rcParams['axes.labelsize'] = 14\n",
        "plt.rcParams['axes.titlesize'] = 16\n",
        "plt.rcParams['xtick.labelsize'] = 12\n",
        "plt.rcParams['ytick.labelsize'] = 12\n",
        "plt.rcParams['legend.fontsize'] = 12\n",
        "plt.rcParams['figure.titlesize'] = 18\n",
        "\n",
        "# Professional color scheme\n",
        "colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#3E92CC']\n",
        "\n",
        "print(\"üéØ FEATURE SELECTION PIPELINE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Load dataset\n",
        "data_df = pd.read_excel('/content/Dataset for Machine Learning.xlsx')\n",
        "\n",
        "print(\"üìä Dataset Information:\")\n",
        "print(f\"Complete dataset: {data_df.shape}\")\n",
        "\n",
        "# STEP 1: Identify feature types based on your exact column structure\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üîç IDENTIFYING FEATURE TYPES\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Get all columns\n",
        "all_columns = data_df.columns.tolist()\n",
        "\n",
        "# Identify feature sets based on your exact column ranges\n",
        "morgan_columns = [col for col in all_columns if col.startswith('fp_bit_')]\n",
        "\n",
        "# Find the exact range for 2D descriptors\n",
        "try:\n",
        "    max_abs_index = all_columns.index('MaxAbsEStateIndex')\n",
        "    fr_urea_index = all_columns.index('fr_urea')\n",
        "    td_columns = all_columns[max_abs_index:fr_urea_index + 1]\n",
        "except ValueError as e:\n",
        "    print(f\"Error finding 2D descriptor boundaries: {e}\")\n",
        "    td_columns = []\n",
        "\n",
        "# Find the exact range for 3D descriptors\n",
        "try:\n",
        "    pmi1_index = all_columns.index('PMI1')\n",
        "    mv_box_index = all_columns.index('MolecularVolume_Box')\n",
        "    threed_columns = all_columns[pmi1_index:mv_box_index + 1]\n",
        "except ValueError as e:\n",
        "    print(f\"Error finding 3D descriptor boundaries: {e}\")\n",
        "    threed_columns = []\n",
        "\n",
        "print(f\"‚úÖ FEATURE SETS IDENTIFIED:\")\n",
        "print(f\"   Morgan Fingerprints: {len(morgan_columns)} features (fp_bit_0 to fp_bit_2047)\")\n",
        "print(f\"   2D Descriptors: {len(td_columns)} features (MaxAbsEStateIndex to fr_urea)\")\n",
        "print(f\"   3D Descriptors: {len(threed_columns)} features (PMI1 to MolecularVolume_Box)\")\n",
        "\n",
        "# STEP 2: Train-Test Split\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üìä TRAIN-TEST SPLIT\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Prepare features and target\n",
        "metadata_columns = ['COMPOUND ID', 'SMILE CODE', 'Ligand Type']\n",
        "X = data_df.drop(metadata_columns, axis=1)\n",
        "y = data_df['Ligand Type']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set: {X_train.shape}\")\n",
        "print(f\"Testing set: {X_test.shape}\")\n",
        "\n",
        "# STEP 3: Feature Selection using ReliefF\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üéØ FEATURE SELECTION USING RELIEFF\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "def perform_relieff_selection(X_train, y_train, feature_columns, n_features_to_select=100, feature_set_name=\"\"):\n",
        "    \"\"\"Perform ReliefF feature selection on specified feature set\"\"\"\n",
        "    print(f\"\\nüîç Selecting features for {feature_set_name}:\")\n",
        "    print(f\"   Total features: {len(feature_columns)}\")\n",
        "\n",
        "    if len(feature_columns) <= n_features_to_select:\n",
        "        print(f\"   ‚úÖ Keeping all {len(feature_columns)} features\")\n",
        "        return feature_columns, pd.DataFrame({\n",
        "            'Feature': feature_columns,\n",
        "            'ReliefF_Score': [1.0] * len(feature_columns)\n",
        "        })\n",
        "\n",
        "    print(f\"   Selecting top {n_features_to_select} features using ReliefF...\")\n",
        "\n",
        "    # Extract the specific features from training set\n",
        "    X_subset = X_train[feature_columns].values\n",
        "\n",
        "    # Encode labels for ReliefF\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_encoded = label_encoder.fit_transform(y_train)\n",
        "\n",
        "    # Apply ReliefF\n",
        "    relief = ReliefF(\n",
        "        n_features_to_select=n_features_to_select,\n",
        "        n_neighbors=100\n",
        "    )\n",
        "\n",
        "    relief.fit(X_subset, y_encoded)\n",
        "\n",
        "    # Get top feature indices and scores\n",
        "    top_indices = relief.top_features_[:n_features_to_select]\n",
        "    top_scores = relief.feature_importances_[top_indices]\n",
        "\n",
        "    selected_features = [feature_columns[i] for i in top_indices]\n",
        "\n",
        "    print(f\"   ‚úÖ Selected {len(selected_features)} features\")\n",
        "\n",
        "    # Create feature importance dataframe for this set\n",
        "    importance_df = pd.DataFrame({\n",
        "        'Feature': selected_features,\n",
        "        'ReliefF_Score': top_scores\n",
        "    }).sort_values('ReliefF_Score', ascending=False)\n",
        "\n",
        "    return selected_features, importance_df\n",
        "\n",
        "# Perform feature selection for each set\n",
        "selected_features = {}\n",
        "feature_importance_dfs = {}\n",
        "\n",
        "# Morgan Fingerprints: Select top 100 (CHANGED FROM 30 TO 100)\n",
        "print(\"üîÑ Processing Morgan Fingerprints...\")\n",
        "morgan_selected, morgan_importance = perform_relieff_selection(\n",
        "    X_train, y_train, morgan_columns, 100, \"Morgan Fingerprints\"  # Changed to 100\n",
        ")\n",
        "selected_features['morgan'] = morgan_selected\n",
        "feature_importance_dfs['morgan'] = morgan_importance\n",
        "\n",
        "# 2D Descriptors: Select top 100 (CHANGED FROM 30 TO 100)\n",
        "print(\"üîÑ Processing 2D Descriptors...\")\n",
        "td_selected, td_importance = perform_relieff_selection(\n",
        "    X_train, y_train, td_columns, 100, \"2D Descriptors\"  # Changed to 100\n",
        ")\n",
        "selected_features['2d_descriptors'] = td_selected\n",
        "feature_importance_dfs['2d_descriptors'] = td_importance\n",
        "\n",
        "# 3D Descriptors: Keep all (as per your specification)\n",
        "print(\"üîÑ Processing 3D Descriptors...\")\n",
        "print(f\"   Total features: {len(threed_columns)}\")\n",
        "print(f\"   ‚úÖ Keeping all {len(threed_columns)} features (as specified)\")\n",
        "selected_features['3d_descriptors'] = threed_columns\n",
        "\n",
        "# Create importance dataframe for 3D (all features with dummy scores)\n",
        "threed_importance = pd.DataFrame({\n",
        "    'Feature': threed_columns,\n",
        "    'ReliefF_Score': [1.0] * len(threed_columns)  # Placeholder score\n",
        "})\n",
        "feature_importance_dfs['3d_descriptors'] = threed_importance\n",
        "\n",
        "# Combine all selected features\n",
        "all_selected_features = (\n",
        "    selected_features['morgan'] +\n",
        "    selected_features['2d_descriptors'] +\n",
        "    selected_features['3d_descriptors']\n",
        ")\n",
        "\n",
        "print(f\"\\nüéØ FINAL FEATURE SELECTION SUMMARY:\")\n",
        "print(f\"   Total selected features: {len(all_selected_features)}\")\n",
        "print(f\"   - Morgan Fingerprints: {len(selected_features['morgan'])}\")\n",
        "print(f\"   - 2D Descriptors: {len(selected_features['2d_descriptors'])}\")\n",
        "print(f\"   - 3D Descriptors: {len(selected_features['3d_descriptors'])}\")\n",
        "\n",
        "# STEP 4: Create new datasets with selected features\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üìä CREATING FEATURE-SELECTED DATASETS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "X_train_selected = X_train[all_selected_features]\n",
        "X_test_selected = X_test[all_selected_features]\n",
        "\n",
        "print(f\"Training set after feature selection: {X_train_selected.shape}\")\n",
        "print(f\"Testing set after feature selection: {X_test_selected.shape}\")\n",
        "\n",
        "# STEP 5: Save all results\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üíæ SAVING FEATURE SELECTION RESULTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Save selected features list\n",
        "selected_features_df = pd.DataFrame({\n",
        "    'Feature_Type': (['Morgan'] * len(selected_features['morgan']) +\n",
        "                    ['2D_Descriptors'] * len(selected_features['2d_descriptors']) +\n",
        "                    ['3D_Descriptors'] * len(selected_features['3d_descriptors'])),\n",
        "    'Feature_Name': all_selected_features\n",
        "})\n",
        "selected_features_df.to_excel('selected_features_complete.xlsx', index=False)\n",
        "print(\"‚úÖ Saved: selected_features_complete.xlsx\")\n",
        "\n",
        "# Save feature importance scores for each set\n",
        "with pd.ExcelWriter('feature_importance_scores.xlsx') as writer:\n",
        "    feature_importance_dfs['morgan'].to_excel(writer, sheet_name='Morgan_Fingerprints', index=False)\n",
        "    feature_importance_dfs['2d_descriptors'].to_excel(writer, sheet_name='2D_Descriptors', index=False)\n",
        "    feature_importance_dfs['3d_descriptors'].to_excel(writer, sheet_name='3D_Descriptors', index=False)\n",
        "print(\"‚úÖ Saved: feature_importance_scores.xlsx\")\n",
        "\n",
        "# Save the train-test splits with selected features\n",
        "train_selected_df = pd.concat([\n",
        "    data_df.loc[X_train.index, metadata_columns].reset_index(drop=True),\n",
        "    X_train_selected.reset_index(drop=True)\n",
        "], axis=1)\n",
        "\n",
        "test_selected_df = pd.concat([\n",
        "    data_df.loc[X_test.index, metadata_columns].reset_index(drop=True),\n",
        "    X_test_selected.reset_index(drop=True)\n",
        "], axis=1)\n",
        "\n",
        "train_selected_df.to_excel('training_set_selected_features.xlsx', index=False)\n",
        "test_selected_df.to_excel('testing_set_selected_features.xlsx', index=False)\n",
        "print(\"‚úÖ Saved: training_set_selected_features.xlsx\")\n",
        "print(\"‚úÖ Saved: testing_set_selected_features.xlsx\")\n",
        "\n",
        "# Save the feature selection objects for reuse\n",
        "feature_selection_artifacts = {\n",
        "    'selected_features': selected_features,\n",
        "    'all_selected_features': all_selected_features,\n",
        "    'feature_columns_mapping': {\n",
        "        'morgan_original': morgan_columns,\n",
        "        '2d_original': td_columns,\n",
        "        '3d_original': threed_columns\n",
        "    }\n",
        "}\n",
        "joblib.dump(feature_selection_artifacts, 'feature_selection_artifacts.pkl')\n",
        "print(\"‚úÖ Saved: feature_selection_artifacts.pkl\")\n",
        "\n",
        "# STEP 6: Create comprehensive visualization\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üìä CREATING FEATURE SELECTION VISUALIZATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Plot 1: Feature selection summary\n",
        "plt.subplot(2, 2, 1)\n",
        "feature_types = ['Morgan Fingerprints', '2D Descriptors', '3D Descriptors']\n",
        "original_counts = [len(morgan_columns), len(td_columns), len(threed_columns)]\n",
        "selected_counts = [len(selected_features['morgan']),\n",
        "                  len(selected_features['2d_descriptors']),\n",
        "                  len(selected_features['3d_descriptors'])]\n",
        "\n",
        "x = np.arange(len(feature_types))\n",
        "width = 0.35\n",
        "\n",
        "bars1 = plt.bar(x - width/2, original_counts, width, label='Original',\n",
        "                color=colors[0], alpha=0.8, edgecolor='black')\n",
        "bars2 = plt.bar(x + width/2, selected_counts, width, label='Selected',\n",
        "                color=colors[1], alpha=0.8, edgecolor='black')\n",
        "\n",
        "plt.ylabel('Number of Features')\n",
        "plt.title('Feature Selection Summary', fontweight='bold')\n",
        "plt.xticks(x, feature_types)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels\n",
        "for bars in [bars1, bars2]:\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, height + 5,\n",
        "                f'{int(height)}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Plot 2: Top features from Morgan fingerprints\n",
        "plt.subplot(2, 2, 2)\n",
        "top_morgan = feature_importance_dfs['morgan'].head(15)\n",
        "plt.barh(range(len(top_morgan)), top_morgan['ReliefF_Score'],\n",
        "         color=colors[2], alpha=0.8, edgecolor='black')\n",
        "plt.yticks(range(len(top_morgan)), [f\"Bit {f.split('_')[-1]}\" for f in top_morgan['Feature']])\n",
        "plt.xlabel('ReliefF Importance Score')\n",
        "plt.title('Top 15 Morgan Fingerprints', fontweight='bold')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Plot 3: Top features from 2D descriptors\n",
        "plt.subplot(2, 2, 3)\n",
        "top_2d = feature_importance_dfs['2d_descriptors'].head(15)\n",
        "plt.barh(range(len(top_2d)), top_2d['ReliefF_Score'],\n",
        "         color=colors[3], alpha=0.8, edgecolor='black')\n",
        "plt.yticks(range(len(top_2d)), top_2d['Feature'])\n",
        "plt.xlabel('ReliefF Importance Score')\n",
        "plt.title('Top 15 2D Descriptors', fontweight='bold')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Plot 4: Dataset dimensions comparison\n",
        "plt.subplot(2, 2, 4)\n",
        "dimensions = ['Original', 'After Selection']\n",
        "train_sizes = [X_train.shape[1], X_train_selected.shape[1]]\n",
        "test_sizes = [X_test.shape[1], X_test_selected.shape[1]]\n",
        "\n",
        "x = np.arange(len(dimensions))\n",
        "width = 0.35\n",
        "\n",
        "bars1 = plt.bar(x - width/2, train_sizes, width, label='Training',\n",
        "                color=colors[0], alpha=0.8, edgecolor='black')\n",
        "bars2 = plt.bar(x + width/2, test_sizes, width, label='Testing',\n",
        "                color=colors[1], alpha=0.8, edgecolor='black')\n",
        "\n",
        "plt.ylabel('Number of Features')\n",
        "plt.title('Dataset Dimensions Comparison', fontweight='bold')\n",
        "plt.xticks(x, dimensions)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels\n",
        "for bars in [bars1, bars2]:\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, height + 10,\n",
        "                f'{int(height)}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('feature_selection_comprehensive.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüéØ FEATURE SELECTION PIPELINE COMPLETED!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"üìä RESULTS SUMMARY:\")\n",
        "print(f\"   ‚Ä¢ Original features: {X.shape[1]}\")\n",
        "print(f\"   ‚Ä¢ Selected features: {len(all_selected_features)}\")\n",
        "print(f\"   ‚Ä¢ Reduction: {X.shape[1] - len(all_selected_features)} features removed\")\n",
        "print(f\"   ‚Ä¢ Morgan fingerprints: {len(selected_features['morgan'])} selected from {len(morgan_columns)}\")\n",
        "print(f\"   ‚Ä¢ 2D descriptors: {len(selected_features['2d_descriptors'])} selected from {len(td_columns)}\")\n",
        "print(f\"   ‚Ä¢ 3D descriptors: {len(selected_features['3d_descriptors'])} kept from {len(threed_columns)}\")\n",
        "\n",
        "print(f\"\\nüíæ FILES GENERATED:\")\n",
        "print(f\"   1. selected_features_complete.xlsx - Complete list of selected features\")\n",
        "print(f\"   2. feature_importance_scores.xlsx - ReliefF scores for each feature set\")\n",
        "print(f\"   3. training_set_selected_features.xlsx - Training set with selected features\")\n",
        "print(f\"   4. testing_set_selected_features.xlsx - Testing set with selected features\")\n",
        "print(f\"   5. feature_selection_artifacts.pkl - Objects for reproducibility\")\n",
        "print(f\"   6. feature_selection_comprehensive.png - Visualization of results\")\n",
        "\n",
        "print(f\"\\nüöÄ You can now use these feature-selected datasets for all your model training!\")\n",
        "print(f\"üìÅ Use 'training_set_selected_features.xlsx' and 'testing_set_selected_features.xlsx'\")\n",
        "print(f\"   for ANN, Random Forest, KNN, and other models!\")"
      ],
      "metadata": {
        "id": "6J9uxqsMJUmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Random Forest Classifier:**"
      ],
      "metadata": {
        "id": "ll-E-OKTJzyy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Requirements:\n",
        "training-set-selected-features.xlsx,\n",
        "testing-set-selected-featres.xlsx"
      ],
      "metadata": {
        "id": "WMqpgR12N3u1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "P7QS6qQWJ3fZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "                           roc_auc_score, roc_curve, matthews_corrcoef, confusion_matrix,\n",
        "                           classification_report)\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, cross_val_predict, GridSearchCV\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "\n",
        "# Set publication-quality parameters (Fixed font settings)\n",
        "plt.rcParams['font.size'] = 12\n",
        "plt.rcParams['axes.labelsize'] = 14\n",
        "plt.rcParams['axes.titlesize'] = 16\n",
        "plt.rcParams['xtick.labelsize'] = 12\n",
        "plt.rcParams['ytick.labelsize'] = 12\n",
        "plt.rcParams['legend.fontsize'] = 12\n",
        "plt.rcParams['figure.titlesize'] = 18\n",
        "\n",
        "# Use available system fonts to avoid warnings\n",
        "plt.rcParams['font.family'] = ['DejaVu Sans', 'Arial', 'Helvetica', 'sans-serif']\n",
        "\n",
        "# Professional Elsevier color scheme\n",
        "elsevier_colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#1A936F', '#114B5F']\n",
        "\n",
        "print(\"üöÄ RANDOM FOREST CLASSIFIER WITH HYPERPARAMETER TUNING (10-FOLD CV + GRID SEARCH)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# STEP 1: Load the feature-selected datasets\n",
        "print(\"üìä Loading feature-selected datasets...\")\n",
        "\n",
        "train_df = pd.read_excel('training_set_selected_features.xlsx')\n",
        "test_df = pd.read_excel('testing_set_selected_features.xlsx')\n",
        "\n",
        "print(f\"Training set: {train_df.shape}\")\n",
        "print(f\"Testing set: {test_df.shape}\")\n",
        "\n",
        "# STEP 2: Prepare features and target\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üîß PREPARING DATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Identify feature columns (exclude metadata)\n",
        "metadata_columns = ['COMPOUND ID', 'SMILE CODE', 'Ligand Type']\n",
        "feature_columns = [col for col in train_df.columns if col not in metadata_columns]\n",
        "\n",
        "X_train = train_df[feature_columns]\n",
        "y_train = train_df['Ligand Type']\n",
        "\n",
        "X_test = test_df[feature_columns]\n",
        "y_test = test_df['Ligand Type']\n",
        "\n",
        "print(f\"Selected features: {len(feature_columns)}\")\n",
        "print(f\"Training samples: {X_train.shape[0]}\")\n",
        "print(f\"Testing samples: {X_test.shape[0]}\")\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# FIXED: Properly get class names as strings\n",
        "class_names = [str(cls) for cls in label_encoder.classes_]\n",
        "print(f\"Class names: {class_names}\")\n",
        "\n",
        "print(\"‚úÖ Data prepared successfully!\")\n",
        "\n",
        "# STEP 3: Hyperparameter Tuning with GridSearchCV and 10-Fold CV\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéØ HYPERPARAMETER TUNING WITH GRIDSEARCHCV (10-FOLD CV)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Define parameter grid for Random Forest\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['sqrt', 'log2', None],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# Initialize Random Forest\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# 10-fold cross-validation strategy\n",
        "cv_strategy = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# GridSearchCV with 10-fold CV\n",
        "print(\"üîç Performing Grid Search with 10-fold CV...\")\n",
        "print(f\"   Parameter grid: {param_grid}\")\n",
        "print(f\"   Total parameter combinations: {len(param_grid['n_estimators']) * len(param_grid['max_depth']) * len(param_grid['min_samples_split']) * len(param_grid['min_samples_leaf']) * len(param_grid['max_features']) * len(param_grid['bootstrap'])}\")\n",
        "print(f\"   This may take a while...\")\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=cv_strategy,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Perform grid search\n",
        "grid_search.fit(X_train, y_train_encoded)\n",
        "\n",
        "print(\"‚úÖ Grid Search completed!\")\n",
        "\n",
        "# Display best parameters and scores\n",
        "print(f\"\\nüéØ BEST PARAMETERS FOUND:\")\n",
        "best_params = grid_search.best_params_\n",
        "for param, value in best_params.items():\n",
        "    print(f\"   {param}: {value}\")\n",
        "print(f\"   Best CV Score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# STEP 4: Compare Default vs Tuned Models\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä COMPARISON: DEFAULT vs TUNED PARAMETERS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Train model with default parameters for comparison\n",
        "rf_default = RandomForestClassifier(random_state=42)\n",
        "rf_default.fit(X_train, y_train_encoded)\n",
        "y_pred_default = rf_default.predict(X_test)\n",
        "accuracy_default = accuracy_score(y_test_encoded, y_pred_default)\n",
        "\n",
        "# Use best model from grid search\n",
        "best_rf_model = grid_search.best_estimator_\n",
        "y_pred_tuned = best_rf_model.predict(X_test)\n",
        "accuracy_tuned = accuracy_score(y_test_encoded, y_pred_tuned)\n",
        "\n",
        "improvement = accuracy_tuned - accuracy_default\n",
        "\n",
        "print(f\"   Default Parameters Accuracy: {accuracy_default:.4f}\")\n",
        "print(f\"   Tuned Parameters Accuracy:   {accuracy_tuned:.4f}\")\n",
        "print(f\"   Improvement:                 {improvement:+.4f}\")\n",
        "\n",
        "if improvement > 0:\n",
        "    print(f\"   ‚úÖ Tuning improved accuracy by {improvement:.2%}\")\n",
        "else:\n",
        "    print(f\"   ‚ö†Ô∏è  Tuning did not improve accuracy\")\n",
        "\n",
        "# STEP 5: 10-Fold Cross-Validation with Tuned Model (for proper comparison)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéØ 10-FOLD CROSS-VALIDATION WITH TUNED PARAMETERS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"üöÄ Performing 10-fold cross-validation with tuned parameters...\")\n",
        "\n",
        "# Get cross-validated predictions for training set using tuned parameters\n",
        "cv_predictions = cross_val_predict(best_rf_model, X_train, y_train_encoded,\n",
        "                                 cv=cv_strategy, method='predict')\n",
        "cv_probabilities = cross_val_predict(best_rf_model, X_train, y_train_encoded,\n",
        "                                   cv=cv_strategy, method='predict_proba')\n",
        "\n",
        "# Calculate training metrics as average across 10-fold CV\n",
        "train_accuracy = accuracy_score(y_train_encoded, cv_predictions)\n",
        "\n",
        "# Calculate metrics for BOTH CLASSES\n",
        "train_precision = precision_score(y_train_encoded, cv_predictions, average=None)\n",
        "train_recall = recall_score(y_train_encoded, cv_predictions, average=None)\n",
        "train_f1 = f1_score(y_train_encoded, cv_predictions, average=None)\n",
        "train_roc_auc = roc_auc_score(y_train_encoded, cv_probabilities[:, 1])\n",
        "train_mcc = matthews_corrcoef(y_train_encoded, cv_predictions)\n",
        "\n",
        "# Also get individual fold accuracies for reporting\n",
        "cv_scores = cross_val_score(best_rf_model, X_train, y_train_encoded,\n",
        "                          cv=cv_strategy, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "print(\"‚úÖ 10-fold cross-validation completed!\")\n",
        "\n",
        "print(f\"\\nüìä 10-FOLD CROSS-VALIDATION RESULTS (TRAINING PERFORMANCE):\")\n",
        "print(f\"   Fold scores: {[f'{score:.4f}' for score in cv_scores]}\")\n",
        "print(f\"   Mean CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
        "print(f\"   Standard Deviation: {cv_scores.std():.4f}\")\n",
        "\n",
        "# STEP 6: Make predictions on TEST set with tuned model\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä MODEL EVALUATION WITH TUNED PARAMETERS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Predictions on TEST set with tuned model\n",
        "y_test_pred = best_rf_model.predict(X_test)\n",
        "y_test_proba = best_rf_model.predict_proba(X_test)\n",
        "\n",
        "# Calculate metrics for TESTING set for BOTH CLASSES\n",
        "test_accuracy = accuracy_score(y_test_encoded, y_test_pred)\n",
        "test_precision = precision_score(y_test_encoded, y_test_pred, average=None)\n",
        "test_recall = recall_score(y_test_encoded, y_test_pred, average=None)\n",
        "test_f1 = f1_score(y_test_encoded, y_test_pred, average=None)\n",
        "test_roc_auc = roc_auc_score(y_test_encoded, y_test_proba[:, 1])\n",
        "test_mcc = matthews_corrcoef(y_test_encoded, y_test_pred)\n",
        "\n",
        "# Confusion matrices\n",
        "train_cm = confusion_matrix(y_train_encoded, cv_predictions)\n",
        "test_cm = confusion_matrix(y_test_encoded, y_test_pred)\n",
        "tn, fp, fn, tp = test_cm.ravel()\n",
        "\n",
        "# Additional metrics\n",
        "specificity = tn / (tn + fp)\n",
        "npv = tn / (tn + fn)\n",
        "\n",
        "# Performance gap\n",
        "accuracy_gap = train_accuracy - test_accuracy\n",
        "\n",
        "# Compute ROC curves for later saving\n",
        "fpr_train, tpr_train, _ = roc_curve(y_train_encoded, cv_probabilities[:, 1])\n",
        "fpr_test, tpr_test, _ = roc_curve(y_test_encoded, y_test_proba[:, 1])\n",
        "\n",
        "# STEP 7: Display comprehensive results for BOTH CLASSES\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìà COMPREHENSIVE PERFORMANCE METRICS FOR BOTH CLASSES (TUNED MODEL)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create detailed comparison dataframe for BOTH CLASSES\n",
        "class_performance_data = []\n",
        "\n",
        "for i, class_name in enumerate(class_names):\n",
        "    class_performance_data.extend([\n",
        "        {\n",
        "            'Class': class_name,\n",
        "            'Metric': 'Precision',\n",
        "            'Training (10-Fold CV)': f\"{train_precision[i]:.4f}\",\n",
        "            'Testing': f\"{test_precision[i]:.4f}\",\n",
        "            'Difference': f\"{train_precision[i]-test_precision[i]:+.4f}\"\n",
        "        },\n",
        "        {\n",
        "            'Class': class_name,\n",
        "            'Metric': 'Recall',\n",
        "            'Training (10-Fold CV)': f\"{train_recall[i]:.4f}\",\n",
        "            'Testing': f\"{test_recall[i]:.4f}\",\n",
        "            'Difference': f\"{train_recall[i]-test_recall[i]:+.4f}\"\n",
        "        },\n",
        "        {\n",
        "            'Class': class_name,\n",
        "            'Metric': 'F1-Score',\n",
        "            'Training (10-Fold CV)': f\"{train_f1[i]:.4f}\",\n",
        "            'Testing': f\"{test_f1[i]:.4f}\",\n",
        "            'Difference': f\"{train_f1[i]-test_f1[i]:+.4f}\"\n",
        "        }\n",
        "    ])\n",
        "\n",
        "# Add overall metrics\n",
        "overall_metrics = [\n",
        "    {\n",
        "        'Class': 'Overall',\n",
        "        'Metric': 'Accuracy',\n",
        "        'Training (10-Fold CV)': f\"{train_accuracy:.4f}\",\n",
        "        'Testing': f\"{test_accuracy:.4f}\",\n",
        "        'Difference': f\"{train_accuracy-test_accuracy:+.4f}\"\n",
        "    },\n",
        "    {\n",
        "        'Class': 'Overall',\n",
        "        'Metric': 'ROC-AUC',\n",
        "        'Training (10-Fold CV)': f\"{train_roc_auc:.4f}\",\n",
        "        'Testing': f\"{test_roc_auc:.4f}\",\n",
        "        'Difference': f\"{train_roc_auc-test_roc_auc:+.4f}\"\n",
        "    },\n",
        "    {\n",
        "        'Class': 'Overall',\n",
        "        'Metric': 'MCC',\n",
        "        'Training (10-Fold CV)': f\"{train_mcc:.4f}\",\n",
        "        'Testing': f\"{test_mcc:.4f}\",\n",
        "        'Difference': f\"{train_mcc-test_mcc:+.4f}\"\n",
        "    }\n",
        "]\n",
        "\n",
        "performance_df = pd.DataFrame(class_performance_data + overall_metrics)\n",
        "print(performance_df.to_string(index=False))\n",
        "\n",
        "# STEP 8: Enhanced Performance Visualization with ROC Curves\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä GENERATING ENHANCED PERFORMANCE VISUALIZATIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create comprehensive performance comparison with ROC curves\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 16))\n",
        "\n",
        "# 1. Main Performance Comparison (Overall Metrics)\n",
        "overall_metrics = ['Accuracy', 'ROC-AUC', 'MCC']\n",
        "train_overall = [train_accuracy, train_roc_auc, train_mcc]\n",
        "test_overall = [test_accuracy, test_roc_auc, test_mcc]\n",
        "\n",
        "x = np.arange(len(overall_metrics))\n",
        "width = 0.35\n",
        "\n",
        "bars1 = ax1.bar(x - width/2, train_overall, width, label='Training (10-Fold CV)',\n",
        "               color=elsevier_colors[0], alpha=0.8, edgecolor='black', linewidth=1.5)\n",
        "bars2 = ax1.bar(x + width/2, test_overall, width, label='Testing (Held-Out)',\n",
        "               color=elsevier_colors[1], alpha=0.8, edgecolor='black', linewidth=1.5)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bars in [bars1, bars2]:\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                f'{height:.3f}', ha='center', va='bottom',\n",
        "                fontweight='bold', fontsize=11)\n",
        "\n",
        "ax1.set_xlabel('Performance Metrics', fontsize=14, fontweight='bold')\n",
        "ax1.set_ylabel('Score', fontsize=14, fontweight='bold')\n",
        "ax1.set_title('Random Forest (Tuned) - Overall Performance\\nTraining vs Testing',\n",
        "              fontsize=16, fontweight='bold', pad=20)\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(overall_metrics)\n",
        "ax1.legend(fontsize=12, framealpha=0.9)\n",
        "ax1.set_ylim(0, 1.1)\n",
        "ax1.grid(True, alpha=0.3, linestyle='--')\n",
        "\n",
        "# 2. ROC CURVE for Training and Testing Sets\n",
        "ax2.plot(fpr_train, tpr_train, color=elsevier_colors[0], lw=2.5,\n",
        "         label=f'Training ROC (AUC = {train_roc_auc:.3f})', alpha=0.8)\n",
        "ax2.plot(fpr_test, tpr_test, color=elsevier_colors[1], lw=2.5,\n",
        "         label=f'Testing ROC (AUC = {test_roc_auc:.3f})', alpha=0.8)\n",
        "\n",
        "# Plot diagonal reference line\n",
        "ax2.plot([0, 1], [0, 1], color='gray', lw=1.5, linestyle='--', alpha=0.7,\n",
        "         label='Random Classifier (AUC = 0.5)')\n",
        "\n",
        "# Fill area under curves\n",
        "ax2.fill_between(fpr_train, tpr_train, alpha=0.2, color=elsevier_colors[0])\n",
        "ax2.fill_between(fpr_test, tpr_test, alpha=0.2, color=elsevier_colors[1])\n",
        "\n",
        "ax2.set_xlabel('False Positive Rate', fontsize=14, fontweight='bold')\n",
        "ax2.set_ylabel('True Positive Rate', fontsize=14, fontweight='bold')\n",
        "ax2.set_title('ROC Curves - Training vs Testing Sets\\nRandom Forest (Tuned)',\n",
        "              fontsize=16, fontweight='bold', pad=20)\n",
        "ax2.legend(loc='lower right', fontsize=12, framealpha=0.9)\n",
        "ax2.grid(True, alpha=0.3, linestyle='--')\n",
        "ax2.set_xlim([0.0, 1.0])\n",
        "ax2.set_ylim([0.0, 1.05])\n",
        "\n",
        "# 3. Confusion Matrix - Training\n",
        "im1 = ax3.imshow(train_cm, cmap='Blues', interpolation='nearest', alpha=0.8)\n",
        "ax3.set_xlabel('Predicted Label', fontsize=14, fontweight='bold')\n",
        "ax3.set_ylabel('True Label', fontsize=14, fontweight='bold')\n",
        "ax3.set_title('Training Set - Confusion Matrix\\n(10-Fold CV Average)',\n",
        "              fontsize=16, fontweight='bold', pad=20)\n",
        "ax3.set_xticks([0, 1])\n",
        "ax3.set_yticks([0, 1])\n",
        "ax3.set_xticklabels(class_names)\n",
        "ax3.set_yticklabels(class_names)\n",
        "\n",
        "# Add text annotations for training CM\n",
        "for i in range(train_cm.shape[0]):\n",
        "    for j in range(train_cm.shape[1]):\n",
        "        ax3.text(j, i, f'{train_cm[i, j]}\\n({train_cm[i, j]/np.sum(train_cm):.1%})',\n",
        "                ha='center', va='center', fontsize=12, fontweight='bold',\n",
        "                color='white' if train_cm[i, j] > np.max(train_cm)/2 else 'black')\n",
        "\n",
        "# 4. Confusion Matrix - Testing\n",
        "im2 = ax4.imshow(test_cm, cmap='Reds', interpolation='nearest', alpha=0.8)\n",
        "ax4.set_xlabel('Predicted Label', fontsize=14, fontweight='bold')\n",
        "ax4.set_ylabel('True Label', fontsize=14, fontweight='bold')\n",
        "ax4.set_title('Testing Set - Confusion Matrix\\n(Held-Out Dataset)',\n",
        "              fontsize=16, fontweight='bold', pad=20)\n",
        "ax4.set_xticks([0, 1])\n",
        "ax4.set_yticks([0, 1])\n",
        "ax4.set_xticklabels(class_names)\n",
        "ax4.set_yticklabels(class_names)\n",
        "\n",
        "# Add text annotations for testing CM\n",
        "for i in range(test_cm.shape[0]):\n",
        "    for j in range(test_cm.shape[1]):\n",
        "        ax4.text(j, i, f'{test_cm[i, j]}\\n({test_cm[i, j]/np.sum(test_cm):.1%})',\n",
        "                ha='center', va='center', fontsize=12, fontweight='bold',\n",
        "                color='white' if test_cm[i, j] > np.max(test_cm)/2 else 'black')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('random_forest_tuned_comprehensive_performance.png', dpi=300,\n",
        "            bbox_inches='tight', facecolor='white')\n",
        "plt.show()\n",
        "\n",
        "# STEP 9: FIXED Classification Reports\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìã DETAILED CLASSIFICATION REPORTS FOR BOTH CLASSES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nTRAINING SET CLASSIFICATION REPORT (10-Fold CV Average):\")\n",
        "training_report = classification_report(y_train_encoded, cv_predictions,\n",
        "                                      target_names=class_names, digits=4)\n",
        "print(training_report)\n",
        "\n",
        "print(\"\\nTESTING SET CLASSIFICATION REPORT (Held-Out):\")\n",
        "testing_report = classification_report(y_test_encoded, y_test_pred,\n",
        "                                     target_names=class_names, digits=4)\n",
        "print(testing_report)\n",
        "\n",
        "# STEP 10: Individual Class Performance Analysis\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéØ INDIVIDUAL CLASS PERFORMANCE ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for i, class_name in enumerate(class_names):\n",
        "    print(f\"\\n{class_name.upper()} CLASS PERFORMANCE:\")\n",
        "    print(f\"  Training (10-Fold CV):\")\n",
        "    print(f\"    Precision: {train_precision[i]:.4f}\")\n",
        "    print(f\"    Recall:    {train_recall[i]:.4f}\")\n",
        "    print(f\"    F1-Score:  {train_f1[i]:.4f}\")\n",
        "    print(f\"  Testing (Held-Out):\")\n",
        "    print(f\"    Precision: {test_precision[i]:.4f}\")\n",
        "    print(f\"    Recall:    {test_recall[i]:.4f}\")\n",
        "    print(f\"    F1-Score:  {test_f1[i]:.4f}\")\n",
        "    print(f\"  Performance Gap:\")\n",
        "    print(f\"    Precision: {train_precision[i]-test_precision[i]:+.4f}\")\n",
        "    print(f\"    Recall:    {train_recall[i]-test_recall[i]:+.4f}\")\n",
        "    print(f\"    F1-Score:  {train_f1[i]-test_f1[i]:+.4f}\")\n",
        "\n",
        "# STEP 11: Tuning Results Summary\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üî¨ HYPERPARAMETER TUNING SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"üéØ BEST PARAMETERS FOUND:\")\n",
        "for param, value in best_params.items():\n",
        "    print(f\"   {param}: {value}\")\n",
        "\n",
        "print(f\"\\nüìä PERFORMANCE COMPARISON:\")\n",
        "print(f\"   Default Model Accuracy: {accuracy_default:.4f}\")\n",
        "print(f\"   Tuned Model Accuracy:   {accuracy_tuned:.4f}\")\n",
        "print(f\"   Improvement:            {improvement:+.4f}\")\n",
        "\n",
        "print(f\"\\nüöÄ FINAL TUNED MODEL PERFORMANCE:\")\n",
        "print(f\"   Training CV Accuracy: {train_accuracy:.4f}\")\n",
        "print(f\"   Test Accuracy:        {test_accuracy:.4f}\")\n",
        "print(f\"   Test ROC-AUC:         {test_roc_auc:.4f}\")\n",
        "print(f\"   Test MCC:             {test_mcc:.4f}\")\n",
        "\n",
        "# STEP 12: Save ROC Data and Feature Importance (LIKE XGBOOST CODE)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üíæ SAVING ROC DATA FOR COMBINED VISUALIZATIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Save the tuned model\n",
        "joblib.dump(best_rf_model, 'random_forest_tuned_model.pkl')\n",
        "joblib.dump(label_encoder, 'random_forest_tuned_label_encoder.pkl')\n",
        "\n",
        "# ============== SAVE ROC PREDICTION DATA ==============\n",
        "print(\"\\nüíæ SAVING ROC PREDICTION DATA FILES...\")\n",
        "\n",
        "# File 1: Save NPZ file for ROC plotting\n",
        "np.savez('random_forest_roc_predictions.npz',\n",
        "         # Training set data (from cross-validation)\n",
        "         y_train_true=y_train_encoded,\n",
        "         y_train_prob=cv_probabilities[:, 1],  # Probability for positive class\n",
        "\n",
        "         # Testing set data\n",
        "         y_test_true=y_test_encoded,\n",
        "         y_test_prob=y_test_proba[:, 1],  # Probability for positive class\n",
        "\n",
        "         # Precomputed ROC curve points\n",
        "         fpr_train=fpr_train,\n",
        "         tpr_train=tpr_train,\n",
        "         fpr_test=fpr_test,\n",
        "         tpr_test=tpr_test,\n",
        "\n",
        "         # AUC values\n",
        "         train_auc=train_roc_auc,\n",
        "         test_auc=test_roc_auc,\n",
        "\n",
        "         # Metadata\n",
        "         model_name='Random Forest',\n",
        "         class_names=class_names,\n",
        "         feature_count=len(feature_columns),\n",
        "         best_params=str(best_params))\n",
        "print(\"‚úÖ Saved: random_forest_roc_predictions.npz\")\n",
        "\n",
        "# File 2: Save detailed predictions CSV\n",
        "predictions_df = pd.DataFrame({\n",
        "    'true_label': y_test_encoded,\n",
        "    'predicted_label': y_test_pred,\n",
        "    'probability_class_0': y_test_proba[:, 0],\n",
        "    'probability_class_1': y_test_proba[:, 1],\n",
        "    'correct': (y_test_encoded == y_test_pred)\n",
        "})\n",
        "predictions_df.to_csv('random_forest_predictions.csv', index=False)\n",
        "print(\"‚úÖ Saved: random_forest_predictions.csv\")\n",
        "\n",
        "# File 3: Save comprehensive metrics CSV\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Model': ['Random Forest'],\n",
        "    'Train_Accuracy': [train_accuracy],\n",
        "    'Test_Accuracy': [test_accuracy],\n",
        "    'Train_AUC': [train_roc_auc],\n",
        "    'Test_AUC': [test_roc_auc],\n",
        "    'Train_Precision_0': [train_precision[0]],\n",
        "    'Train_Precision_1': [train_precision[1]],\n",
        "    'Test_Precision_0': [test_precision[0]],\n",
        "    'Test_Precision_1': [test_precision[1]],\n",
        "    'Train_Recall_0': [train_recall[0]],\n",
        "    'Train_Recall_1': [train_recall[1]],\n",
        "    'Test_Recall_0': [test_recall[0]],\n",
        "    'Test_Recall_1': [test_recall[1]],\n",
        "    'Train_F1_0': [train_f1[0]],\n",
        "    'Train_F1_1': [train_f1[1]],\n",
        "    'Test_F1_0': [test_f1[0]],\n",
        "    'Test_F1_1': [test_f1[1]],\n",
        "    'Train_MCC': [train_mcc],\n",
        "    'Test_MCC': [test_mcc],\n",
        "    'Num_Features': [len(feature_columns)],\n",
        "    'N_Estimators': [best_params.get('n_estimators', 'N/A')],\n",
        "    'Max_Depth': [best_params.get('max_depth', 'N/A')],\n",
        "    'Min_Samples_Split': [best_params.get('min_samples_split', 'N/A')],\n",
        "    'Min_Samples_Leaf': [best_params.get('min_samples_leaf', 'N/A')]\n",
        "})\n",
        "metrics_df.to_csv('random_forest_metrics.csv', index=False)\n",
        "print(\"‚úÖ Saved: random_forest_metrics.csv\")\n",
        "\n",
        "# File 4: Save all predictions (training + testing) for reference\n",
        "all_predictions_df = pd.DataFrame({\n",
        "    'dataset': ['train'] * len(y_train_encoded) + ['test'] * len(y_test_encoded),\n",
        "    'true_label': np.concatenate([y_train_encoded, y_test_encoded]),\n",
        "    'predicted_label': np.concatenate([cv_predictions, y_test_pred]),\n",
        "    'probability_class_1': np.concatenate([cv_probabilities[:, 1], y_test_proba[:, 1]])\n",
        "})\n",
        "all_predictions_df.to_csv('random_forest_all_predictions.csv', index=False)\n",
        "print(\"‚úÖ Saved: random_forest_all_predictions.csv\")\n",
        "\n",
        "# File 5: Save feature importance (Random Forest specific)\n",
        "feature_importances = best_rf_model.feature_importances_\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': feature_columns,\n",
        "    'Importance': feature_importances\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "# Save top 30 features for visualization\n",
        "top_features = importance_df.head(30)\n",
        "importance_df.to_excel('random_forest_feature_importances.xlsx', index=False)\n",
        "top_features.to_excel('random_forest_top30_features.xlsx', index=False)\n",
        "print(\"‚úÖ Saved: random_forest_feature_importances.xlsx\")\n",
        "print(\"‚úÖ Saved: random_forest_top30_features.xlsx\")\n",
        "\n",
        "# Create feature importance visualization\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.barh(range(len(top_features)), top_features['Importance'][::-1],\n",
        "         color=elsevier_colors[0], alpha=0.8, edgecolor='black')\n",
        "plt.yticks(range(len(top_features)), top_features['Feature'][::-1], fontsize=10)\n",
        "plt.xlabel('Feature Importance Score', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Feature', fontsize=14, fontweight='bold')\n",
        "plt.title('Random Forest - Top 30 Feature Importances', fontsize=16, fontweight='bold', pad=20)\n",
        "plt.grid(True, alpha=0.3, linestyle='--', axis='x')\n",
        "plt.tight_layout()\n",
        "plt.savefig('random_forest_feature_importances.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
        "plt.show()\n",
        "\n",
        "# File 6: Save grid search results\n",
        "grid_results_df = pd.DataFrame(grid_search.cv_results_)\n",
        "grid_results_df.to_excel('random_forest_grid_search_results.xlsx', index=False)\n",
        "print(\"‚úÖ Saved: random_forest_grid_search_results.xlsx\")\n",
        "\n",
        "# File 7: Save tuning parameters\n",
        "tuning_params = {\n",
        "    'best_params': best_params,\n",
        "    'best_cv_score': grid_search.best_score_,\n",
        "    'default_accuracy': accuracy_default,\n",
        "    'tuned_accuracy': accuracy_tuned,\n",
        "    'improvement': improvement,\n",
        "    'test_roc_auc': test_roc_auc,\n",
        "    'test_mcc': test_mcc\n",
        "}\n",
        "tuning_params_df = pd.DataFrame([tuning_params])\n",
        "tuning_params_df.to_excel('random_forest_tuning_parameters.xlsx', index=False)\n",
        "\n",
        "print(f\"\\nüéØ RANDOM FOREST WITH HYPERPARAMETER TUNING COMPLETED!\")\n",
        "print(f\"üìä Training (10-Fold CV) Accuracy: {train_accuracy:.1%}\")\n",
        "print(f\"üìä Testing Accuracy: {test_accuracy:.1%}\")\n",
        "print(f\"üìà Training ROC-AUC: {train_roc_auc:.3f}\")\n",
        "print(f\"üìà Testing ROC-AUC: {test_roc_auc:.3f}\")\n",
        "print(f\"‚ö° Improvement over default: {improvement:+.2%}\")\n",
        "print(f\"üîç Performance metrics shown for BOTH classes: {class_names}\")\n",
        "\n",
        "print(f\"\\nüìÅ FILES GENERATED FOR ROC CURVES:\")\n",
        "print(f\"   1. random_forest_roc_predictions.npz     - Main ROC data file\")\n",
        "print(f\"   2. random_forest_predictions.csv         - Detailed test predictions\")\n",
        "print(f\"   3. random_forest_metrics.csv             - Performance metrics\")\n",
        "print(f\"   4. random_forest_all_predictions.csv     - All predictions (train+test)\")\n",
        "print(f\"   5. random_forest_feature_importances.xlsx - Feature importance scores\")\n",
        "print(f\"   6. random_forest_top30_features.xlsx     - Top 30 features\")\n",
        "print(f\"   7. random_forest_tuned_model.pkl         - Trained model\")\n",
        "print(f\"   8. random_forest_grid_search_results.xlsx - Grid search results\")\n",
        "\n",
        "print(f\"\\nüéØ Use 'random_forest_roc_predictions.npz' with other model files for combined ROC plots!\")\n",
        "print(\"üöÄ TUNED RANDOM FOREST MODEL READY FOR DEPLOYMENT!\")"
      ],
      "metadata": {
        "id": "jTD9EEidJ5LA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Support Vector Machine:**"
      ],
      "metadata": {
        "id": "kIxj9KG4KJfB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Requirements:\n",
        "training-set-selected-features.xlsx,\n",
        "testing-set-selected-features.xlsx"
      ],
      "metadata": {
        "id": "6na8ScNMOU6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Intel optimizations (run this once)\n",
        "!pip install scikit-learn-intelex\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "import time\n",
        "\n",
        "# Apply Intel optimizations at the VERY BEGINNING\n",
        "from sklearnex import patch_sklearn\n",
        "patch_sklearn()\n",
        "\n",
        "# Now import sklearn modules - they will be Intel-optimized\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "                           roc_auc_score, roc_curve, matthews_corrcoef, confusion_matrix,\n",
        "                           classification_report)\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, cross_val_predict, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "print(\"üöÄ ENHANCED SVM WITH INTEL OPTIMIZATIONS & EXPANDED PARAMETERS (10-FOLD CV)\")\n",
        "print(\"=\"*80)\n",
        "print(\"‚ö° Intel-optimized scikit-learn + Expanded parameter grid\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Set publication-quality parameters\n",
        "plt.rcParams['font.size'] = 12\n",
        "plt.rcParams['axes.labelsize'] = 14\n",
        "plt.rcParams['axes.titlesize'] = 16\n",
        "plt.rcParams['xtick.labelsize'] = 12\n",
        "plt.rcParams['ytick.labelsize'] = 12\n",
        "plt.rcParams['legend.fontsize'] = 12\n",
        "plt.rcParams['figure.titlesize'] = 18\n",
        "plt.rcParams['font.family'] = ['DejaVu Sans', 'Arial', 'Helvetica', 'sans-serif']\n",
        "\n",
        "# Professional Elsevier color scheme\n",
        "elsevier_colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#1A936F', '#114B5F']\n",
        "\n",
        "# STEP 1: Load the feature-selected datasets\n",
        "print(\"üìä Loading feature-selected datasets...\")\n",
        "\n",
        "train_df = pd.read_excel('training_set_selected_features.xlsx')\n",
        "test_df = pd.read_excel('testing_set_selected_features.xlsx')\n",
        "\n",
        "print(f\"Training set: {train_df.shape}\")\n",
        "print(f\"Testing set: {test_df.shape}\")\n",
        "\n",
        "# STEP 2: Prepare features and target\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üîß PREPARING DATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Identify feature columns (exclude metadata)\n",
        "metadata_columns = ['COMPOUND ID', 'SMILE CODE', 'Ligand Type']\n",
        "feature_columns = [col for col in train_df.columns if col not in metadata_columns]\n",
        "\n",
        "X_train = train_df[feature_columns]\n",
        "y_train = train_df['Ligand Type']\n",
        "\n",
        "X_test = test_df[feature_columns]\n",
        "y_test = test_df['Ligand Type']\n",
        "\n",
        "print(f\"Selected features: {len(feature_columns)}\")\n",
        "print(f\"Training samples: {X_train.shape[0]}\")\n",
        "print(f\"Testing samples: {X_test.shape[0]}\")\n",
        "\n",
        "# Handle missing values if any\n",
        "if X_train.isnull().sum().sum() > 0:\n",
        "    print(\"   ‚ö†Ô∏è  Missing values found - imputing with mean...\")\n",
        "    X_train = X_train.fillna(X_train.mean())\n",
        "    X_test = X_test.fillna(X_test.mean())\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "class_names = [str(cls) for cls in label_encoder.classes_]\n",
        "print(f\"Class names: {class_names}\")\n",
        "\n",
        "print(\"‚úÖ Data prepared successfully!\")\n",
        "\n",
        "# STEP 3: ENHANCED Hyperparameter Tuning with EXPANDED PARAMETERS + INTEL OPTIMIZATIONS\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéØ ENHANCED HYPERPARAMETER TUNING WITH EXPANDED GRID (10-FOLD CV)\")\n",
        "print(\"‚ö° Intel-optimized + More parameters + Faster execution\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# EXPANDED parameter grid - more comprehensive but still manageable\n",
        "enhanced_param_grid = {\n",
        "    'svm__C': [0.01, 0.1, 1, 10, 100],           # Expanded from 3 to 5 values\n",
        "    'svm__kernel': ['linear', 'rbf', 'poly'],     # Added back 'poly' kernel\n",
        "    'svm__gamma': ['scale', 'auto', 0.01, 0.1, 1], # Expanded from 2 to 5 values\n",
        "    'svm__degree': [2, 3]                         # Added for poly kernel\n",
        "}\n",
        "\n",
        "# Create pipeline with Intel-optimized SVM\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('svm', SVC(\n",
        "        probability=True,\n",
        "        random_state=42,\n",
        "        max_iter=5000,    # Prevent endless runs\n",
        "        tol=1e-3          # Faster convergence\n",
        "    ))\n",
        "])\n",
        "\n",
        "# 10-fold cross-validation strategy\n",
        "cv_strategy = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# Calculate enhanced metrics\n",
        "total_combinations = (len(enhanced_param_grid['svm__C']) *\n",
        "                     len(enhanced_param_grid['svm__kernel']) *\n",
        "                     len(enhanced_param_grid['svm__gamma']) *\n",
        "                     len(enhanced_param_grid['svm__degree']))\n",
        "total_fits = total_combinations * 10\n",
        "\n",
        "print(\"üîç Performing ENHANCED Grid Search with 10-fold CV...\")\n",
        "print(f\"   ENHANCED Parameter grid: {enhanced_param_grid}\")\n",
        "print(f\"   Total parameter combinations: {total_combinations}\")\n",
        "print(f\"   Total fits to perform: {total_fits}\")\n",
        "print(f\"   ‚ö° Intel optimizations make this feasible despite larger grid!\")\n",
        "print(f\"   üéØ More comprehensive search for better performance!\")\n",
        "\n",
        "# GridSearchCV with parallel processing\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=pipeline,\n",
        "    param_grid=enhanced_param_grid,\n",
        "    cv=cv_strategy,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,           # Use all available cores\n",
        "    verbose=1,\n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "# Perform ENHANCED grid search with timing\n",
        "print(f\"\\nüïí Starting ENHANCED grid search at {time.strftime('%H:%M:%S')}\")\n",
        "start_time = time.time()\n",
        "\n",
        "grid_search.fit(X_train, y_train_encoded)\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "\n",
        "print(f\"‚úÖ ENHANCED Grid Search completed in {execution_time:.2f} seconds ({execution_time/60:.2f} minutes)!\")\n",
        "print(\"‚ö° Intel optimizations + expanded parameters successfully executed!\")\n",
        "\n",
        "# Display best parameters and scores\n",
        "print(f\"\\nüéØ BEST PARAMETERS FOUND:\")\n",
        "best_params = grid_search.best_params_\n",
        "for param, value in best_params.items():\n",
        "    print(f\"   {param}: {value}\")\n",
        "print(f\"   Best CV Score (10-fold): {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Analyze grid search results\n",
        "print(f\"\\nüìä GRID SEARCH ANALYSIS:\")\n",
        "results_df = pd.DataFrame(grid_search.cv_results_)\n",
        "top_5_results = results_df.nlargest(5, 'mean_test_score')[['params', 'mean_test_score', 'std_test_score']]\n",
        "print(\"Top 5 parameter combinations:\")\n",
        "for i, (idx, row) in enumerate(top_5_results.iterrows()):\n",
        "    print(f\"   {i+1}. Score: {row['mean_test_score']:.4f} ¬± {row['std_test_score']:.4f}\")\n",
        "    print(f\"      Params: {row['params']}\")\n",
        "\n",
        "# STEP 4: Compare Default vs Enhanced Tuned Models\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä COMPARISON: DEFAULT vs ENHANCED TUNED MODEL\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Train model with default parameters for comparison\n",
        "default_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('svm', SVC(probability=True, random_state=42, max_iter=5000, tol=1e-3))\n",
        "])\n",
        "\n",
        "default_start = time.time()\n",
        "default_pipeline.fit(X_train, y_train_encoded)\n",
        "default_end = time.time()\n",
        "y_pred_default = default_pipeline.predict(X_test)\n",
        "accuracy_default = accuracy_score(y_test_encoded, y_pred_default)\n",
        "\n",
        "# Use best model from enhanced grid search\n",
        "best_svm_model = grid_search.best_estimator_\n",
        "tuned_start = time.time()\n",
        "y_pred_tuned = best_svm_model.predict(X_test)\n",
        "tuned_end = time.time()\n",
        "accuracy_tuned = accuracy_score(y_test_encoded, y_pred_tuned)\n",
        "\n",
        "improvement = accuracy_tuned - accuracy_default\n",
        "default_time = default_end - default_start\n",
        "tuned_time = tuned_end - tuned_start\n",
        "\n",
        "print(f\"   Default Parameters Accuracy: {accuracy_default:.4f}\")\n",
        "print(f\"   Enhanced Tuned Accuracy:     {accuracy_tuned:.4f}\")\n",
        "print(f\"   Improvement:                 {improvement:+.4f}\")\n",
        "print(f\"   Default training time: {default_time:.2f}s\")\n",
        "print(f\"   Tuned prediction time: {tuned_time:.2f}s\")\n",
        "\n",
        "if improvement > 0:\n",
        "    print(f\"   ‚úÖ Enhanced tuning improved accuracy by {improvement:.2%}\")\n",
        "else:\n",
        "    print(f\"   ‚ö†Ô∏è  Enhanced tuning did not improve accuracy\")\n",
        "\n",
        "# STEP 5: 10-Fold Cross-Validation with Enhanced Tuned Model\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéØ 10-FOLD CROSS-VALIDATION WITH ENHANCED TUNED PARAMETERS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"üöÄ Performing 10-fold cross-validation with enhanced tuned parameters...\")\n",
        "\n",
        "cv_start = time.time()\n",
        "\n",
        "# Get cross-validated predictions\n",
        "cv_predictions = cross_val_predict(best_svm_model, X_train, y_train_encoded,\n",
        "                                 cv=cv_strategy, method='predict', n_jobs=-1)\n",
        "cv_probabilities = cross_val_predict(best_svm_model, X_train, y_train_encoded,\n",
        "                                   cv=cv_strategy, method='predict_proba', n_jobs=-1)\n",
        "\n",
        "cv_end = time.time()\n",
        "\n",
        "print(f\"‚úÖ 10-fold cross-validation completed in {cv_end - cv_start:.2f} seconds!\")\n",
        "\n",
        "# Calculate training metrics\n",
        "train_accuracy = accuracy_score(y_train_encoded, cv_predictions)\n",
        "train_precision = precision_score(y_train_encoded, cv_predictions, average=None)\n",
        "train_recall = recall_score(y_train_encoded, cv_predictions, average=None)\n",
        "train_f1 = f1_score(y_train_encoded, cv_predictions, average=None)\n",
        "train_roc_auc = roc_auc_score(y_train_encoded, cv_probabilities[:, 1])\n",
        "train_mcc = matthews_corrcoef(y_train_encoded, cv_predictions)\n",
        "\n",
        "# Individual fold accuracies\n",
        "cv_scores = cross_val_score(best_svm_model, X_train, y_train_encoded,\n",
        "                          cv=cv_strategy, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "print(f\"\\nüìä 10-FOLD CROSS-VALIDATION RESULTS (TRAINING PERFORMANCE):\")\n",
        "print(f\"   Fold scores: {[f'{score:.4f}' for score in cv_scores]}\")\n",
        "print(f\"   Mean CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
        "print(f\"   Standard Deviation: {cv_scores.std():.4f}\")\n",
        "\n",
        "# STEP 6: Make predictions on TEST set with enhanced tuned model\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä MODEL EVALUATION WITH ENHANCED TUNED PARAMETERS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Predictions on TEST set\n",
        "y_test_pred = best_svm_model.predict(X_test)\n",
        "y_test_proba = best_svm_model.predict_proba(X_test)\n",
        "\n",
        "# Calculate metrics for TESTING set\n",
        "test_accuracy = accuracy_score(y_test_encoded, y_test_pred)\n",
        "test_precision = precision_score(y_test_encoded, y_test_pred, average=None)\n",
        "test_recall = recall_score(y_test_encoded, y_test_pred, average=None)\n",
        "test_f1 = f1_score(y_test_encoded, y_test_pred, average=None)\n",
        "test_roc_auc = roc_auc_score(y_test_encoded, y_test_proba[:, 1])\n",
        "test_mcc = matthews_corrcoef(y_test_encoded, y_test_pred)\n",
        "\n",
        "# Confusion matrices\n",
        "train_cm = confusion_matrix(y_train_encoded, cv_predictions)\n",
        "test_cm = confusion_matrix(y_test_encoded, y_test_pred)\n",
        "\n",
        "# STEP 7: Display comprehensive results\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìà COMPREHENSIVE PERFORMANCE METRICS (ENHANCED TUNED MODEL)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create detailed comparison dataframe\n",
        "class_performance_data = []\n",
        "\n",
        "for i, class_name in enumerate(class_names):\n",
        "    class_performance_data.extend([\n",
        "        {\n",
        "            'Class': class_name,\n",
        "            'Metric': 'Precision',\n",
        "            'Training (10-Fold CV)': f\"{train_precision[i]:.4f}\",\n",
        "            'Testing': f\"{test_precision[i]:.4f}\",\n",
        "            'Difference': f\"{train_precision[i]-test_precision[i]:+.4f}\"\n",
        "        },\n",
        "        {\n",
        "            'Class': class_name,\n",
        "            'Metric': 'Recall',\n",
        "            'Training (10-Fold CV)': f\"{train_recall[i]:.4f}\",\n",
        "            'Testing': f\"{test_recall[i]:.4f}\",\n",
        "            'Difference': f\"{train_recall[i]-test_recall[i]:+.4f}\"\n",
        "        },\n",
        "        {\n",
        "            'Class': class_name,\n",
        "            'Metric': 'F1-Score',\n",
        "            'Training (10-Fold CV)': f\"{train_f1[i]:.4f}\",\n",
        "            'Testing': f\"{test_f1[i]:.4f}\",\n",
        "            'Difference': f\"{train_f1[i]-test_f1[i]:+.4f}\"\n",
        "        }\n",
        "    ])\n",
        "\n",
        "# Add overall metrics\n",
        "overall_metrics = [\n",
        "    {\n",
        "        'Class': 'Overall',\n",
        "        'Metric': 'Accuracy',\n",
        "        'Training (10-Fold CV)': f\"{train_accuracy:.4f}\",\n",
        "        'Testing': f\"{test_accuracy:.4f}\",\n",
        "        'Difference': f\"{train_accuracy-test_accuracy:+.4f}\"\n",
        "    },\n",
        "    {\n",
        "        'Class': 'Overall',\n",
        "        'Metric': 'ROC-AUC',\n",
        "        'Training (10-Fold CV)': f\"{train_roc_auc:.4f}\",\n",
        "        'Testing': f\"{test_roc_auc:.4f}\",\n",
        "        'Difference': f\"{train_roc_auc-test_roc_auc:+.4f}\"\n",
        "    },\n",
        "    {\n",
        "        'Class': 'Overall',\n",
        "        'Metric': 'MCC',\n",
        "        'Training (10-Fold CV)': f\"{train_mcc:.4f}\",\n",
        "        'Testing': f\"{test_mcc:.4f}\",\n",
        "        'Difference': f\"{train_mcc-test_mcc:+.4f}\"\n",
        "    }\n",
        "]\n",
        "\n",
        "performance_df = pd.DataFrame(class_performance_data + overall_metrics)\n",
        "print(performance_df.to_string(index=False))\n",
        "\n",
        "# STEP 8: Enhanced Performance Visualization\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä GENERATING ENHANCED PERFORMANCE VISUALIZATIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create comprehensive performance comparison\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 16))\n",
        "\n",
        "# 1. Main Performance Comparison\n",
        "overall_metrics = ['Accuracy', 'ROC-AUC', 'MCC']\n",
        "train_overall = [train_accuracy, train_roc_auc, train_mcc]\n",
        "test_overall = [test_accuracy, test_roc_auc, test_mcc]\n",
        "\n",
        "x = np.arange(len(overall_metrics))\n",
        "width = 0.35\n",
        "\n",
        "bars1 = ax1.bar(x - width/2, train_overall, width, label='Training (10-Fold CV)',\n",
        "               color=elsevier_colors[0], alpha=0.8, edgecolor='black', linewidth=1.5)\n",
        "bars2 = ax1.bar(x + width/2, test_overall, width, label='Testing (Held-Out)',\n",
        "               color=elsevier_colors[1], alpha=0.8, edgecolor='black', linewidth=1.5)\n",
        "\n",
        "for bars in [bars1, bars2]:\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                f'{height:.3f}', ha='center', va='bottom',\n",
        "                fontweight='bold', fontsize=11)\n",
        "\n",
        "ax1.set_xlabel('Performance Metrics', fontsize=14, fontweight='bold')\n",
        "ax1.set_ylabel('Score', fontsize=14, fontweight='bold')\n",
        "ax1.set_title('ENHANCED SVM (Tuned) - Overall Performance\\nTraining vs Testing',\n",
        "              fontsize=16, fontweight='bold', pad=20)\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(overall_metrics)\n",
        "ax1.legend(fontsize=12, framealpha=0.9)\n",
        "ax1.set_ylim(0, 1.1)\n",
        "ax1.grid(True, alpha=0.3, linestyle='--')\n",
        "\n",
        "# 2. Class-wise Precision Comparison\n",
        "class_metrics = ['Precision', 'Recall', 'F1-Score']\n",
        "x_class = np.arange(len(class_metrics))\n",
        "width_class = 0.25\n",
        "\n",
        "for i, class_name in enumerate(class_names):\n",
        "    offset = width_class * (i - 0.5)\n",
        "    class_train_scores = [train_precision[i], train_recall[i], train_f1[i]]\n",
        "    class_test_scores = [test_precision[i], test_recall[i], test_f1[i]]\n",
        "\n",
        "    ax2.bar(x_class + offset, class_train_scores, width_class,\n",
        "            label=f'{class_name} (Train)', alpha=0.7,\n",
        "            color=elsevier_colors[i*2], edgecolor='black')\n",
        "    ax2.bar(x_class + offset + width_class, class_test_scores, width_class,\n",
        "            label=f'{class_name} (Test)', alpha=0.7,\n",
        "            color=elsevier_colors[i*2+1], edgecolor='black')\n",
        "\n",
        "ax2.set_xlabel('Metrics', fontsize=14, fontweight='bold')\n",
        "ax2.set_ylabel('Score', fontsize=14, fontweight='bold')\n",
        "ax2.set_title('ENHANCED SVM (Tuned) - Class-wise Performance\\nTraining vs Testing',\n",
        "              fontsize=16, fontweight='bold', pad=20)\n",
        "ax2.set_xticks(x_class + width_class/2)\n",
        "ax2.set_xticklabels(class_metrics)\n",
        "ax2.legend(fontsize=10, framealpha=0.9, ncol=2)\n",
        "ax2.set_ylim(0, 1.1)\n",
        "ax2.grid(True, alpha=0.3, linestyle='--')\n",
        "\n",
        "# 3. Confusion Matrix - Training\n",
        "im1 = ax3.imshow(train_cm, cmap='Blues', interpolation='nearest', alpha=0.8)\n",
        "ax3.set_xlabel('Predicted Label', fontsize=14, fontweight='bold')\n",
        "ax3.set_ylabel('True Label', fontsize=14, fontweight='bold')\n",
        "ax3.set_title('Training Set - Confusion Matrix\\n(10-Fold CV Average)',\n",
        "              fontsize=16, fontweight='bold', pad=20)\n",
        "ax3.set_xticks([0, 1])\n",
        "ax3.set_yticks([0, 1])\n",
        "ax3.set_xticklabels(class_names)\n",
        "ax3.set_yticklabels(class_names)\n",
        "\n",
        "for i in range(train_cm.shape[0]):\n",
        "    for j in range(train_cm.shape[1]):\n",
        "        ax3.text(j, i, f'{train_cm[i, j]}\\n({train_cm[i, j]/np.sum(train_cm):.1%})',\n",
        "                ha='center', va='center', fontsize=12, fontweight='bold',\n",
        "                color='white' if train_cm[i, j] > np.max(train_cm)/2 else 'black')\n",
        "\n",
        "# 4. Confusion Matrix - Testing\n",
        "im2 = ax4.imshow(test_cm, cmap='Reds', interpolation='nearest', alpha=0.8)\n",
        "ax4.set_xlabel('Predicted Label', fontsize=14, fontweight='bold')\n",
        "ax4.set_ylabel('True Label', fontsize=14, fontweight='bold')\n",
        "ax4.set_title('Testing Set - Confusion Matrix\\n(Held-Out Dataset)',\n",
        "              fontsize=16, fontweight='bold', pad=20)\n",
        "ax4.set_xticks([0, 1])\n",
        "ax4.set_yticks([0, 1])\n",
        "ax4.set_xticklabels(class_names)\n",
        "ax4.set_yticklabels(class_names)\n",
        "\n",
        "for i in range(test_cm.shape[0]):\n",
        "    for j in range(test_cm.shape[1]):\n",
        "        ax4.text(j, i, f'{test_cm[i, j]}\\n({test_cm[i, j]/np.sum(test_cm):.1%})',\n",
        "                ha='center', va='center', fontsize=12, fontweight='bold',\n",
        "                color='white' if test_cm[i, j] > np.max(test_cm)/2 else 'black')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('enhanced_svm_tuned_comprehensive_performance.png', dpi=300,\n",
        "            bbox_inches='tight', facecolor='white')\n",
        "plt.show()\n",
        "\n",
        "# STEP 9: Classification Reports\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìã DETAILED CLASSIFICATION REPORTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nTRAINING SET CLASSIFICATION REPORT (10-Fold CV Average):\")\n",
        "training_report = classification_report(y_train_encoded, cv_predictions,\n",
        "                                      target_names=class_names, digits=4)\n",
        "print(training_report)\n",
        "\n",
        "print(\"\\nTESTING SET CLASSIFICATION REPORT (Held-Out):\")\n",
        "testing_report = classification_report(y_test_encoded, y_test_pred,\n",
        "                                     target_names=class_names, digits=4)\n",
        "print(testing_report)\n",
        "\n",
        "# STEP 10: Enhanced Tuning Results Summary\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üî¨ ENHANCED HYPERPARAMETER TUNING SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"üéØ BEST PARAMETERS FOUND:\")\n",
        "for param, value in best_params.items():\n",
        "    print(f\"   {param}: {value}\")\n",
        "\n",
        "print(f\"\\nüìä PERFORMANCE COMPARISON:\")\n",
        "print(f\"   Default Model Accuracy: {accuracy_default:.4f}\")\n",
        "print(f\"   Enhanced Tuned Accuracy: {accuracy_tuned:.4f}\")\n",
        "print(f\"   Improvement:            {improvement:+.4f}\")\n",
        "\n",
        "print(f\"\\n‚ö° ENHANCEMENTS APPLIED:\")\n",
        "print(f\"   ‚Ä¢ Intel optimizations: 2-10x speedup\")\n",
        "print(f\"   ‚Ä¢ Expanded parameter grid: {total_combinations} combinations\")\n",
        "print(f\"   ‚Ä¢ Added 'poly' kernel with degree parameter\")\n",
        "print(f\"   ‚Ä¢ More C values: [0.01, 0.1, 1, 10, 100]\")\n",
        "print(f\"   ‚Ä¢ More gamma values: ['scale', 'auto', 0.01, 0.1, 1]\")\n",
        "print(f\"   ‚Ä¢ Total execution time: {execution_time:.2f}s\")\n",
        "\n",
        "print(f\"\\nüöÄ FINAL ENHANCED TUNED MODEL PERFORMANCE:\")\n",
        "print(f\"   Training CV Accuracy: {train_accuracy:.4f}\")\n",
        "print(f\"   Test Accuracy:        {test_accuracy:.4f}\")\n",
        "print(f\"   Test ROC-AUC:         {test_roc_auc:.4f}\")\n",
        "print(f\"   Test MCC:             {test_mcc:.4f}\")\n",
        "\n",
        "# STEP 11: Save enhanced model and results\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üíæ SAVING ENHANCED MODEL AND RESULTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Save the enhanced model\n",
        "joblib.dump(best_svm_model, 'enhanced_svm_tuned_model.pkl')\n",
        "joblib.dump(label_encoder, 'enhanced_svm_tuned_label_encoder.pkl')\n",
        "\n",
        "# Save predictions with probabilities\n",
        "test_results_df = test_df.copy()\n",
        "test_results_df['Predicted_Class'] = label_encoder.inverse_transform(y_test_pred)\n",
        "test_results_df['Prediction_Probability_0'] = y_test_proba[:, 0]\n",
        "test_results_df['Prediction_Probability_1'] = y_test_proba[:, 1]\n",
        "test_results_df['Prediction_Correct'] = (y_test_encoded == y_test_pred)\n",
        "\n",
        "test_results_df.to_excel('enhanced_svm_tuned_testing_predictions.xlsx', index=False)\n",
        "\n",
        "# Save enhanced tuning parameters\n",
        "tuning_params = {\n",
        "    'best_params': best_params,\n",
        "    'best_cv_score': grid_search.best_score_,\n",
        "    'default_accuracy': accuracy_default,\n",
        "    'tuned_accuracy': accuracy_tuned,\n",
        "    'improvement': improvement,\n",
        "    'test_roc_auc': test_roc_auc,\n",
        "    'test_mcc': test_mcc,\n",
        "    'enhancements': [\n",
        "        'Intel optimizations (2-10x speedup)',\n",
        "        'Expanded parameter grid',\n",
        "        'Added poly kernel with degree',\n",
        "        'More C and gamma values',\n",
        "        '10-fold cross-validation'\n",
        "    ],\n",
        "    'execution_time_seconds': execution_time,\n",
        "    'total_combinations': total_combinations\n",
        "}\n",
        "tuning_params_df = pd.DataFrame([tuning_params])\n",
        "tuning_params_df.to_excel('enhanced_svm_tuning_parameters.xlsx', index=False)\n",
        "\n",
        "# Save grid search results\n",
        "grid_results_df = pd.DataFrame(grid_search.cv_results_)\n",
        "grid_results_df.to_excel('enhanced_svm_grid_search_results.xlsx', index=False)\n",
        "\n",
        "# Save top 10 parameter combinations\n",
        "top_10_results = results_df.nlargest(10, 'mean_test_score')[\n",
        "    ['mean_test_score', 'std_test_score', 'params']\n",
        "]\n",
        "top_10_results.to_excel('enhanced_svm_top_10_parameters.xlsx', index=False)\n",
        "\n",
        "# ============== ADD THIS SECTION: SAVE ROC DATA ==============\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üíæ SAVING ROC DATA FOR COMBINED VISUALIZATIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Compute ROC curve points for SVM (required for NPZ file)\n",
        "fpr_train_svm, tpr_train_svm, _ = roc_curve(y_train_encoded, cv_probabilities[:, 1])\n",
        "fpr_test_svm, tpr_test_svm, _ = roc_curve(y_test_encoded, y_test_proba[:, 1])\n",
        "\n",
        "print(\"\\nüíæ SAVING ROC PREDICTION DATA FILES...\")\n",
        "\n",
        "# File 1: Save NPZ file for ROC plotting (MATCHES XGBoost FORMAT)\n",
        "np.savez('enhanced_svm_roc_predictions.npz',\n",
        "         # Training set data (from cross-validation)\n",
        "         y_train_true=y_train_encoded,\n",
        "         y_train_prob=cv_probabilities[:, 1],  # Probability for positive class\n",
        "\n",
        "         # Testing set data\n",
        "         y_test_true=y_test_encoded,\n",
        "         y_test_prob=y_test_proba[:, 1],  # Probability for positive class\n",
        "\n",
        "         # Precomputed ROC curve points\n",
        "         fpr_train=fpr_train_svm,\n",
        "         tpr_train=tpr_train_svm,\n",
        "         fpr_test=fpr_test_svm,\n",
        "         tpr_test=tpr_test_svm,\n",
        "\n",
        "         # AUC values\n",
        "         train_auc=train_roc_auc,\n",
        "         test_auc=test_roc_auc,\n",
        "\n",
        "         # Metadata\n",
        "         model_name='Enhanced_SVM',\n",
        "         class_names=class_names,\n",
        "         feature_count=len(feature_columns),\n",
        "         best_params=str(best_params))\n",
        "print(\"‚úÖ Saved: enhanced_svm_roc_predictions.npz\")\n",
        "\n",
        "# File 2: Save detailed predictions CSV\n",
        "predictions_df = pd.DataFrame({\n",
        "    'true_label': y_test_encoded,\n",
        "    'predicted_label': y_test_pred,\n",
        "    'probability_class_0': y_test_proba[:, 0],\n",
        "    'probability_class_1': y_test_proba[:, 1],\n",
        "    'correct': (y_test_encoded == y_test_pred)\n",
        "})\n",
        "predictions_df.to_csv('enhanced_svm_predictions.csv', index=False)\n",
        "print(\"‚úÖ Saved: enhanced_svm_predictions.csv\")\n",
        "\n",
        "# File 3: Save comprehensive metrics CSV\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Model': ['Enhanced_SVM'],\n",
        "    'Train_Accuracy': [train_accuracy],\n",
        "    'Test_Accuracy': [test_accuracy],\n",
        "    'Train_AUC': [train_roc_auc],\n",
        "    'Test_AUC': [test_roc_auc],\n",
        "    'Train_Precision_0': [train_precision[0]],\n",
        "    'Train_Precision_1': [train_precision[1]],\n",
        "    'Test_Precision_0': [test_precision[0]],\n",
        "    'Test_Precision_1': [test_precision[1]],\n",
        "    'Train_Recall_0': [train_recall[0]],\n",
        "    'Train_Recall_1': [train_recall[1]],\n",
        "    'Test_Recall_0': [test_recall[0]],\n",
        "    'Test_Recall_1': [test_recall[1]],\n",
        "    'Train_F1_0': [train_f1[0]],\n",
        "    'Train_F1_1': [train_f1[1]],\n",
        "    'Test_F1_0': [test_f1[0]],\n",
        "    'Test_F1_1': [test_f1[1]],\n",
        "    'Train_MCC': [train_mcc],\n",
        "    'Test_MCC': [test_mcc],\n",
        "    'Num_Features': [len(feature_columns)],\n",
        "    'SVM_C': [best_params.get('svm__C', 'N/A')],\n",
        "    'SVM_Kernel': [best_params.get('svm__kernel', 'N/A')],\n",
        "    'SVM_Gamma': [best_params.get('svm__gamma', 'N/A')]\n",
        "})\n",
        "metrics_df.to_csv('enhanced_svm_metrics.csv', index=False)\n",
        "print(\"‚úÖ Saved: enhanced_svm_metrics.csv\")\n",
        "\n",
        "# File 4: Save all predictions (training + testing) for reference\n",
        "all_predictions_df = pd.DataFrame({\n",
        "    'dataset': ['train'] * len(y_train_encoded) + ['test'] * len(y_test_encoded),\n",
        "    'true_label': np.concatenate([y_train_encoded, y_test_encoded]),\n",
        "    'predicted_label': np.concatenate([cv_predictions, y_test_pred]),\n",
        "    'probability_class_1': np.concatenate([cv_probabilities[:, 1], y_test_proba[:, 1]])\n",
        "})\n",
        "all_predictions_df.to_csv('enhanced_svm_all_predictions.csv', index=False)\n",
        "print(\"‚úÖ Saved: enhanced_svm_all_predictions.csv\")\n",
        "\n",
        "print(f\"\\nüìÅ NEW ROC DATA FILES GENERATED:\")\n",
        "print(f\"   1. enhanced_svm_roc_predictions.npz - Main ROC data file (matches XGBoost format)\")\n",
        "print(f\"   2. enhanced_svm_predictions.csv     - Detailed test predictions\")\n",
        "print(f\"   3. enhanced_svm_metrics.csv         - Performance metrics\")\n",
        "print(f\"   4. enhanced_svm_all_predictions.csv - All predictions (train+test)\")\n",
        "print(f\"üéØ Use 'enhanced_svm_roc_predictions.npz' with other model files for combined ROC plots!\")\n",
        "# ============== END OF ADDED SECTION ==============\n",
        "\n",
        "print(\"‚úÖ ENHANCED FILES SAVED:\")\n",
        "print(f\"   ‚Ä¢ enhanced_svm_tuned_model.pkl\")\n",
        "print(f\"   ‚Ä¢ enhanced_svm_tuned_label_encoder.pkl\")\n",
        "print(f\"   ‚Ä¢ enhanced_svm_tuned_testing_predictions.xlsx\")\n",
        "print(f\"   ‚Ä¢ enhanced_svm_tuning_parameters.xlsx\")\n",
        "print(f\"   ‚Ä¢ enhanced_svm_grid_search_results.xlsx\")\n",
        "print(f\"   ‚Ä¢ enhanced_svm_top_10_parameters.xlsx\")\n",
        "print(f\"   ‚Ä¢ enhanced_svm_tuned_comprehensive_performance.png\")\n",
        "\n",
        "print(f\"\\nüéØ ENHANCED SUPPORT VECTOR MACHINE COMPLETED!\")\n",
        "print(f\"‚ö° Intel optimizations + Expanded parameters applied!\")\n",
        "print(f\"üìä Training (10-Fold CV) Accuracy: {train_accuracy:.1%}\")\n",
        "print(f\"üìä Testing Accuracy: {test_accuracy:.1%}\")\n",
        "print(f\"üìà ROC-AUC: {test_roc_auc:.3f}\")\n",
        "print(f\"‚ö° Improvement over default: {improvement:+.2%}\")\n",
        "print(f\"üîç Comprehensive parameter search completed!\")\n",
        "print(\"üöÄ ENHANCED TUNED MODEL READY FOR DEPLOYMENT!\")"
      ],
      "metadata": {
        "id": "N7_i9IP0KNIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Multi-Layer Perceptron / MLP Classifier:**"
      ],
      "metadata": {
        "id": "hrxeF-mWKSDC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Requirements: training-set-selected-features.xlsx, testing-set-selected-features.xlsx"
      ],
      "metadata": {
        "id": "n0ET0eILOeK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "                           roc_auc_score, roc_curve, matthews_corrcoef, confusion_matrix,\n",
        "                           classification_report)\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, cross_val_predict, GridSearchCV\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "\n",
        "# Set publication-quality parameters (Fixed font settings)\n",
        "plt.rcParams['font.size'] = 12\n",
        "plt.rcParams['axes.labelsize'] = 14\n",
        "plt.rcParams['axes.titlesize'] = 16\n",
        "plt.rcParams['xtick.labelsize'] = 12\n",
        "plt.rcParams['ytick.labelsize'] = 12\n",
        "plt.rcParams['legend.fontsize'] = 12\n",
        "plt.rcParams['figure.titlesize'] = 18\n",
        "\n",
        "# Use available system fonts to avoid warnings\n",
        "plt.rcParams['font.family'] = ['DejaVu Sans', 'Arial', 'Helvetica', 'sans-serif']\n",
        "\n",
        "# Professional Elsevier color scheme\n",
        "elsevier_colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#1A936F', '#114B5F']\n",
        "\n",
        "print(\"üöÄ MLP CLASSIFIER WITH HYPERPARAMETER TUNING (10-FOLD CV + GRID SEARCH)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# STEP 1: Load the feature-selected datasets\n",
        "print(\"üìä Loading feature-selected datasets...\")\n",
        "\n",
        "train_df = pd.read_excel('training_set_selected_features.xlsx')\n",
        "test_df = pd.read_excel('testing_set_selected_features.xlsx')\n",
        "\n",
        "print(f\"Training set: {train_df.shape}\")\n",
        "print(f\"Testing set: {test_df.shape}\")\n",
        "\n",
        "# STEP 2: Prepare features and target\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üîß PREPARING DATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Identify feature columns (exclude metadata)\n",
        "metadata_columns = ['COMPOUND ID', 'SMILE CODE', 'Ligand Type']\n",
        "feature_columns = [col for col in train_df.columns if col not in metadata_columns]\n",
        "\n",
        "X_train = train_df[feature_columns]\n",
        "y_train = train_df['Ligand Type']\n",
        "\n",
        "X_test = test_df[feature_columns]\n",
        "y_test = test_df['Ligand Type']\n",
        "\n",
        "print(f\"Selected features: {len(feature_columns)}\")\n",
        "print(f\"Training samples: {X_train.shape[0]}\")\n",
        "print(f\"Testing samples: {X_test.shape[0]}\")\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# FIXED: Properly get class names as strings\n",
        "class_names = [str(cls) for cls in label_encoder.classes_]\n",
        "print(f\"Class names: {class_names}\")\n",
        "\n",
        "# Scale features for neural network (important for MLP)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"‚úÖ Data prepared and scaled successfully!\")\n",
        "\n",
        "# STEP 3: Hyperparameter Tuning with GridSearchCV and 10-Fold CV\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéØ HYPERPARAMETER TUNING WITH GRIDSEARCHCV (10-FOLD CV)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Define parameter grid for MLPClassifier\n",
        "param_grid = {\n",
        "    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],\n",
        "    'activation': ['relu', 'tanh'],\n",
        "    'alpha': [0.0001, 0.001, 0.01],\n",
        "    'learning_rate': ['constant', 'adaptive'],\n",
        "    'learning_rate_init': [0.001, 0.01],\n",
        "    'max_iter': [500, 1000]\n",
        "}\n",
        "\n",
        "# Initialize MLPClassifier\n",
        "mlp = MLPClassifier(random_state=42, early_stopping=True, validation_fraction=0.1)\n",
        "\n",
        "# 10-fold cross-validation strategy\n",
        "cv_strategy = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# GridSearchCV with 10-fold CV\n",
        "print(\"üîç Performing Grid Search with 10-fold CV...\")\n",
        "print(f\"   Parameter grid: {param_grid}\")\n",
        "print(f\"   Total parameter combinations: {len(param_grid['hidden_layer_sizes']) * len(param_grid['activation']) * len(param_grid['alpha']) * len(param_grid['learning_rate']) * len(param_grid['learning_rate_init']) * len(param_grid['max_iter'])}\")\n",
        "print(f\"   This may take a while...\")\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=mlp,\n",
        "    param_grid=param_grid,\n",
        "    cv=cv_strategy,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Perform grid search\n",
        "grid_search.fit(X_train_scaled, y_train_encoded)\n",
        "\n",
        "print(\"‚úÖ Grid Search completed!\")\n",
        "\n",
        "# Display best parameters and scores\n",
        "print(f\"\\nüéØ BEST PARAMETERS FOUND:\")\n",
        "best_params = grid_search.best_params_\n",
        "for param, value in best_params.items():\n",
        "    print(f\"   {param}: {value}\")\n",
        "print(f\"   Best CV Score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# STEP 4: Compare Default vs Tuned Models\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä COMPARISON: DEFAULT vs TUNED PARAMETERS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Train model with default parameters for comparison\n",
        "mlp_default = MLPClassifier(random_state=42, early_stopping=True, validation_fraction=0.1)\n",
        "mlp_default.fit(X_train_scaled, y_train_encoded)\n",
        "y_pred_default = mlp_default.predict(X_test_scaled)\n",
        "accuracy_default = accuracy_score(y_test_encoded, y_pred_default)\n",
        "\n",
        "# Use best model from grid search\n",
        "best_mlp_model = grid_search.best_estimator_\n",
        "y_pred_tuned = best_mlp_model.predict(X_test_scaled)\n",
        "accuracy_tuned = accuracy_score(y_test_encoded, y_pred_tuned)\n",
        "\n",
        "improvement = accuracy_tuned - accuracy_default\n",
        "\n",
        "print(f\"   Default Parameters Accuracy: {accuracy_default:.4f}\")\n",
        "print(f\"   Tuned Parameters Accuracy:   {accuracy_tuned:.4f}\")\n",
        "print(f\"   Improvement:                 {improvement:+.4f}\")\n",
        "\n",
        "if improvement > 0:\n",
        "    print(f\"   ‚úÖ Tuning improved accuracy by {improvement:.2%}\")\n",
        "else:\n",
        "    print(f\"   ‚ö†Ô∏è  Tuning did not improve accuracy\")\n",
        "\n",
        "# STEP 5: 10-Fold Cross-Validation with Tuned Model (for proper comparison)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéØ 10-FOLD CROSS-VALIDATION WITH TUNED PARAMETERS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"üöÄ Performing 10-fold cross-validation with tuned parameters...\")\n",
        "\n",
        "# Get cross-validated predictions for training set using tuned parameters\n",
        "cv_predictions = cross_val_predict(best_mlp_model, X_train_scaled, y_train_encoded,\n",
        "                                 cv=cv_strategy, method='predict')\n",
        "cv_probabilities = cross_val_predict(best_mlp_model, X_train_scaled, y_train_encoded,\n",
        "                                   cv=cv_strategy, method='predict_proba')\n",
        "\n",
        "# Calculate training metrics as average across 10-fold CV\n",
        "train_accuracy = accuracy_score(y_train_encoded, cv_predictions)\n",
        "\n",
        "# Calculate metrics for BOTH CLASSES\n",
        "train_precision = precision_score(y_train_encoded, cv_predictions, average=None)\n",
        "train_recall = recall_score(y_train_encoded, cv_predictions, average=None)\n",
        "train_f1 = f1_score(y_train_encoded, cv_predictions, average=None)\n",
        "train_roc_auc = roc_auc_score(y_train_encoded, cv_probabilities[:, 1])\n",
        "train_mcc = matthews_corrcoef(y_train_encoded, cv_predictions)\n",
        "\n",
        "# Also get individual fold accuracies for reporting\n",
        "cv_scores = cross_val_score(best_mlp_model, X_train_scaled, y_train_encoded,\n",
        "                          cv=cv_strategy, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "print(\"‚úÖ 10-fold cross-validation completed!\")\n",
        "\n",
        "print(f\"\\nüìä 10-FOLD CROSS-VALIDATION RESULTS (TRAINING PERFORMANCE):\")\n",
        "print(f\"   Fold scores: {[f'{score:.4f}' for score in cv_scores]}\")\n",
        "print(f\"   Mean CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
        "print(f\"   Standard Deviation: {cv_scores.std():.4f}\")\n",
        "\n",
        "# STEP 6: Make predictions on TEST set with tuned model\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä MODEL EVALUATION WITH TUNED PARAMETERS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Predictions on TEST set with tuned model\n",
        "y_test_pred = best_mlp_model.predict(X_test_scaled)\n",
        "y_test_proba = best_mlp_model.predict_proba(X_test_scaled)\n",
        "\n",
        "# Calculate metrics for TESTING set for BOTH CLASSES\n",
        "test_accuracy = accuracy_score(y_test_encoded, y_test_pred)\n",
        "test_precision = precision_score(y_test_encoded, y_test_pred, average=None)\n",
        "test_recall = recall_score(y_test_encoded, y_test_pred, average=None)\n",
        "test_f1 = f1_score(y_test_encoded, y_test_pred, average=None)\n",
        "test_roc_auc = roc_auc_score(y_test_encoded, y_test_proba[:, 1])\n",
        "test_mcc = matthews_corrcoef(y_test_encoded, y_test_pred)\n",
        "\n",
        "# Confusion matrices\n",
        "train_cm = confusion_matrix(y_train_encoded, cv_predictions)\n",
        "test_cm = confusion_matrix(y_test_encoded, y_test_pred)\n",
        "tn, fp, fn, tp = test_cm.ravel()\n",
        "\n",
        "# Additional metrics\n",
        "specificity = tn / (tn + fp)\n",
        "npv = tn / (tn + fn)\n",
        "\n",
        "# Performance gap\n",
        "accuracy_gap = train_accuracy - test_accuracy\n",
        "\n",
        "# Compute ROC curves for later saving\n",
        "fpr_train, tpr_train, _ = roc_curve(y_train_encoded, cv_probabilities[:, 1])\n",
        "fpr_test, tpr_test, _ = roc_curve(y_test_encoded, y_test_proba[:, 1])\n",
        "\n",
        "# STEP 7: Display comprehensive results for BOTH CLASSES\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìà COMPREHENSIVE PERFORMANCE METRICS FOR BOTH CLASSES (TUNED MODEL)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create detailed comparison dataframe for BOTH CLASSES\n",
        "class_performance_data = []\n",
        "\n",
        "for i, class_name in enumerate(class_names):\n",
        "    class_performance_data.extend([\n",
        "        {\n",
        "            'Class': class_name,\n",
        "            'Metric': 'Precision',\n",
        "            'Training (10-Fold CV)': f\"{train_precision[i]:.4f}\",\n",
        "            'Testing': f\"{test_precision[i]:.4f}\",\n",
        "            'Difference': f\"{train_precision[i]-test_precision[i]:+.4f}\"\n",
        "        },\n",
        "        {\n",
        "            'Class': class_name,\n",
        "            'Metric': 'Recall',\n",
        "            'Training (10-Fold CV)': f\"{train_recall[i]:.4f}\",\n",
        "            'Testing': f\"{test_recall[i]:.4f}\",\n",
        "            'Difference': f\"{train_recall[i]-test_recall[i]:+.4f}\"\n",
        "        },\n",
        "        {\n",
        "            'Class': class_name,\n",
        "            'Metric': 'F1-Score',\n",
        "            'Training (10-Fold CV)': f\"{train_f1[i]:.4f}\",\n",
        "            'Testing': f\"{test_f1[i]:.4f}\",\n",
        "            'Difference': f\"{train_f1[i]-test_f1[i]:+.4f}\"\n",
        "        }\n",
        "    ])\n",
        "\n",
        "# Add overall metrics\n",
        "overall_metrics = [\n",
        "    {\n",
        "        'Class': 'Overall',\n",
        "        'Metric': 'Accuracy',\n",
        "        'Training (10-Fold CV)': f\"{train_accuracy:.4f}\",\n",
        "        'Testing': f\"{test_accuracy:.4f}\",\n",
        "        'Difference': f\"{train_accuracy-test_accuracy:+.4f}\"\n",
        "    },\n",
        "    {\n",
        "        'Class': 'Overall',\n",
        "        'Metric': 'ROC-AUC',\n",
        "        'Training (10-Fold CV)': f\"{train_roc_auc:.4f}\",\n",
        "        'Testing': f\"{test_roc_auc:.4f}\",\n",
        "        'Difference': f\"{train_roc_auc-test_roc_auc:+.4f}\"\n",
        "    },\n",
        "    {\n",
        "        'Class': 'Overall',\n",
        "        'Metric': 'MCC',\n",
        "        'Training (10-Fold CV)': f\"{train_mcc:.4f}\",\n",
        "        'Testing': f\"{test_mcc:.4f}\",\n",
        "        'Difference': f\"{train_mcc-test_mcc:+.4f}\"\n",
        "    }\n",
        "]\n",
        "\n",
        "performance_df = pd.DataFrame(class_performance_data + overall_metrics)\n",
        "print(performance_df.to_string(index=False))\n",
        "\n",
        "# STEP 8: Enhanced Performance Visualization for Both Classes (WITH ROC CURVE)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä GENERATING ENHANCED PERFORMANCE VISUALIZATIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create comprehensive performance comparison for both classes\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 16))\n",
        "\n",
        "# 1. Main Performance Comparison (Overall Metrics)\n",
        "overall_metrics = ['Accuracy', 'ROC-AUC', 'MCC']\n",
        "train_overall = [train_accuracy, train_roc_auc, train_mcc]\n",
        "test_overall = [test_accuracy, test_roc_auc, test_mcc]\n",
        "\n",
        "x = np.arange(len(overall_metrics))\n",
        "width = 0.35\n",
        "\n",
        "bars1 = ax1.bar(x - width/2, train_overall, width, label='Training (10-Fold CV)',\n",
        "               color=elsevier_colors[0], alpha=0.8, edgecolor='black', linewidth=1.5)\n",
        "bars2 = ax1.bar(x + width/2, test_overall, width, label='Testing (Held-Out)',\n",
        "               color=elsevier_colors[1], alpha=0.8, edgecolor='black', linewidth=1.5)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bars in [bars1, bars2]:\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                f'{height:.3f}', ha='center', va='bottom',\n",
        "                fontweight='bold', fontsize=11)\n",
        "\n",
        "ax1.set_xlabel('Performance Metrics', fontsize=14, fontweight='bold')\n",
        "ax1.set_ylabel('Score', fontsize=14, fontweight='bold')\n",
        "ax1.set_title('MLP Classifier (Tuned) - Overall Performance\\nTraining vs Testing',\n",
        "              fontsize=16, fontweight='bold', pad=20)\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(overall_metrics)\n",
        "ax1.legend(fontsize=12, framealpha=0.9)\n",
        "ax1.set_ylim(0, 1.1)\n",
        "ax1.grid(True, alpha=0.3, linestyle='--')\n",
        "\n",
        "# 2. ROC CURVE for Training and Testing Sets\n",
        "ax2.plot(fpr_train, tpr_train, color=elsevier_colors[0], lw=2.5,\n",
        "         label=f'Training ROC (AUC = {train_roc_auc:.3f})', alpha=0.8)\n",
        "ax2.plot(fpr_test, tpr_test, color=elsevier_colors[1], lw=2.5,\n",
        "         label=f'Testing ROC (AUC = {test_roc_auc:.3f})', alpha=0.8)\n",
        "\n",
        "# Plot diagonal reference line\n",
        "ax2.plot([0, 1], [0, 1], color='gray', lw=1.5, linestyle='--', alpha=0.7,\n",
        "         label='Random Classifier (AUC = 0.5)')\n",
        "\n",
        "# Fill area under curves\n",
        "ax2.fill_between(fpr_train, tpr_train, alpha=0.2, color=elsevier_colors[0])\n",
        "ax2.fill_between(fpr_test, tpr_test, alpha=0.2, color=elsevier_colors[1])\n",
        "\n",
        "ax2.set_xlabel('False Positive Rate', fontsize=14, fontweight='bold')\n",
        "ax2.set_ylabel('True Positive Rate', fontsize=14, fontweight='bold')\n",
        "ax2.set_title('ROC Curves - Training vs Testing Sets\\nMLP Classifier (Tuned)',\n",
        "              fontsize=16, fontweight='bold', pad=20)\n",
        "ax2.legend(loc='lower right', fontsize=12, framealpha=0.9)\n",
        "ax2.grid(True, alpha=0.3, linestyle='--')\n",
        "ax2.set_xlim([0.0, 1.0])\n",
        "ax2.set_ylim([0.0, 1.05])\n",
        "\n",
        "# 3. Confusion Matrix - Training\n",
        "im1 = ax3.imshow(train_cm, cmap='Blues', interpolation='nearest', alpha=0.8)\n",
        "ax3.set_xlabel('Predicted Label', fontsize=14, fontweight='bold')\n",
        "ax3.set_ylabel('True Label', fontsize=14, fontweight='bold')\n",
        "ax3.set_title('Training Set - Confusion Matrix\\n(10-Fold CV Average)',\n",
        "              fontsize=16, fontweight='bold', pad=20)\n",
        "ax3.set_xticks([0, 1])\n",
        "ax3.set_yticks([0, 1])\n",
        "ax3.set_xticklabels(class_names)\n",
        "ax3.set_yticklabels(class_names)\n",
        "\n",
        "# Add text annotations for training CM\n",
        "for i in range(train_cm.shape[0]):\n",
        "    for j in range(train_cm.shape[1]):\n",
        "        ax3.text(j, i, f'{train_cm[i, j]}\\n({train_cm[i, j]/np.sum(train_cm):.1%})',\n",
        "                ha='center', va='center', fontsize=12, fontweight='bold',\n",
        "                color='white' if train_cm[i, j] > np.max(train_cm)/2 else 'black')\n",
        "\n",
        "# 4. Confusion Matrix - Testing\n",
        "im2 = ax4.imshow(test_cm, cmap='Reds', interpolation='nearest', alpha=0.8)\n",
        "ax4.set_xlabel('Predicted Label', fontsize=14, fontweight='bold')\n",
        "ax4.set_ylabel('True Label', fontsize=14, fontweight='bold')\n",
        "ax4.set_title('Testing Set - Confusion Matrix\\n(Held-Out Dataset)',\n",
        "              fontsize=16, fontweight='bold', pad=20)\n",
        "ax4.set_xticks([0, 1])\n",
        "ax4.set_yticks([0, 1])\n",
        "ax4.set_xticklabels(class_names)\n",
        "ax4.set_yticklabels(class_names)\n",
        "\n",
        "# Add text annotations for testing CM\n",
        "for i in range(test_cm.shape[0]):\n",
        "    for j in range(test_cm.shape[1]):\n",
        "        ax4.text(j, i, f'{test_cm[i, j]}\\n({test_cm[i, j]/np.sum(test_cm):.1%})',\n",
        "                ha='center', va='center', fontsize=12, fontweight='bold',\n",
        "                color='white' if test_cm[i, j] > np.max(test_cm)/2 else 'black')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('mlp_tuned_comprehensive_performance.png', dpi=300,\n",
        "            bbox_inches='tight', facecolor='white')\n",
        "plt.show()\n",
        "\n",
        "# STEP 9: FIXED Classification Reports\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìã DETAILED CLASSIFICATION REPORTS FOR BOTH CLASSES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nTRAINING SET CLASSIFICATION REPORT (10-Fold CV Average):\")\n",
        "training_report = classification_report(y_train_encoded, cv_predictions,\n",
        "                                      target_names=class_names, digits=4)\n",
        "print(training_report)\n",
        "\n",
        "print(\"\\nTESTING SET CLASSIFICATION REPORT (Held-Out):\")\n",
        "testing_report = classification_report(y_test_encoded, y_test_pred,\n",
        "                                     target_names=class_names, digits=4)\n",
        "print(testing_report)\n",
        "\n",
        "# STEP 10: Individual Class Performance Analysis\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéØ INDIVIDUAL CLASS PERFORMANCE ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for i, class_name in enumerate(class_names):\n",
        "    print(f\"\\n{class_name.upper()} CLASS PERFORMANCE:\")\n",
        "    print(f\"  Training (10-Fold CV):\")\n",
        "    print(f\"    Precision: {train_precision[i]:.4f}\")\n",
        "    print(f\"    Recall:    {train_recall[i]:.4f}\")\n",
        "    print(f\"    F1-Score:  {train_f1[i]:.4f}\")\n",
        "    print(f\"  Testing (Held-Out):\")\n",
        "    print(f\"    Precision: {test_precision[i]:.4f}\")\n",
        "    print(f\"    Recall:    {test_recall[i]:.4f}\")\n",
        "    print(f\"    F1-Score:  {test_f1[i]:.4f}\")\n",
        "    print(f\"  Performance Gap:\")\n",
        "    print(f\"    Precision: {train_precision[i]-test_precision[i]:+.4f}\")\n",
        "    print(f\"    Recall:    {train_recall[i]-test_recall[i]:+.4f}\")\n",
        "    print(f\"    F1-Score:  {train_f1[i]-test_f1[i]:+.4f}\")\n",
        "\n",
        "# STEP 11: Tuning Results Summary\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üî¨ HYPERPARAMETER TUNING SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"üéØ BEST PARAMETERS FOUND:\")\n",
        "for param, value in best_params.items():\n",
        "    print(f\"   {param}: {value}\")\n",
        "\n",
        "print(f\"\\nüìä PERFORMANCE COMPARISON:\")\n",
        "print(f\"   Default Model Accuracy: {accuracy_default:.4f}\")\n",
        "print(f\"   Tuned Model Accuracy:   {accuracy_tuned:.4f}\")\n",
        "print(f\"   Improvement:            {improvement:+.4f}\")\n",
        "\n",
        "print(f\"\\nüöÄ FINAL TUNED MODEL PERFORMANCE:\")\n",
        "print(f\"   Training CV Accuracy: {train_accuracy:.4f}\")\n",
        "print(f\"   Test Accuracy:        {test_accuracy:.4f}\")\n",
        "print(f\"   Test ROC-AUC:         {test_roc_auc:.4f}\")\n",
        "print(f\"   Test MCC:             {test_mcc:.4f}\")\n",
        "\n",
        "# STEP 12: Save ROC Data for Later Use (LIKE NAIVE BAYES AND DECISION TREE)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üíæ SAVING ROC DATA FOR COMBINED VISUALIZATIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Save the tuned model and scaler\n",
        "joblib.dump(best_mlp_model, 'mlp_tuned_model.pkl')\n",
        "joblib.dump(scaler, 'mlp_tuned_scaler.pkl')\n",
        "joblib.dump(label_encoder, 'mlp_tuned_label_encoder.pkl')\n",
        "\n",
        "# ============== SAVE ROC PREDICTION DATA ==============\n",
        "print(\"\\nüíæ SAVING ROC PREDICTION DATA FILES...\")\n",
        "\n",
        "# File 1: Save NPZ file for ROC plotting\n",
        "np.savez('mlp_roc_predictions.npz',\n",
        "         # Training set data (from cross-validation)\n",
        "         y_train_true=y_train_encoded,\n",
        "         y_train_prob=cv_probabilities[:, 1],  # Probability for positive class\n",
        "\n",
        "         # Testing set data\n",
        "         y_test_true=y_test_encoded,\n",
        "         y_test_prob=y_test_proba[:, 1],  # Probability for positive class\n",
        "\n",
        "         # Precomputed ROC curve points\n",
        "         fpr_train=fpr_train,\n",
        "         tpr_train=tpr_train,\n",
        "         fpr_test=fpr_test,\n",
        "         tpr_test=tpr_test,\n",
        "\n",
        "         # AUC values\n",
        "         train_auc=train_roc_auc,\n",
        "         test_auc=test_roc_auc,\n",
        "\n",
        "         # Metadata\n",
        "         model_name='MLP Classifier',\n",
        "         class_names=class_names,\n",
        "         feature_count=len(feature_columns),\n",
        "         best_params=str(best_params),\n",
        "         scaling_used=True)\n",
        "print(\"‚úÖ Saved: mlp_roc_predictions.npz\")\n",
        "\n",
        "# File 2: Save detailed predictions CSV\n",
        "predictions_df = pd.DataFrame({\n",
        "    'true_label': y_test_encoded,\n",
        "    'predicted_label': y_test_pred,\n",
        "    'probability_class_0': y_test_proba[:, 0],\n",
        "    'probability_class_1': y_test_proba[:, 1],\n",
        "    'correct': (y_test_encoded == y_test_pred)\n",
        "})\n",
        "predictions_df.to_csv('mlp_predictions.csv', index=False)\n",
        "print(\"‚úÖ Saved: mlp_predictions.csv\")\n",
        "\n",
        "# File 3: Save comprehensive metrics CSV\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Model': ['MLP Classifier'],\n",
        "    'Train_Accuracy': [train_accuracy],\n",
        "    'Test_Accuracy': [test_accuracy],\n",
        "    'Train_AUC': [train_roc_auc],\n",
        "    'Test_AUC': [test_roc_auc],\n",
        "    'Train_Precision_0': [train_precision[0]],\n",
        "    'Train_Precision_1': [train_precision[1]],\n",
        "    'Test_Precision_0': [test_precision[0]],\n",
        "    'Test_Precision_1': [test_precision[1]],\n",
        "    'Train_Recall_0': [train_recall[0]],\n",
        "    'Train_Recall_1': [train_recall[1]],\n",
        "    'Test_Recall_0': [test_recall[0]],\n",
        "    'Test_Recall_1': [test_recall[1]],\n",
        "    'Train_F1_0': [train_f1[0]],\n",
        "    'Train_F1_1': [train_f1[1]],\n",
        "    'Test_F1_0': [test_f1[0]],\n",
        "    'Test_F1_1': [test_f1[1]],\n",
        "    'Train_MCC': [train_mcc],\n",
        "    'Test_MCC': [test_mcc],\n",
        "    'Num_Features': [len(feature_columns)],\n",
        "    'Hidden_Layers': [str(best_params.get('hidden_layer_sizes', 'N/A'))],\n",
        "    'Activation': [best_params.get('activation', 'N/A')],\n",
        "    'Learning_Rate': [best_params.get('learning_rate', 'N/A')],\n",
        "    'Scaling_Used': ['Yes']\n",
        "})\n",
        "metrics_df.to_csv('mlp_metrics.csv', index=False)\n",
        "print(\"‚úÖ Saved: mlp_metrics.csv\")\n",
        "\n",
        "# File 4: Save all predictions (training + testing) for reference\n",
        "all_predictions_df = pd.DataFrame({\n",
        "    'dataset': ['train'] * len(y_train_encoded) + ['test'] * len(y_test_encoded),\n",
        "    'true_label': np.concatenate([y_train_encoded, y_test_encoded]),\n",
        "    'predicted_label': np.concatenate([cv_predictions, y_test_pred]),\n",
        "    'probability_class_1': np.concatenate([cv_probabilities[:, 1], y_test_proba[:, 1]])\n",
        "})\n",
        "all_predictions_df.to_csv('mlp_all_predictions.csv', index=False)\n",
        "print(\"‚úÖ Saved: mlp_all_predictions.csv\")\n",
        "\n",
        "# File 5: Save grid search results\n",
        "grid_results_df = pd.DataFrame(grid_search.cv_results_)\n",
        "grid_results_df.to_excel('mlp_grid_search_results.xlsx', index=False)\n",
        "print(\"‚úÖ Saved: mlp_grid_search_results.xlsx\")\n",
        "\n",
        "# File 6: Save tuning parameters\n",
        "tuning_params = {\n",
        "    'best_params': best_params,\n",
        "    'best_cv_score': grid_search.best_score_,\n",
        "    'default_accuracy': accuracy_default,\n",
        "    'tuned_accuracy': accuracy_tuned,\n",
        "    'improvement': improvement,\n",
        "    'test_roc_auc': test_roc_auc,\n",
        "    'test_mcc': test_mcc\n",
        "}\n",
        "tuning_params_df = pd.DataFrame([tuning_params])\n",
        "tuning_params_df.to_excel('mlp_tuning_parameters.xlsx', index=False)\n",
        "\n",
        "print(f\"\\nüéØ MLP CLASSIFIER WITH HYPERPARAMETER TUNING COMPLETED!\")\n",
        "print(f\"üìä Training (10-Fold CV) Accuracy: {train_accuracy:.1%}\")\n",
        "print(f\"üìä Testing Accuracy: {test_accuracy:.1%}\")\n",
        "print(f\"üìà Training ROC-AUC: {train_roc_auc:.3f}\")\n",
        "print(f\"üìà Testing ROC-AUC: {test_roc_auc:.3f}\")\n",
        "print(f\"‚ö° Improvement over default: {improvement:+.2%}\")\n",
        "print(f\"üîç Performance metrics shown for BOTH classes: {class_names}\")\n",
        "\n",
        "print(f\"\\nüìÅ FILES GENERATED FOR ROC CURVES:\")\n",
        "print(f\"   1. mlp_roc_predictions.npz     - Main ROC data file\")\n",
        "print(f\"   2. mlp_predictions.csv         - Detailed test predictions\")\n",
        "print(f\"   3. mlp_metrics.csv             - Performance metrics\")\n",
        "print(f\"   4. mlp_all_predictions.csv     - All predictions (train+test)\")\n",
        "print(f\"   5. mlp_tuned_model.pkl         - Trained model\")\n",
        "print(f\"   6. mlp_tuned_scaler.pkl        - Scaler for new data\")\n",
        "print(f\"   7. mlp_grid_search_results.xlsx - Grid search results\")\n",
        "\n",
        "print(f\"\\nüéØ Use 'mlp_roc_predictions.npz' with other model files for combined ROC plots!\")\n",
        "print(\"üöÄ TUNED MLP MODEL READY FOR DEPLOYMENT!\")"
      ],
      "metadata": {
        "id": "VdFNl03rKZJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Decission Tree:**"
      ],
      "metadata": {
        "id": "gMou__0PKdaa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Requirements: training-set-selected-features.xlsx, testing-set-selected-features.xlsx"
      ],
      "metadata": {
        "id": "RzbWuEvxOkD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "                           roc_auc_score, roc_curve, matthews_corrcoef, confusion_matrix,\n",
        "                           classification_report)\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, cross_val_predict, GridSearchCV\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "\n",
        "# Set publication-quality parameters (Fixed font settings - REMOVED PROBLEMATIC FONTS)\n",
        "plt.rcParams.update({\n",
        "    'font.size': 12,\n",
        "    'axes.labelsize': 14,\n",
        "    'axes.titlesize': 16,\n",
        "    'xtick.labelsize': 12,\n",
        "    'ytick.labelsize': 12,\n",
        "    'legend.fontsize': 12,\n",
        "    'figure.titlesize': 18,\n",
        "    'font.family': ['DejaVu Sans', 'Liberation Sans', 'Bitstream Vera Sans', 'sans-serif']\n",
        "})\n",
        "\n",
        "# Professional Elsevier color scheme\n",
        "elsevier_colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#1A936F', '#114B5F']\n",
        "\n",
        "print(\"üå≥ DECISION TREE CLASSIFIER WITH HYPERPARAMETER TUNING (10-FOLD CV + GRID SEARCH)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# STEP 1: Load the feature-selected datasets\n",
        "print(\"üìä Loading feature-selected datasets...\")\n",
        "\n",
        "train_df = pd.read_excel('training_set_selected_features.xlsx')\n",
        "test_df = pd.read_excel('testing_set_selected_features.xlsx')\n",
        "\n",
        "print(f\"Training set: {train_df.shape}\")\n",
        "print(f\"Testing set: {test_df.shape}\")\n",
        "\n",
        "# STEP 2: Prepare features and target\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üîß PREPARING DATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Identify feature columns (exclude metadata)\n",
        "metadata_columns = ['COMPOUND ID', 'SMILE CODE', 'Ligand Type']\n",
        "feature_columns = [col for col in train_df.columns if col not in metadata_columns]\n",
        "\n",
        "X_train = train_df[feature_columns]\n",
        "y_train = train_df['Ligand Type']\n",
        "\n",
        "X_test = test_df[feature_columns]\n",
        "y_test = test_df['Ligand Type']\n",
        "\n",
        "print(f\"Selected features: {len(feature_columns)}\")\n",
        "print(f\"Training samples: {X_train.shape[0]}\")\n",
        "print(f\"Testing samples: {X_test.shape[0]}\")\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# FIXED: Properly get class names as strings\n",
        "class_names = [str(cls) for cls in label_encoder.classes_]\n",
        "print(f\"Class names: {class_names}\")\n",
        "\n",
        "print(\"‚úÖ Data prepared successfully!\")\n",
        "\n",
        "# STEP 3: Hyperparameter Tuning with GridSearchCV and 10-Fold CV\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéØ HYPERPARAMETER TUNING WITH GRIDSEARCHCV (10-FOLD CV)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Define parameter grid for Decision Tree\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 7, 10, 15, None],\n",
        "    'min_samples_split': [2, 5, 10, 20],\n",
        "    'min_samples_leaf': [1, 2, 5, 10],\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'max_features': ['sqrt', 'log2', None]\n",
        "}\n",
        "\n",
        "# Initialize Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# 10-fold cross-validation strategy\n",
        "cv_strategy = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# GridSearchCV with 10-fold CV\n",
        "print(\"üîç Performing Grid Search with 10-fold CV...\")\n",
        "print(f\"   Parameter grid: {param_grid}\")\n",
        "print(f\"   Total parameter combinations: {len(param_grid['max_depth']) * len(param_grid['min_samples_split']) * len(param_grid['min_samples_leaf']) * len(param_grid['criterion']) * len(param_grid['max_features'])}\")\n",
        "print(f\"   This may take a while...\")\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=dt,\n",
        "    param_grid=param_grid,\n",
        "    cv=cv_strategy,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Perform grid search\n",
        "grid_search.fit(X_train, y_train_encoded)\n",
        "\n",
        "print(\"‚úÖ Grid Search completed!\")\n",
        "\n",
        "# Display best parameters and scores\n",
        "print(f\"\\nüéØ BEST PARAMETERS FOUND:\")\n",
        "best_params = grid_search.best_params_\n",
        "for param, value in best_params.items():\n",
        "    print(f\"   {param}: {value}\")\n",
        "print(f\"   Best CV Score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# STEP 4: Compare Default vs Tuned Models\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä COMPARISON: DEFAULT vs TUNED PARAMETERS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Train model with default parameters for comparison\n",
        "dt_default = DecisionTreeClassifier(random_state=42)\n",
        "dt_default.fit(X_train, y_train_encoded)\n",
        "y_pred_default = dt_default.predict(X_test)\n",
        "accuracy_default = accuracy_score(y_test_encoded, y_pred_default)\n",
        "\n",
        "# Use best model from grid search\n",
        "best_dt_model = grid_search.best_estimator_\n",
        "y_pred_tuned = best_dt_model.predict(X_test)\n",
        "accuracy_tuned = accuracy_score(y_test_encoded, y_pred_tuned)\n",
        "\n",
        "improvement = accuracy_tuned - accuracy_default\n",
        "\n",
        "print(f\"   Default Parameters Accuracy: {accuracy_default:.4f}\")\n",
        "print(f\"   Tuned Parameters Accuracy:   {accuracy_tuned:.4f}\")\n",
        "print(f\"   Improvement:                 {improvement:+.4f}\")\n",
        "\n",
        "if improvement > 0:\n",
        "    print(f\"   ‚úÖ Tuning improved accuracy by {improvement:.2%}\")\n",
        "else:\n",
        "    print(f\"   ‚ö†Ô∏è  Tuning did not improve accuracy\")\n",
        "\n",
        "# STEP 5: 10-Fold Cross-Validation with Tuned Model (for proper comparison)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéØ 10-FOLD CROSS-VALIDATION WITH TUNED PARAMETERS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"üöÄ Performing 10-fold cross-validation with tuned parameters...\")\n",
        "\n",
        "# Get cross-validated predictions for training set using tuned parameters\n",
        "cv_predictions = cross_val_predict(best_dt_model, X_train, y_train_encoded,\n",
        "                                 cv=cv_strategy, method='predict')\n",
        "cv_probabilities = cross_val_predict(best_dt_model, X_train, y_train_encoded,\n",
        "                                   cv=cv_strategy, method='predict_proba')\n",
        "\n",
        "# Calculate training metrics as average across 10-fold CV\n",
        "train_accuracy = accuracy_score(y_train_encoded, cv_predictions)\n",
        "\n",
        "# Calculate metrics for BOTH CLASSES\n",
        "train_precision = precision_score(y_train_encoded, cv_predictions, average=None)\n",
        "train_recall = recall_score(y_train_encoded, cv_predictions, average=None)\n",
        "train_f1 = f1_score(y_train_encoded, cv_predictions, average=None)\n",
        "train_roc_auc = roc_auc_score(y_train_encoded, cv_probabilities[:, 1])\n",
        "train_mcc = matthews_corrcoef(y_train_encoded, cv_predictions)\n",
        "\n",
        "# Also get individual fold accuracies for reporting\n",
        "cv_scores = cross_val_score(best_dt_model, X_train, y_train_encoded,\n",
        "                          cv=cv_strategy, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "print(\"‚úÖ 10-fold cross-validation completed!\")\n",
        "\n",
        "print(f\"\\nüìä 10-FOLD CROSS-VALIDATION RESULTS (TRAINING PERFORMANCE):\")\n",
        "print(f\"   Fold scores: {[f'{score:.4f}' for score in cv_scores]}\")\n",
        "print(f\"   Mean CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
        "print(f\"   Standard Deviation: {cv_scores.std():.4f}\")\n",
        "\n",
        "# STEP 6: Make predictions on TEST set with tuned model\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä MODEL EVALUATION WITH TUNED PARAMETERS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Predictions on TEST set with tuned model\n",
        "y_test_pred = best_dt_model.predict(X_test)\n",
        "y_test_proba = best_dt_model.predict_proba(X_test)\n",
        "\n",
        "# Calculate metrics for TESTING set for BOTH CLASSES\n",
        "test_accuracy = accuracy_score(y_test_encoded, y_test_pred)\n",
        "test_precision = precision_score(y_test_encoded, y_test_pred, average=None)\n",
        "test_recall = recall_score(y_test_encoded, y_test_pred, average=None)\n",
        "test_f1 = f1_score(y_test_encoded, y_test_pred, average=None)\n",
        "test_roc_auc = roc_auc_score(y_test_encoded, y_test_proba[:, 1])\n",
        "test_mcc = matthews_corrcoef(y_test_encoded, y_test_pred)\n",
        "\n",
        "# Confusion matrices\n",
        "train_cm = confusion_matrix(y_train_encoded, cv_predictions)\n",
        "test_cm = confusion_matrix(y_test_encoded, y_test_pred)\n",
        "tn, fp, fn, tp = test_cm.ravel()\n",
        "\n",
        "# Additional metrics\n",
        "specificity = tn / (tn + fp)\n",
        "npv = tn / (tn + fn)\n",
        "\n",
        "# Performance gap\n",
        "accuracy_gap = train_accuracy - test_accuracy\n",
        "\n",
        "# Compute ROC curves for later saving\n",
        "fpr_train, tpr_train, _ = roc_curve(y_train_encoded, cv_probabilities[:, 1])\n",
        "fpr_test, tpr_test, _ = roc_curve(y_test_encoded, y_test_proba[:, 1])\n",
        "\n",
        "# STEP 7: Display comprehensive results for BOTH CLASSES\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìà COMPREHENSIVE PERFORMANCE METRICS FOR BOTH CLASSES (TUNED MODEL)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create detailed comparison dataframe for BOTH CLASSES\n",
        "class_performance_data = []\n",
        "\n",
        "for i, class_name in enumerate(class_names):\n",
        "    class_performance_data.extend([\n",
        "        {\n",
        "            'Class': class_name,\n",
        "            'Metric': 'Precision',\n",
        "            'Training (10-Fold CV)': f\"{train_precision[i]:.4f}\",\n",
        "            'Testing': f\"{test_precision[i]:.4f}\",\n",
        "            'Difference': f\"{train_precision[i]-test_precision[i]:+.4f}\"\n",
        "        },\n",
        "        {\n",
        "            'Class': class_name,\n",
        "            'Metric': 'Recall',\n",
        "            'Training (10-Fold CV)': f\"{train_recall[i]:.4f}\",\n",
        "            'Testing': f\"{test_recall[i]:.4f}\",\n",
        "            'Difference': f\"{train_recall[i]-test_recall[i]:+.4f}\"\n",
        "        },\n",
        "        {\n",
        "            'Class': class_name,\n",
        "            'Metric': 'F1-Score',\n",
        "            'Training (10-Fold CV)': f\"{train_f1[i]:.4f}\",\n",
        "            'Testing': f\"{test_f1[i]:.4f}\",\n",
        "            'Difference': f\"{train_f1[i]-test_f1[i]:+.4f}\"\n",
        "        }\n",
        "    ])\n",
        "\n",
        "# Add overall metrics\n",
        "overall_metrics = [\n",
        "    {\n",
        "        'Class': 'Overall',\n",
        "        'Metric': 'Accuracy',\n",
        "        'Training (10-Fold CV)': f\"{train_accuracy:.4f}\",\n",
        "        'Testing': f\"{test_accuracy:.4f}\",\n",
        "        'Difference': f\"{train_accuracy-test_accuracy:+.4f}\"\n",
        "    },\n",
        "    {\n",
        "        'Class': 'Overall',\n",
        "        'Metric': 'ROC-AUC',\n",
        "        'Training (10-Fold CV)': f\"{train_roc_auc:.4f}\",\n",
        "        'Testing': f\"{test_roc_auc:.4f}\",\n",
        "        'Difference': f\"{train_roc_auc-test_roc_auc:+.4f}\"\n",
        "    },\n",
        "    {\n",
        "        'Class': 'Overall',\n",
        "        'Metric': 'MCC',\n",
        "        'Training (10-Fold CV)': f\"{train_mcc:.4f}\",\n",
        "        'Testing': f\"{test_mcc:.4f}\",\n",
        "        'Difference': f\"{train_mcc-test_mcc:+.4f}\"\n",
        "    }\n",
        "]\n",
        "\n",
        "performance_df = pd.DataFrame(class_performance_data + overall_metrics)\n",
        "print(performance_df.to_string(index=False))\n",
        "\n",
        "# STEP 8: Enhanced Performance Visualization for Both Classes (WITH ROC CURVE)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä GENERATING ENHANCED PERFORMANCE VISUALIZATIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create comprehensive performance comparison for both classes\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 16))\n",
        "\n",
        "# 1. Main Performance Comparison (Overall Metrics)\n",
        "overall_metrics = ['Accuracy', 'ROC-AUC', 'MCC']\n",
        "train_overall = [train_accuracy, train_roc_auc, train_mcc]\n",
        "test_overall = [test_accuracy, test_roc_auc, test_mcc]\n",
        "\n",
        "x = np.arange(len(overall_metrics))\n",
        "width = 0.35\n",
        "\n",
        "bars1 = ax1.bar(x - width/2, train_overall, width, label='Training (10-Fold CV)',\n",
        "               color=elsevier_colors[0], alpha=0.8, edgecolor='black', linewidth=1.5)\n",
        "bars2 = ax1.bar(x + width/2, test_overall, width, label='Testing (Held-Out)',\n",
        "               color=elsevier_colors[1], alpha=0.8, edgecolor='black', linewidth=1.5)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bars in [bars1, bars2]:\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                f'{height:.3f}', ha='center', va='bottom',\n",
        "                fontweight='bold', fontsize=11)\n",
        "\n",
        "ax1.set_xlabel('Performance Metrics', fontsize=14, fontweight='bold')\n",
        "ax1.set_ylabel('Score', fontsize=14, fontweight='bold')\n",
        "ax1.set_title('Decision Tree (Tuned) - Overall Performance\\nTraining vs Testing',\n",
        "              fontsize=16, fontweight='bold', pad=20)\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(overall_metrics)\n",
        "ax1.legend(fontsize=12, framealpha=0.9)\n",
        "ax1.set_ylim(0, 1.1)\n",
        "ax1.grid(True, alpha=0.3, linestyle='--')\n",
        "\n",
        "# 2. ROC CURVE for Training and Testing Sets\n",
        "ax2.plot(fpr_train, tpr_train, color=elsevier_colors[0], lw=2.5,\n",
        "         label=f'Training ROC (AUC = {train_roc_auc:.3f})', alpha=0.8)\n",
        "ax2.plot(fpr_test, tpr_test, color=elsevier_colors[1], lw=2.5,\n",
        "         label=f'Testing ROC (AUC = {test_roc_auc:.3f})', alpha=0.8)\n",
        "\n",
        "# Plot diagonal reference line\n",
        "ax2.plot([0, 1], [0, 1], color='gray', lw=1.5, linestyle='--', alpha=0.7,\n",
        "         label='Random Classifier (AUC = 0.5)')\n",
        "\n",
        "# Fill area under curves\n",
        "ax2.fill_between(fpr_train, tpr_train, alpha=0.2, color=elsevier_colors[0])\n",
        "ax2.fill_between(fpr_test, tpr_test, alpha=0.2, color=elsevier_colors[1])\n",
        "\n",
        "ax2.set_xlabel('False Positive Rate', fontsize=14, fontweight='bold')\n",
        "ax2.set_ylabel('True Positive Rate', fontsize=14, fontweight='bold')\n",
        "ax2.set_title('ROC Curves - Training vs Testing Sets\\nDecision Tree (Tuned)',\n",
        "              fontsize=16, fontweight='bold', pad=20)\n",
        "ax2.legend(loc='lower right', fontsize=12, framealpha=0.9)\n",
        "ax2.grid(True, alpha=0.3, linestyle='--')\n",
        "ax2.set_xlim([0.0, 1.0])\n",
        "ax2.set_ylim([0.0, 1.05])\n",
        "\n",
        "# 3. Confusion Matrix - Training\n",
        "im1 = ax3.imshow(train_cm, cmap='Blues', interpolation='nearest', alpha=0.8)\n",
        "ax3.set_xlabel('Predicted Label', fontsize=14, fontweight='bold')\n",
        "ax3.set_ylabel('True Label', fontsize=14, fontweight='bold')\n",
        "ax3.set_title('Training Set - Confusion Matrix\\n(10-Fold CV Average)',\n",
        "              fontsize=16, fontweight='bold', pad=20)\n",
        "ax3.set_xticks([0, 1])\n",
        "ax3.set_yticks([0, 1])\n",
        "ax3.set_xticklabels(class_names)\n",
        "ax3.set_yticklabels(class_names)\n",
        "\n",
        "# Add text annotations for training CM\n",
        "for i in range(train_cm.shape[0]):\n",
        "    for j in range(train_cm.shape[1]):\n",
        "        ax3.text(j, i, f'{train_cm[i, j]}\\n({train_cm[i, j]/np.sum(train_cm):.1%})',\n",
        "                ha='center', va='center', fontsize=12, fontweight='bold',\n",
        "                color='white' if train_cm[i, j] > np.max(train_cm)/2 else 'black')\n",
        "\n",
        "# 4. Confusion Matrix - Testing\n",
        "im2 = ax4.imshow(test_cm, cmap='Reds', interpolation='nearest', alpha=0.8)\n",
        "ax4.set_xlabel('Predicted Label', fontsize=14, fontweight='bold')\n",
        "ax4.set_ylabel('True Label', fontsize=14, fontweight='bold')\n",
        "ax4.set_title('Testing Set - Confusion Matrix\\n(Held-Out Dataset)',\n",
        "              fontsize=16, fontweight='bold', pad=20)\n",
        "ax4.set_xticks([0, 1])\n",
        "ax4.set_yticks([0, 1])\n",
        "ax4.set_xticklabels(class_names)\n",
        "ax4.set_yticklabels(class_names)\n",
        "\n",
        "# Add text annotations for testing CM\n",
        "for i in range(test_cm.shape[0]):\n",
        "    for j in range(test_cm.shape[1]):\n",
        "        ax4.text(j, i, f'{test_cm[i, j]}\\n({test_cm[i, j]/np.sum(test_cm):.1%})',\n",
        "                ha='center', va='center', fontsize=12, fontweight='bold',\n",
        "                color='white' if test_cm[i, j] > np.max(test_cm)/2 else 'black')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('decision_tree_tuned_comprehensive_performance.png', dpi=300,\n",
        "            bbox_inches='tight', facecolor='white')\n",
        "plt.show()\n",
        "\n",
        "# STEP 9: FIXED Classification Reports\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìã DETAILED CLASSIFICATION REPORTS FOR BOTH CLASSES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nTRAINING SET CLASSIFICATION REPORT (10-Fold CV Average):\")\n",
        "training_report = classification_report(y_train_encoded, cv_predictions,\n",
        "                                      target_names=class_names, digits=4)\n",
        "print(training_report)\n",
        "\n",
        "print(\"\\nTESTING SET CLASSIFICATION REPORT (Held-Out):\")\n",
        "testing_report = classification_report(y_test_encoded, y_test_pred,\n",
        "                                     target_names=class_names, digits=4)\n",
        "print(testing_report)\n",
        "\n",
        "# STEP 10: Individual Class Performance Analysis\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéØ INDIVIDUAL CLASS PERFORMANCE ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for i, class_name in enumerate(class_names):\n",
        "    print(f\"\\n{class_name.upper()} CLASS PERFORMANCE:\")\n",
        "    print(f\"  Training (10-Fold CV):\")\n",
        "    print(f\"    Precision: {train_precision[i]:.4f}\")\n",
        "    print(f\"    Recall:    {train_recall[i]:.4f}\")\n",
        "    print(f\"    F1-Score:  {train_f1[i]:.4f}\")\n",
        "    print(f\"  Testing (Held-Out):\")\n",
        "    print(f\"    Precision: {test_precision[i]:.4f}\")\n",
        "    print(f\"    Recall:    {test_recall[i]:.4f}\")\n",
        "    print(f\"    F1-Score:  {test_f1[i]:.4f}\")\n",
        "    print(f\"  Performance Gap:\")\n",
        "    print(f\"    Precision: {train_precision[i]-test_precision[i]:+.4f}\")\n",
        "    print(f\"    Recall:    {train_recall[i]-test_recall[i]:+.4f}\")\n",
        "    print(f\"    F1-Score:  {train_f1[i]-test_f1[i]:+.4f}\")\n",
        "\n",
        "# STEP 11: Tuning Results Summary\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üî¨ HYPERPARAMETER TUNING SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"üéØ BEST PARAMETERS FOUND:\")\n",
        "for param, value in best_params.items():\n",
        "    print(f\"   {param}: {value}\")\n",
        "\n",
        "print(f\"\\nüìä PERFORMANCE COMPARISON:\")\n",
        "print(f\"   Default Model Accuracy: {accuracy_default:.4f}\")\n",
        "print(f\"   Tuned Model Accuracy:   {accuracy_tuned:.4f}\")\n",
        "print(f\"   Improvement:            {improvement:+.4f}\")\n",
        "\n",
        "print(f\"\\nüöÄ FINAL TUNED MODEL PERFORMANCE:\")\n",
        "print(f\"   Training CV Accuracy: {train_accuracy:.4f}\")\n",
        "print(f\"   Test Accuracy:        {test_accuracy:.4f}\")\n",
        "print(f\"   Test ROC-AUC:         {test_roc_auc:.4f}\")\n",
        "print(f\"   Test MCC:             {test_mcc:.4f}\")\n",
        "\n",
        "# STEP 12: Feature Importance Analysis (Specific to Decision Trees)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üîç FEATURE IMPORTANCE ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Get feature importances from the tuned model\n",
        "feature_importances = best_dt_model.feature_importances_\n",
        "\n",
        "# Create a DataFrame for feature importance\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': feature_columns,\n",
        "    'Importance': feature_importances\n",
        "})\n",
        "\n",
        "# Sort by importance\n",
        "importance_df = importance_df.sort_values('Importance', ascending=False)\n",
        "\n",
        "# Display top 20 most important features\n",
        "print(\"üìä TOP 20 MOST IMPORTANT FEATURES:\")\n",
        "print(importance_df.head(20).to_string(index=False))\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(12, 8))\n",
        "top_features = importance_df.head(15)\n",
        "plt.barh(top_features['Feature'], top_features['Importance'], color=elsevier_colors[0])\n",
        "plt.xlabel('Feature Importance', fontsize=14, fontweight='bold')\n",
        "plt.title('Decision Tree - Top 15 Feature Importances', fontsize=16, fontweight='bold')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.savefig('decision_tree_feature_importances.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
        "plt.show()\n",
        "\n",
        "# STEP 13: Save ROC Data for Later Use (LIKE NAIVE BAYES)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üíæ SAVING ROC DATA FOR COMBINED VISUALIZATIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Save the tuned model\n",
        "joblib.dump(best_dt_model, 'decision_tree_tuned_model.pkl')\n",
        "joblib.dump(label_encoder, 'decision_tree_tuned_label_encoder.pkl')\n",
        "\n",
        "# ============== SAVE ROC PREDICTION DATA ==============\n",
        "print(\"\\nüíæ SAVING ROC PREDICTION DATA FILES...\")\n",
        "\n",
        "# File 1: Save NPZ file for ROC plotting\n",
        "np.savez('decision_tree_roc_predictions.npz',\n",
        "         # Training set data (from cross-validation)\n",
        "         y_train_true=y_train_encoded,\n",
        "         y_train_prob=cv_probabilities[:, 1],  # Probability for positive class\n",
        "\n",
        "         # Testing set data\n",
        "         y_test_true=y_test_encoded,\n",
        "         y_test_prob=y_test_proba[:, 1],  # Probability for positive class\n",
        "\n",
        "         # Precomputed ROC curve points\n",
        "         fpr_train=fpr_train,\n",
        "         tpr_train=tpr_train,\n",
        "         fpr_test=fpr_test,\n",
        "         tpr_test=tpr_test,\n",
        "\n",
        "         # AUC values\n",
        "         train_auc=train_roc_auc,\n",
        "         test_auc=test_roc_auc,\n",
        "\n",
        "         # Metadata\n",
        "         model_name='Decision Tree',\n",
        "         class_names=class_names,\n",
        "         feature_count=len(feature_columns),\n",
        "         best_params=str(best_params))\n",
        "print(\"‚úÖ Saved: decision_tree_roc_predictions.npz\")\n",
        "\n",
        "# File 2: Save detailed predictions CSV\n",
        "predictions_df = pd.DataFrame({\n",
        "    'true_label': y_test_encoded,\n",
        "    'predicted_label': y_test_pred,\n",
        "    'probability_class_0': y_test_proba[:, 0],\n",
        "    'probability_class_1': y_test_proba[:, 1],\n",
        "    'correct': (y_test_encoded == y_test_pred)\n",
        "})\n",
        "predictions_df.to_csv('decision_tree_predictions.csv', index=False)\n",
        "print(\"‚úÖ Saved: decision_tree_predictions.csv\")\n",
        "\n",
        "# File 3: Save comprehensive metrics CSV\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Model': ['Decision Tree'],\n",
        "    'Train_Accuracy': [train_accuracy],\n",
        "    'Test_Accuracy': [test_accuracy],\n",
        "    'Train_AUC': [train_roc_auc],\n",
        "    'Test_AUC': [test_roc_auc],\n",
        "    'Train_Precision_0': [train_precision[0]],\n",
        "    'Train_Precision_1': [train_precision[1]],\n",
        "    'Test_Precision_0': [test_precision[0]],\n",
        "    'Test_Precision_1': [test_precision[1]],\n",
        "    'Train_Recall_0': [train_recall[0]],\n",
        "    'Train_Recall_1': [train_recall[1]],\n",
        "    'Test_Recall_0': [test_recall[0]],\n",
        "    'Test_Recall_1': [test_recall[1]],\n",
        "    'Train_F1_0': [train_f1[0]],\n",
        "    'Train_F1_1': [train_f1[1]],\n",
        "    'Test_F1_0': [test_f1[0]],\n",
        "    'Test_F1_1': [test_f1[1]],\n",
        "    'Train_MCC': [train_mcc],\n",
        "    'Test_MCC': [test_mcc],\n",
        "    'Num_Features': [len(feature_columns)],\n",
        "    'Max_Depth': [best_params.get('max_depth', 'N/A')],\n",
        "    'Criterion': [best_params.get('criterion', 'N/A')]\n",
        "})\n",
        "metrics_df.to_csv('decision_tree_metrics.csv', index=False)\n",
        "print(\"‚úÖ Saved: decision_tree_metrics.csv\")\n",
        "\n",
        "# File 4: Save all predictions (training + testing) for reference\n",
        "all_predictions_df = pd.DataFrame({\n",
        "    'dataset': ['train'] * len(y_train_encoded) + ['test'] * len(y_test_encoded),\n",
        "    'true_label': np.concatenate([y_train_encoded, y_test_encoded]),\n",
        "    'predicted_label': np.concatenate([cv_predictions, y_test_pred]),\n",
        "    'probability_class_1': np.concatenate([cv_probabilities[:, 1], y_test_proba[:, 1]])\n",
        "})\n",
        "all_predictions_df.to_csv('decision_tree_all_predictions.csv', index=False)\n",
        "print(\"‚úÖ Saved: decision_tree_all_predictions.csv\")\n",
        "\n",
        "# File 5: Save feature importances\n",
        "importance_df.to_excel('decision_tree_feature_importances.xlsx', index=False)\n",
        "\n",
        "# File 6: Save grid search results\n",
        "grid_results_df = pd.DataFrame(grid_search.cv_results_)\n",
        "grid_results_df.to_excel('decision_tree_grid_search_results.xlsx', index=False)\n",
        "print(\"‚úÖ Saved: decision_tree_grid_search_results.xlsx\")\n",
        "\n",
        "# File 7: Save tuning parameters\n",
        "tuning_params = {\n",
        "    'best_params': best_params,\n",
        "    'best_cv_score': grid_search.best_score_,\n",
        "    'default_accuracy': accuracy_default,\n",
        "    'tuned_accuracy': accuracy_tuned,\n",
        "    'improvement': improvement,\n",
        "    'test_roc_auc': test_roc_auc,\n",
        "    'test_mcc': test_mcc\n",
        "}\n",
        "tuning_params_df = pd.DataFrame([tuning_params])\n",
        "tuning_params_df.to_excel('decision_tree_tuning_parameters.xlsx', index=False)\n",
        "\n",
        "print(f\"\\nüéØ DECISION TREE WITH HYPERPARAMETER TUNING COMPLETED!\")\n",
        "print(f\"üìä Training (10-Fold CV) Accuracy: {train_accuracy:.1%}\")\n",
        "print(f\"üìä Testing Accuracy: {test_accuracy:.1%}\")\n",
        "print(f\"üìà Training ROC-AUC: {train_roc_auc:.3f}\")\n",
        "print(f\"üìà Testing ROC-AUC: {test_roc_auc:.3f}\")\n",
        "print(f\"‚ö° Improvement over default: {improvement:+.2%}\")\n",
        "print(f\"üîç Performance metrics shown for BOTH classes: {class_names}\")\n",
        "\n",
        "print(f\"\\nüìÅ FILES GENERATED FOR ROC CURVES:\")\n",
        "print(f\"   1. decision_tree_roc_predictions.npz     - Main ROC data file\")\n",
        "print(f\"   2. decision_tree_predictions.csv         - Detailed test predictions\")\n",
        "print(f\"   3. decision_tree_metrics.csv             - Performance metrics\")\n",
        "print(f\"   4. decision_tree_all_predictions.csv     - All predictions (train+test)\")\n",
        "print(f\"   5. decision_tree_tuned_model.pkl         - Trained model\")\n",
        "print(f\"   6. decision_tree_grid_search_results.xlsx - Grid search results\")\n",
        "print(f\"   7. decision_tree_feature_importances.xlsx - Feature importance scores\")\n",
        "\n",
        "print(f\"\\nüéØ Use 'decision_tree_roc_predictions.npz' with Naive Bayes file for combined ROC plots!\")\n",
        "print(\"üå≥ TUNED DECISION TREE READY FOR DEPLOYMENT!\")"
      ],
      "metadata": {
        "id": "KhraX_MKKgKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **XGBoost Classifier:**"
      ],
      "metadata": {
        "id": "4pXu2VyqKlvF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Requirements: training-set-selected-features.xlsx, testing-set-selected-features.xlsx"
      ],
      "metadata": {
        "id": "jNpQ27T3OlpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "                           roc_auc_score, roc_curve, matthews_corrcoef, confusion_matrix,\n",
        "                           classification_report)\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, cross_val_predict, GridSearchCV\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "\n",
        "# Set publication-quality parameters (Fixed font settings)\n",
        "plt.rcParams['font.size'] = 12\n",
        "plt.rcParams['axes.labelsize'] = 14\n",
        "plt.rcParams['axes.titlesize'] = 16\n",
        "plt.rcParams['xtick.labelsize'] = 12\n",
        "plt.rcParams['ytick.labelsize'] = 12\n",
        "plt.rcParams['legend.fontsize'] = 12\n",
        "plt.rcParams['figure.titlesize'] = 18\n",
        "\n",
        "# Use available system fonts to avoid warnings\n",
        "plt.rcParams['font.family'] = ['DejaVu Sans', 'Arial', 'Helvetica', 'sans-serif']\n",
        "\n",
        "# Professional Elsevier color scheme\n",
        "elsevier_colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#1A936F', '#114B5F']\n",
        "\n",
        "print(\"üöÄ XGBOOST CLASSIFIER WITH HYPERPARAMETER TUNING (10-FOLD CV + GRID SEARCH)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# STEP 1: Load the feature-selected datasets\n",
        "print(\"üìä Loading feature-selected datasets...\")\n",
        "\n",
        "train_df = pd.read_excel('training_set_selected_features.xlsx')\n",
        "test_df = pd.read_excel('testing_set_selected_features.xlsx')\n",
        "\n",
        "print(f\"Training set: {train_df.shape}\")\n",
        "print(f\"Testing set: {test_df.shape}\")\n",
        "\n",
        "# STEP 2: Prepare features and target\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üîß PREPARING DATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Identify feature columns (exclude metadata)\n",
        "metadata_columns = ['COMPOUND ID', 'SMILE CODE', 'Ligand Type']\n",
        "feature_columns = [col for col in train_df.columns if col not in metadata_columns]\n",
        "\n",
        "X_train = train_df[feature_columns]\n",
        "y_train = train_df['Ligand Type']\n",
        "\n",
        "X_test = test_df[feature_columns]\n",
        "y_test = test_df['Ligand Type']\n",
        "\n",
        "print(f\"Selected features: {len(feature_columns)}\")\n",
        "print(f\"Training samples: {X_train.shape[0]}\")\n",
        "print(f\"Testing samples: {X_test.shape[0]}\")\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# FIXED: Properly get class names as strings\n",
        "class_names = [str(cls) for cls in label_encoder.classes_]\n",
        "print(f\"Class names: {class_names}\")\n",
        "\n",
        "print(\"‚úÖ Data prepared successfully!\")\n",
        "\n",
        "# STEP 3: Hyperparameter Tuning with GridSearchCV and 10-Fold CV\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéØ HYPERPARAMETER TUNING WITH GRIDSEARCHCV (10-FOLD CV)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Define parameter grid for XGBoost\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'subsample': [0.8, 0.9, 1.0],\n",
        "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
        "    'gamma': [0, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "# Initialize XGBoost\n",
        "xgb = XGBClassifier(random_state=42, eval_metric='logloss')\n",
        "\n",
        "# 10-fold cross-validation strategy\n",
        "cv_strategy = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# GridSearchCV with 10-fold CV\n",
        "print(\"üîç Performing Grid Search with 10-fold CV...\")\n",
        "print(f\"   Parameter grid: {param_grid}\")\n",
        "print(f\"   Total parameter combinations: {len(param_grid['n_estimators']) * len(param_grid['max_depth']) * len(param_grid['learning_rate']) * len(param_grid['subsample']) * len(param_grid['colsample_bytree']) * len(param_grid['gamma'])}\")\n",
        "print(f\"   This may take a while...\")\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=xgb,\n",
        "    param_grid=param_grid,\n",
        "    cv=cv_strategy,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Perform grid search\n",
        "grid_search.fit(X_train, y_train_encoded)\n",
        "\n",
        "print(\"‚úÖ Grid Search completed!\")\n",
        "\n",
        "# Display best parameters and scores\n",
        "print(f\"\\nüéØ BEST PARAMETERS FOUND:\")\n",
        "best_params = grid_search.best_params_\n",
        "for param, value in best_params.items():\n",
        "    print(f\"   {param}: {value}\")\n",
        "print(f\"   Best CV Score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# STEP 4: Compare Default vs Tuned Models\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä COMPARISON: DEFAULT vs TUNED PARAMETERS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Train model with default parameters for comparison\n",
        "xgb_default = XGBClassifier(random_state=42, eval_metric='logloss')\n",
        "xgb_default.fit(X_train, y_train_encoded)\n",
        "y_pred_default = xgb_default.predict(X_test)\n",
        "accuracy_default = accuracy_score(y_test_encoded, y_pred_default)\n",
        "\n",
        "# Use best model from grid search\n",
        "best_xgb_model = grid_search.best_estimator_\n",
        "y_pred_tuned = best_xgb_model.predict(X_test)\n",
        "accuracy_tuned = accuracy_score(y_test_encoded, y_pred_tuned)\n",
        "\n",
        "improvement = accuracy_tuned - accuracy_default\n",
        "\n",
        "print(f\"   Default Parameters Accuracy: {accuracy_default:.4f}\")\n",
        "print(f\"   Tuned Parameters Accuracy:   {accuracy_tuned:.4f}\")\n",
        "print(f\"   Improvement:                 {improvement:+.4f}\")\n",
        "\n",
        "if improvement > 0:\n",
        "    print(f\"   ‚úÖ Tuning improved accuracy by {improvement:.2%}\")\n",
        "else:\n",
        "    print(f\"   ‚ö†Ô∏è  Tuning did not improve accuracy\")\n",
        "\n",
        "# STEP 5: 10-Fold Cross-Validation with Tuned Model (for proper comparison)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéØ 10-FOLD CROSS-VALIDATION WITH TUNED PARAMETERS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"üöÄ Performing 10-fold cross-validation with tuned parameters...\")\n",
        "\n",
        "# Get cross-validated predictions for training set using tuned parameters\n",
        "cv_predictions = cross_val_predict(best_xgb_model, X_train, y_train_encoded,\n",
        "                                 cv=cv_strategy, method='predict')\n",
        "cv_probabilities = cross_val_predict(best_xgb_model, X_train, y_train_encoded,\n",
        "                                   cv=cv_strategy, method='predict_proba')\n",
        "\n",
        "# Calculate training metrics as average across 10-fold CV\n",
        "train_accuracy = accuracy_score(y_train_encoded, cv_predictions)\n",
        "\n",
        "# Calculate metrics for BOTH CLASSES\n",
        "train_precision = precision_score(y_train_encoded, cv_predictions, average=None)\n",
        "train_recall = recall_score(y_train_encoded, cv_predictions, average=None)\n",
        "train_f1 = f1_score(y_train_encoded, cv_predictions, average=None)\n",
        "train_roc_auc = roc_auc_score(y_train_encoded, cv_probabilities[:, 1])\n",
        "train_mcc = matthews_corrcoef(y_train_encoded, cv_predictions)\n",
        "\n",
        "# Also get individual fold accuracies for reporting\n",
        "cv_scores = cross_val_score(best_xgb_model, X_train, y_train_encoded,\n",
        "                          cv=cv_strategy, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "print(\"‚úÖ 10-fold cross-validation completed!\")\n",
        "\n",
        "print(f\"\\nüìä 10-FOLD CROSS-VALIDATION RESULTS (TRAINING PERFORMANCE):\")\n",
        "print(f\"   Fold scores: {[f'{score:.4f}' for score in cv_scores]}\")\n",
        "print(f\"   Mean CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
        "print(f\"   Standard Deviation: {cv_scores.std():.4f}\")\n",
        "\n",
        "# STEP 6: Make predictions on TEST set with tuned model\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä MODEL EVALUATION WITH TUNED PARAMETERS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Predictions on TEST set with tuned model\n",
        "y_test_pred = best_xgb_model.predict(X_test)\n",
        "y_test_proba = best_xgb_model.predict_proba(X_test)\n",
        "\n",
        "# Calculate metrics for TESTING set for BOTH CLASSES\n",
        "test_accuracy = accuracy_score(y_test_encoded, y_test_pred)\n",
        "test_precision = precision_score(y_test_encoded, y_test_pred, average=None)\n",
        "test_recall = recall_score(y_test_encoded, y_test_pred, average=None)\n",
        "test_f1 = f1_score(y_test_encoded, y_test_pred, average=None)\n",
        "test_roc_auc = roc_auc_score(y_test_encoded, y_test_proba[:, 1])\n",
        "test_mcc = matthews_corrcoef(y_test_encoded, y_test_pred)\n",
        "\n",
        "# Confusion matrices\n",
        "train_cm = confusion_matrix(y_train_encoded, cv_predictions)\n",
        "test_cm = confusion_matrix(y_test_encoded, y_test_pred)\n",
        "tn, fp, fn, tp = test_cm.ravel()\n",
        "\n",
        "# Additional metrics\n",
        "specificity = tn / (tn + fp)\n",
        "npv = tn / (tn + fn)\n",
        "\n",
        "# Performance gap\n",
        "accuracy_gap = train_accuracy - test_accuracy\n",
        "\n",
        "# Compute ROC curves for later saving\n",
        "fpr_train, tpr_train, _ = roc_curve(y_train_encoded, cv_probabilities[:, 1])\n",
        "fpr_test, tpr_test, _ = roc_curve(y_test_encoded, y_test_proba[:, 1])\n",
        "\n",
        "# STEP 7: Display comprehensive results for BOTH CLASSES\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìà COMPREHENSIVE PERFORMANCE METRICS FOR BOTH CLASSES (TUNED MODEL)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create detailed comparison dataframe for BOTH CLASSES\n",
        "class_performance_data = []\n",
        "\n",
        "for i, class_name in enumerate(class_names):\n",
        "    class_performance_data.extend([\n",
        "        {\n",
        "            'Class': class_name,\n",
        "            'Metric': 'Precision',\n",
        "            'Training (10-Fold CV)': f\"{train_precision[i]:.4f}\",\n",
        "            'Testing': f\"{test_precision[i]:.4f}\",\n",
        "            'Difference': f\"{train_precision[i]-test_precision[i]:+.4f}\"\n",
        "        },\n",
        "        {\n",
        "            'Class': class_name,\n",
        "            'Metric': 'Recall',\n",
        "            'Training (10-Fold CV)': f\"{train_recall[i]:.4f}\",\n",
        "            'Testing': f\"{test_recall[i]:.4f}\",\n",
        "            'Difference': f\"{train_recall[i]-test_recall[i]:+.4f}\"\n",
        "        },\n",
        "        {\n",
        "            'Class': class_name,\n",
        "            'Metric': 'F1-Score',\n",
        "            'Training (10-Fold CV)': f\"{train_f1[i]:.4f}\",\n",
        "            'Testing': f\"{test_f1[i]:.4f}\",\n",
        "            'Difference': f\"{train_f1[i]-test_f1[i]:+.4f}\"\n",
        "        }\n",
        "    ])\n",
        "\n",
        "# Add overall metrics\n",
        "overall_metrics = [\n",
        "    {\n",
        "        'Class': 'Overall',\n",
        "        'Metric': 'Accuracy',\n",
        "        'Training (10-Fold CV)': f\"{train_accuracy:.4f}\",\n",
        "        'Testing': f\"{test_accuracy:.4f}\",\n",
        "        'Difference': f\"{train_accuracy-test_accuracy:+.4f}\"\n",
        "    },\n",
        "    {\n",
        "        'Class': 'Overall',\n",
        "        'Metric': 'ROC-AUC',\n",
        "        'Training (10-Fold CV)': f\"{train_roc_auc:.4f}\",\n",
        "        'Testing': f\"{test_roc_auc:.4f}\",\n",
        "        'Difference': f\"{train_roc_auc-test_roc_auc:+.4f}\"\n",
        "    },\n",
        "    {\n",
        "        'Class': 'Overall',\n",
        "        'Metric': 'MCC',\n",
        "        'Training (10-Fold CV)': f\"{train_mcc:.4f}\",\n",
        "        'Testing': f\"{test_mcc:.4f}\",\n",
        "        'Difference': f\"{train_mcc-test_mcc:+.4f}\"\n",
        "    }\n",
        "]\n",
        "\n",
        "performance_df = pd.DataFrame(class_performance_data + overall_metrics)\n",
        "print(performance_df.to_string(index=False))\n",
        "\n",
        "# STEP 8: Enhanced Performance Visualization for Both Classes (WITH ROC CURVE)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä GENERATING ENHANCED PERFORMANCE VISUALIZATIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create comprehensive performance comparison for both classes\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 16))\n",
        "\n",
        "# 1. Main Performance Comparison (Overall Metrics)\n",
        "overall_metrics = ['Accuracy', 'ROC-AUC', 'MCC']\n",
        "train_overall = [train_accuracy, train_roc_auc, train_mcc]\n",
        "test_overall = [test_accuracy, test_roc_auc, test_mcc]\n",
        "\n",
        "x = np.arange(len(overall_metrics))\n",
        "width = 0.35\n",
        "\n",
        "bars1 = ax1.bar(x - width/2, train_overall, width, label='Training (10-Fold CV)',\n",
        "               color=elsevier_colors[0], alpha=0.8, edgecolor='black', linewidth=1.5)\n",
        "bars2 = ax1.bar(x + width/2, test_overall, width, label='Testing (Held-Out)',\n",
        "               color=elsevier_colors[1], alpha=0.8, edgecolor='black', linewidth=1.5)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bars in [bars1, bars2]:\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                f'{height:.3f}', ha='center', va='bottom',\n",
        "                fontweight='bold', fontsize=11)\n",
        "\n",
        "ax1.set_xlabel('Performance Metrics', fontsize=14, fontweight='bold')\n",
        "ax1.set_ylabel('Score', fontsize=14, fontweight='bold')\n",
        "ax1.set_title('XGBoost (Tuned) - Overall Performance\\nTraining vs Testing',\n",
        "              fontsize=16, fontweight='bold', pad=20)\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(overall_metrics)\n",
        "ax1.legend(fontsize=12, framealpha=0.9)\n",
        "ax1.set_ylim(0, 1.1)\n",
        "ax1.grid(True, alpha=0.3, linestyle='--')\n",
        "\n",
        "# 2. ROC CURVE for Training and Testing Sets\n",
        "ax2.plot(fpr_train, tpr_train, color=elsevier_colors[0], lw=2.5,\n",
        "         label=f'Training ROC (AUC = {train_roc_auc:.3f})', alpha=0.8)\n",
        "ax2.plot(fpr_test, tpr_test, color=elsevier_colors[1], lw=2.5,\n",
        "         label=f'Testing ROC (AUC = {test_roc_auc:.3f})', alpha=0.8)\n",
        "\n",
        "# Plot diagonal reference line\n",
        "ax2.plot([0, 1], [0, 1], color='gray', lw=1.5, linestyle='--', alpha=0.7,\n",
        "         label='Random Classifier (AUC = 0.5)')\n",
        "\n",
        "# Fill area under curves\n",
        "ax2.fill_between(fpr_train, tpr_train, alpha=0.2, color=elsevier_colors[0])\n",
        "ax2.fill_between(fpr_test, tpr_test, alpha=0.2, color=elsevier_colors[1])\n",
        "\n",
        "ax2.set_xlabel('False Positive Rate', fontsize=14, fontweight='bold')\n",
        "ax2.set_ylabel('True Positive Rate', fontsize=14, fontweight='bold')\n",
        "ax2.set_title('ROC Curves - Training vs Testing Sets\\nXGBoost (Tuned)',\n",
        "              fontsize=16, fontweight='bold', pad=20)\n",
        "ax2.legend(loc='lower right', fontsize=12, framealpha=0.9)\n",
        "ax2.grid(True, alpha=0.3, linestyle='--')\n",
        "ax2.set_xlim([0.0, 1.0])\n",
        "ax2.set_ylim([0.0, 1.05])\n",
        "\n",
        "# 3. Confusion Matrix - Training\n",
        "im1 = ax3.imshow(train_cm, cmap='Blues', interpolation='nearest', alpha=0.8)\n",
        "ax3.set_xlabel('Predicted Label', fontsize=14, fontweight='bold')\n",
        "ax3.set_ylabel('True Label', fontsize=14, fontweight='bold')\n",
        "ax3.set_title('Training Set - Confusion Matrix\\n(10-Fold CV Average)',\n",
        "              fontsize=16, fontweight='bold', pad=20)\n",
        "ax3.set_xticks([0, 1])\n",
        "ax3.set_yticks([0, 1])\n",
        "ax3.set_xticklabels(class_names)\n",
        "ax3.set_yticklabels(class_names)\n",
        "\n",
        "# Add text annotations for training CM\n",
        "for i in range(train_cm.shape[0]):\n",
        "    for j in range(train_cm.shape[1]):\n",
        "        ax3.text(j, i, f'{train_cm[i, j]}\\n({train_cm[i, j]/np.sum(train_cm):.1%})',\n",
        "                ha='center', va='center', fontsize=12, fontweight='bold',\n",
        "                color='white' if train_cm[i, j] > np.max(train_cm)/2 else 'black')\n",
        "\n",
        "# 4. Confusion Matrix - Testing\n",
        "im2 = ax4.imshow(test_cm, cmap='Reds', interpolation='nearest', alpha=0.8)\n",
        "ax4.set_xlabel('Predicted Label', fontsize=14, fontweight='bold')\n",
        "ax4.set_ylabel('True Label', fontsize=14, fontweight='bold')\n",
        "ax4.set_title('Testing Set - Confusion Matrix\\n(Held-Out Dataset)',\n",
        "              fontsize=16, fontweight='bold', pad=20)\n",
        "ax4.set_xticks([0, 1])\n",
        "ax4.set_yticks([0, 1])\n",
        "ax4.set_xticklabels(class_names)\n",
        "ax4.set_yticklabels(class_names)\n",
        "\n",
        "# Add text annotations for testing CM\n",
        "for i in range(test_cm.shape[0]):\n",
        "    for j in range(test_cm.shape[1]):\n",
        "        ax4.text(j, i, f'{test_cm[i, j]}\\n({test_cm[i, j]/np.sum(test_cm):.1%})',\n",
        "                ha='center', va='center', fontsize=12, fontweight='bold',\n",
        "                color='white' if test_cm[i, j] > np.max(test_cm)/2 else 'black')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('xgboost_tuned_comprehensive_performance.png', dpi=300,\n",
        "            bbox_inches='tight', facecolor='white')\n",
        "plt.show()\n",
        "\n",
        "# STEP 9: FIXED Classification Reports\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìã DETAILED CLASSIFICATION REPORTS FOR BOTH CLASSES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nTRAINING SET CLASSIFICATION REPORT (10-Fold CV Average):\")\n",
        "training_report = classification_report(y_train_encoded, cv_predictions,\n",
        "                                      target_names=class_names, digits=4)\n",
        "print(training_report)\n",
        "\n",
        "print(\"\\nTESTING SET CLASSIFICATION REPORT (Held-Out):\")\n",
        "testing_report = classification_report(y_test_encoded, y_test_pred,\n",
        "                                     target_names=class_names, digits=4)\n",
        "print(testing_report)\n",
        "\n",
        "# STEP 10: Individual Class Performance Analysis\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéØ INDIVIDUAL CLASS PERFORMANCE ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for i, class_name in enumerate(class_names):\n",
        "    print(f\"\\n{class_name.upper()} CLASS PERFORMANCE:\")\n",
        "    print(f\"  Training (10-Fold CV):\")\n",
        "    print(f\"    Precision: {train_precision[i]:.4f}\")\n",
        "    print(f\"    Recall:    {train_recall[i]:.4f}\")\n",
        "    print(f\"    F1-Score:  {train_f1[i]:.4f}\")\n",
        "    print(f\"  Testing (Held-Out):\")\n",
        "    print(f\"    Precision: {test_precision[i]:.4f}\")\n",
        "    print(f\"    Recall:    {test_recall[i]:.4f}\")\n",
        "    print(f\"    F1-Score:  {test_f1[i]:.4f}\")\n",
        "    print(f\"  Performance Gap:\")\n",
        "    print(f\"    Precision: {train_precision[i]-test_precision[i]:+.4f}\")\n",
        "    print(f\"    Recall:    {train_recall[i]-test_recall[i]:+.4f}\")\n",
        "    print(f\"    F1-Score:  {train_f1[i]-test_f1[i]:+.4f}\")\n",
        "\n",
        "# STEP 11: Tuning Results Summary\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üî¨ HYPERPARAMETER TUNING SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"üéØ BEST PARAMETERS FOUND:\")\n",
        "for param, value in best_params.items():\n",
        "    print(f\"   {param}: {value}\")\n",
        "\n",
        "print(f\"\\nüìä PERFORMANCE COMPARISON:\")\n",
        "print(f\"   Default Model Accuracy: {accuracy_default:.4f}\")\n",
        "print(f\"   Tuned Model Accuracy:   {accuracy_tuned:.4f}\")\n",
        "print(f\"   Improvement:            {improvement:+.4f}\")\n",
        "\n",
        "print(f\"\\nüöÄ FINAL TUNED MODEL PERFORMANCE:\")\n",
        "print(f\"   Training CV Accuracy: {train_accuracy:.4f}\")\n",
        "print(f\"   Test Accuracy:        {test_accuracy:.4f}\")\n",
        "print(f\"   Test ROC-AUC:         {test_roc_auc:.4f}\")\n",
        "print(f\"   Test MCC:             {test_mcc:.4f}\")\n",
        "\n",
        "# STEP 12: Save ROC Data for Later Use (LIKE OTHER MODELS)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üíæ SAVING ROC DATA FOR COMBINED VISUALIZATIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Save the tuned model\n",
        "joblib.dump(best_xgb_model, 'xgboost_tuned_model.pkl')\n",
        "joblib.dump(label_encoder, 'xgboost_tuned_label_encoder.pkl')\n",
        "\n",
        "# ============== SAVE ROC PREDICTION DATA ==============\n",
        "print(\"\\nüíæ SAVING ROC PREDICTION DATA FILES...\")\n",
        "\n",
        "# File 1: Save NPZ file for ROC plotting\n",
        "np.savez('xgboost_roc_predictions.npz',\n",
        "         # Training set data (from cross-validation)\n",
        "         y_train_true=y_train_encoded,\n",
        "         y_train_prob=cv_probabilities[:, 1],  # Probability for positive class\n",
        "\n",
        "         # Testing set data\n",
        "         y_test_true=y_test_encoded,\n",
        "         y_test_prob=y_test_proba[:, 1],  # Probability for positive class\n",
        "\n",
        "         # Precomputed ROC curve points\n",
        "         fpr_train=fpr_train,\n",
        "         tpr_train=tpr_train,\n",
        "         fpr_test=fpr_test,\n",
        "         tpr_test=tpr_test,\n",
        "\n",
        "         # AUC values\n",
        "         train_auc=train_roc_auc,\n",
        "         test_auc=test_roc_auc,\n",
        "\n",
        "         # Metadata\n",
        "         model_name='XGBoost',\n",
        "         class_names=class_names,\n",
        "         feature_count=len(feature_columns),\n",
        "         best_params=str(best_params))\n",
        "print(\"‚úÖ Saved: xgboost_roc_predictions.npz\")\n",
        "\n",
        "# File 2: Save detailed predictions CSV\n",
        "predictions_df = pd.DataFrame({\n",
        "    'true_label': y_test_encoded,\n",
        "    'predicted_label': y_test_pred,\n",
        "    'probability_class_0': y_test_proba[:, 0],\n",
        "    'probability_class_1': y_test_proba[:, 1],\n",
        "    'correct': (y_test_encoded == y_test_pred)\n",
        "})\n",
        "predictions_df.to_csv('xgboost_predictions.csv', index=False)\n",
        "print(\"‚úÖ Saved: xgboost_predictions.csv\")\n",
        "\n",
        "# File 3: Save comprehensive metrics CSV\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Model': ['XGBoost'],\n",
        "    'Train_Accuracy': [train_accuracy],\n",
        "    'Test_Accuracy': [test_accuracy],\n",
        "    'Train_AUC': [train_roc_auc],\n",
        "    'Test_AUC': [test_roc_auc],\n",
        "    'Train_Precision_0': [train_precision[0]],\n",
        "    'Train_Precision_1': [train_precision[1]],\n",
        "    'Test_Precision_0': [test_precision[0]],\n",
        "    'Test_Precision_1': [test_precision[1]],\n",
        "    'Train_Recall_0': [train_recall[0]],\n",
        "    'Train_Recall_1': [train_recall[1]],\n",
        "    'Test_Recall_0': [test_recall[0]],\n",
        "    'Test_Recall_1': [test_recall[1]],\n",
        "    'Train_F1_0': [train_f1[0]],\n",
        "    'Train_F1_1': [train_f1[1]],\n",
        "    'Test_F1_0': [test_f1[0]],\n",
        "    'Test_F1_1': [test_f1[1]],\n",
        "    'Train_MCC': [train_mcc],\n",
        "    'Test_MCC': [test_mcc],\n",
        "    'Num_Features': [len(feature_columns)],\n",
        "    'N_Estimators': [best_params.get('n_estimators', 'N/A')],\n",
        "    'Max_Depth': [best_params.get('max_depth', 'N/A')],\n",
        "    'Learning_Rate': [best_params.get('learning_rate', 'N/A')]\n",
        "})\n",
        "metrics_df.to_csv('xgboost_metrics.csv', index=False)\n",
        "print(\"‚úÖ Saved: xgboost_metrics.csv\")\n",
        "\n",
        "# File 4: Save all predictions (training + testing) for reference\n",
        "all_predictions_df = pd.DataFrame({\n",
        "    'dataset': ['train'] * len(y_train_encoded) + ['test'] * len(y_test_encoded),\n",
        "    'true_label': np.concatenate([y_train_encoded, y_test_encoded]),\n",
        "    'predicted_label': np.concatenate([cv_predictions, y_test_pred]),\n",
        "    'probability_class_1': np.concatenate([cv_probabilities[:, 1], y_test_proba[:, 1]])\n",
        "})\n",
        "all_predictions_df.to_csv('xgboost_all_predictions.csv', index=False)\n",
        "print(\"‚úÖ Saved: xgboost_all_predictions.csv\")\n",
        "\n",
        "# File 5: Save feature importance (XGBoost specific)\n",
        "feature_importances = best_xgb_model.feature_importances_\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': feature_columns,\n",
        "    'Importance': feature_importances\n",
        "}).sort_values('Importance', ascending=False)\n",
        "importance_df.to_excel('xgboost_feature_importances.xlsx', index=False)\n",
        "print(\"‚úÖ Saved: xgboost_feature_importances.xlsx\")\n",
        "\n",
        "# File 6: Save grid search results\n",
        "grid_results_df = pd.DataFrame(grid_search.cv_results_)\n",
        "grid_results_df.to_excel('xgboost_grid_search_results.xlsx', index=False)\n",
        "print(\"‚úÖ Saved: xgboost_grid_search_results.xlsx\")\n",
        "\n",
        "# File 7: Save tuning parameters\n",
        "tuning_params = {\n",
        "    'best_params': best_params,\n",
        "    'best_cv_score': grid_search.best_score_,\n",
        "    'default_accuracy': accuracy_default,\n",
        "    'tuned_accuracy': accuracy_tuned,\n",
        "    'improvement': improvement,\n",
        "    'test_roc_auc': test_roc_auc,\n",
        "    'test_mcc': test_mcc\n",
        "}\n",
        "tuning_params_df = pd.DataFrame([tuning_params])\n",
        "tuning_params_df.to_excel('xgboost_tuning_parameters.xlsx', index=False)\n",
        "\n",
        "print(f\"\\nüéØ XGBOOST WITH HYPERPARAMETER TUNING COMPLETED!\")\n",
        "print(f\"üìä Training (10-Fold CV) Accuracy: {train_accuracy:.1%}\")\n",
        "print(f\"üìä Testing Accuracy: {test_accuracy:.1%}\")\n",
        "print(f\"üìà Training ROC-AUC: {train_roc_auc:.3f}\")\n",
        "print(f\"üìà Testing ROC-AUC: {test_roc_auc:.3f}\")\n",
        "print(f\"‚ö° Improvement over default: {improvement:+.2%}\")\n",
        "print(f\"üîç Performance metrics shown for BOTH classes: {class_names}\")\n",
        "\n",
        "print(f\"\\nüìÅ FILES GENERATED FOR ROC CURVES:\")\n",
        "print(f\"   1. xgboost_roc_predictions.npz     - Main ROC data file\")\n",
        "print(f\"   2. xgboost_predictions.csv         - Detailed test predictions\")\n",
        "print(f\"   3. xgboost_metrics.csv             - Performance metrics\")\n",
        "print(f\"   4. xgboost_all_predictions.csv     - All predictions (train+test)\")\n",
        "print(f\"   5. xgboost_tuned_model.pkl         - Trained model\")\n",
        "print(f\"   6. xgboost_feature_importances.xlsx - Feature importance scores\")\n",
        "print(f\"   7. xgboost_grid_search_results.xlsx - Grid search results\")\n",
        "\n",
        "print(f\"\\nüéØ Use 'xgboost_roc_predictions.npz' with other model files for combined ROC plots!\")\n",
        "print(\"üöÄ TUNED XGBOOST MODEL READY FOR DEPLOYMENT!\")"
      ],
      "metadata": {
        "id": "B0yw2ti6Kpb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Naive Bayes:**"
      ],
      "metadata": {
        "id": "m13o2AxrKuZ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Requirements: training-set-selected-features.xlsx, testing-set-selected-features.xlsx"
      ],
      "metadata": {
        "id": "vl-omMQ4OpEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "                           roc_auc_score, roc_curve, matthews_corrcoef, confusion_matrix,\n",
        "                           classification_report)\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, cross_val_predict, GridSearchCV\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "\n",
        "# Set publication-quality parameters (Fixed font settings)\n",
        "plt.rcParams['font.size'] = 12\n",
        "plt.rcParams['axes.labelsize'] = 14\n",
        "plt.rcParams['axes.titlesize'] = 16\n",
        "plt.rcParams['xtick.labelsize'] = 12\n",
        "plt.rcParams['ytick.labelsize'] = 12\n",
        "plt.rcParams['legend.fontsize'] = 12\n",
        "plt.rcParams['figure.titlesize'] = 18\n",
        "\n",
        "# Use available system fonts to avoid warnings\n",
        "plt.rcParams['font.family'] = ['DejaVu Sans', 'Arial', 'Helvetica', 'sans-serif']\n",
        "\n",
        "# Professional Elsevier color scheme\n",
        "elsevier_colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#1A936F', '#114B5F']\n",
        "\n",
        "print(\"üöÄ NAIVE BAYES CLASSIFIER WITH HYPERPARAMETER TUNING (10-FOLD CV + GRID SEARCH)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# STEP 1: Load the feature-selected datasets\n",
        "print(\"üìä Loading feature-selected datasets...\")\n",
        "\n",
        "train_df = pd.read_excel('training_set_selected_features.xlsx')\n",
        "test_df = pd.read_excel('testing_set_selected_features.xlsx')\n",
        "\n",
        "print(f\"Training set: {train_df.shape}\")\n",
        "print(f\"Testing set: {test_df.shape}\")\n",
        "\n",
        "# STEP 2: Prepare features and target\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üîß PREPARING DATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Identify feature columns (exclude metadata)\n",
        "metadata_columns = ['COMPOUND ID', 'SMILE CODE', 'Ligand Type']\n",
        "feature_columns = [col for col in train_df.columns if col not in metadata_columns]\n",
        "\n",
        "X_train = train_df[feature_columns]\n",
        "y_train = train_df['Ligand Type']\n",
        "\n",
        "X_test = test_df[feature_columns]\n",
        "y_test = test_df['Ligand Type']\n",
        "\n",
        "print(f\"Selected features: {len(feature_columns)}\")\n",
        "print(f\"Training samples: {X_train.shape[0]}\")\n",
        "print(f\"Testing samples: {X_test.shape[0]}\")\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# FIXED: Properly get class names as strings\n",
        "class_names = [str(cls) for cls in label_encoder.classes_]\n",
        "print(f\"Class names: {class_names}\")\n",
        "\n",
        "print(\"‚úÖ Data prepared successfully!\")\n",
        "\n",
        "# STEP 3: Hyperparameter Tuning with GridSearchCV and 10-Fold CV\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéØ HYPERPARAMETER TUNING WITH GRIDSEARCHCV (10-FOLD CV)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Define parameter grid for Naive Bayes\n",
        "param_grid = {\n",
        "    'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1]\n",
        "}\n",
        "\n",
        "# Initialize Naive Bayes\n",
        "nb = GaussianNB()\n",
        "\n",
        "# 10-fold cross-validation strategy\n",
        "cv_strategy = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# GridSearchCV with 10-fold CV\n",
        "print(\"üîç Performing Grid Search with 10-fold CV...\")\n",
        "print(f\"   Parameter grid: {param_grid}\")\n",
        "print(f\"   Total parameter combinations: {len(param_grid['var_smoothing'])}\")\n",
        "print(f\"   This may take a while...\")\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=nb,\n",
        "    param_grid=param_grid,\n",
        "    cv=cv_strategy,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Perform grid search\n",
        "grid_search.fit(X_train, y_train_encoded)\n",
        "\n",
        "print(\"‚úÖ Grid Search completed!\")\n",
        "\n",
        "# Display best parameters and scores\n",
        "print(f\"\\nüéØ BEST PARAMETERS FOUND:\")\n",
        "best_params = grid_search.best_params_\n",
        "for param, value in best_params.items():\n",
        "    print(f\"   {param}: {value}\")\n",
        "print(f\"   Best CV Score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# STEP 4: Compare Default vs Tuned Models\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä COMPARISON: DEFAULT vs TUNED PARAMETERS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Train model with default parameters for comparison\n",
        "nb_default = GaussianNB()\n",
        "nb_default.fit(X_train, y_train_encoded)\n",
        "y_pred_default = nb_default.predict(X_test)\n",
        "accuracy_default = accuracy_score(y_test_encoded, y_pred_default)\n",
        "\n",
        "# Use best model from grid search\n",
        "best_nb_model = grid_search.best_estimator_\n",
        "y_pred_tuned = best_nb_model.predict(X_test)\n",
        "accuracy_tuned = accuracy_score(y_test_encoded, y_pred_tuned)\n",
        "\n",
        "improvement = accuracy_tuned - accuracy_default\n",
        "\n",
        "print(f\"   Default Parameters Accuracy: {accuracy_default:.4f}\")\n",
        "print(f\"   Tuned Parameters Accuracy:   {accuracy_tuned:.4f}\")\n",
        "print(f\"   Improvement:                 {improvement:+.4f}\")\n",
        "\n",
        "if improvement > 0:\n",
        "    print(f\"   ‚úÖ Tuning improved accuracy by {improvement:.2%}\")\n",
        "else:\n",
        "    print(f\"   ‚ö†Ô∏è  Tuning did not improve accuracy\")\n",
        "\n",
        "# STEP 5: 10-Fold Cross-Validation with Tuned Model (for proper comparison)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéØ 10-FOLD CROSS-VALIDATION WITH TUNED PARAMETERS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"üöÄ Performing 10-fold cross-validation with tuned parameters...\")\n",
        "\n",
        "# Get cross-validated predictions for training set using tuned parameters\n",
        "cv_predictions = cross_val_predict(best_nb_model, X_train, y_train_encoded,\n",
        "                                 cv=cv_strategy, method='predict')\n",
        "cv_probabilities = cross_val_predict(best_nb_model, X_train, y_train_encoded,\n",
        "                                   cv=cv_strategy, method='predict_proba')\n",
        "\n",
        "# Calculate training metrics as average across 10-fold CV\n",
        "train_accuracy = accuracy_score(y_train_encoded, cv_predictions)\n",
        "\n",
        "# Calculate metrics for BOTH CLASSES\n",
        "train_precision = precision_score(y_train_encoded, cv_predictions, average=None)\n",
        "train_recall = recall_score(y_train_encoded, cv_predictions, average=None)\n",
        "train_f1 = f1_score(y_train_encoded, cv_predictions, average=None)\n",
        "train_roc_auc = roc_auc_score(y_train_encoded, cv_probabilities[:, 1])\n",
        "train_mcc = matthews_corrcoef(y_train_encoded, cv_predictions)\n",
        "\n",
        "# Also get individual fold accuracies for reporting\n",
        "cv_scores = cross_val_score(best_nb_model, X_train, y_train_encoded,\n",
        "                          cv=cv_strategy, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "print(\"‚úÖ 10-fold cross-validation completed!\")\n",
        "\n",
        "print(f\"\\nüìä 10-FOLD CROSS-VALIDATION RESULTS (TRAINING PERFORMANCE):\")\n",
        "print(f\"   Fold scores: {[f'{score:.4f}' for score in cv_scores]}\")\n",
        "print(f\"   Mean CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
        "print(f\"   Standard Deviation: {cv_scores.std():.4f}\")\n",
        "\n",
        "# STEP 6: Make predictions on TEST set with tuned model\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä MODEL EVALUATION WITH TUNED PARAMETERS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Predictions on TEST set with tuned model\n",
        "y_test_pred = best_nb_model.predict(X_test)\n",
        "y_test_proba = best_nb_model.predict_proba(X_test)\n",
        "\n",
        "# Calculate metrics for TESTING set for BOTH CLASSES\n",
        "test_accuracy = accuracy_score(y_test_encoded, y_test_pred)\n",
        "test_precision = precision_score(y_test_encoded, y_test_pred, average=None)\n",
        "test_recall = recall_score(y_test_encoded, y_test_pred, average=None)\n",
        "test_f1 = f1_score(y_test_encoded, y_test_pred, average=None)\n",
        "test_roc_auc = roc_auc_score(y_test_encoded, y_test_proba[:, 1])\n",
        "test_mcc = matthews_corrcoef(y_test_encoded, y_test_pred)\n",
        "\n",
        "# Confusion matrices\n",
        "train_cm = confusion_matrix(y_train_encoded, cv_predictions)\n",
        "test_cm = confusion_matrix(y_test_encoded, y_test_pred)\n",
        "tn, fp, fn, tp = test_cm.ravel()\n",
        "\n",
        "# Additional metrics\n",
        "specificity = tn / (tn + fp)\n",
        "npv = tn / (tn + fn)\n",
        "\n",
        "# Performance gap\n",
        "accuracy_gap = train_accuracy - test_accuracy\n",
        "\n",
        "# STEP 7: Display comprehensive results for BOTH CLASSES\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìà COMPREHENSIVE PERFORMANCE METRICS FOR BOTH CLASSES (TUNED MODEL)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create detailed comparison dataframe for BOTH CLASSES\n",
        "class_performance_data = []\n",
        "\n",
        "for i, class_name in enumerate(class_names):\n",
        "    class_performance_data.extend([\n",
        "        {\n",
        "            'Class': class_name,\n",
        "            'Metric': 'Precision',\n",
        "            'Training (10-Fold CV)': f\"{train_precision[i]:.4f}\",\n",
        "            'Testing': f\"{test_precision[i]:.4f}\",\n",
        "            'Difference': f\"{train_precision[i]-test_precision[i]:+.4f}\"\n",
        "        },\n",
        "        {\n",
        "            'Class': class_name,\n",
        "            'Metric': 'Recall',\n",
        "            'Training (10-Fold CV)': f\"{train_recall[i]:.4f}\",\n",
        "            'Testing': f\"{test_recall[i]:.4f}\",\n",
        "            'Difference': f\"{train_recall[i]-test_recall[i]:+.4f}\"\n",
        "        },\n",
        "        {\n",
        "            'Class': class_name,\n",
        "            'Metric': 'F1-Score',\n",
        "            'Training (10-Fold CV)': f\"{train_f1[i]:.4f}\",\n",
        "            'Testing': f\"{test_f1[i]:.4f}\",\n",
        "            'Difference': f\"{train_f1[i]-test_f1[i]:+.4f}\"\n",
        "        }\n",
        "    ])\n",
        "\n",
        "# Add overall metrics\n",
        "overall_metrics = [\n",
        "    {\n",
        "        'Class': 'Overall',\n",
        "        'Metric': 'Accuracy',\n",
        "        'Training (10-Fold CV)': f\"{train_accuracy:.4f}\",\n",
        "        'Testing': f\"{test_accuracy:.4f}\",\n",
        "        'Difference': f\"{train_accuracy-test_accuracy:+.4f}\"\n",
        "    },\n",
        "    {\n",
        "        'Class': 'Overall',\n",
        "        'Metric': 'ROC-AUC',\n",
        "        'Training (10-Fold CV)': f\"{train_roc_auc:.4f}\",\n",
        "        'Testing': f\"{test_roc_auc:.4f}\",\n",
        "        'Difference': f\"{train_roc_auc-test_roc_auc:+.4f}\"\n",
        "    },\n",
        "    {\n",
        "        'Class': 'Overall',\n",
        "        'Metric': 'MCC',\n",
        "        'Training (10-Fold CV)': f\"{train_mcc:.4f}\",\n",
        "        'Testing': f\"{test_mcc:.4f}\",\n",
        "        'Difference': f\"{train_mcc-test_mcc:+.4f}\"\n",
        "    }\n",
        "]\n",
        "\n",
        "performance_df = pd.DataFrame(class_performance_data + overall_metrics)\n",
        "print(performance_df.to_string(index=False))\n",
        "\n",
        "# STEP 8: Enhanced Performance Visualization (WITH ROC CURVE)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä GENERATING ENHANCED PERFORMANCE VISUALIZATIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Compute ROC curves for training and testing\n",
        "fpr_train, tpr_train, _ = roc_curve(y_train_encoded, cv_probabilities[:, 1])\n",
        "fpr_test, tpr_test, _ = roc_curve(y_test_encoded, y_test_proba[:, 1])\n",
        "\n",
        "# Create comprehensive performance comparison\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 16))\n",
        "\n",
        "# 1. Main Performance Comparison (Overall Metrics)\n",
        "overall_metrics = ['Accuracy', 'ROC-AUC', 'MCC']\n",
        "train_overall = [train_accuracy, train_roc_auc, train_mcc]\n",
        "test_overall = [test_accuracy, test_roc_auc, test_mcc]\n",
        "\n",
        "x = np.arange(len(overall_metrics))\n",
        "width = 0.35\n",
        "\n",
        "bars1 = ax1.bar(x - width/2, train_overall, width, label='Training (10-Fold CV)',\n",
        "               color=elsevier_colors[0], alpha=0.8, edgecolor='black', linewidth=1.5)\n",
        "bars2 = ax1.bar(x + width/2, test_overall, width, label='Testing (Held-Out)',\n",
        "               color=elsevier_colors[1], alpha=0.8, edgecolor='black', linewidth=1.5)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bars in [bars1, bars2]:\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                f'{height:.3f}', ha='center', va='bottom',\n",
        "                fontweight='bold', fontsize=11)\n",
        "\n",
        "ax1.set_xlabel('Performance Metrics', fontsize=14, fontweight='bold')\n",
        "ax1.set_ylabel('Score', fontsize=14, fontweight='bold')\n",
        "ax1.set_title('Naive Bayes (Tuned) - Overall Performance\\nTraining vs Testing',\n",
        "              fontsize=16, fontweight='bold', pad=20)\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(overall_metrics)\n",
        "ax1.legend(fontsize=12, framealpha=0.9)\n",
        "ax1.set_ylim(0, 1.1)\n",
        "ax1.grid(True, alpha=0.3, linestyle='--')\n",
        "\n",
        "# 2. ROC CURVE for Training and Testing Sets\n",
        "ax2.plot(fpr_train, tpr_train, color=elsevier_colors[0], lw=2.5,\n",
        "         label=f'Training ROC (AUC = {train_roc_auc:.3f})', alpha=0.8)\n",
        "ax2.plot(fpr_test, tpr_test, color=elsevier_colors[1], lw=2.5,\n",
        "         label=f'Testing ROC (AUC = {test_roc_auc:.3f})', alpha=0.8)\n",
        "\n",
        "# Plot diagonal reference line\n",
        "ax2.plot([0, 1], [0, 1], color='gray', lw=1.5, linestyle='--', alpha=0.7,\n",
        "         label='Random Classifier (AUC = 0.5)')\n",
        "\n",
        "# Fill area under curves\n",
        "ax2.fill_between(fpr_train, tpr_train, alpha=0.2, color=elsevier_colors[0])\n",
        "ax2.fill_between(fpr_test, tpr_test, alpha=0.2, color=elsevier_colors[1])\n",
        "\n",
        "ax2.set_xlabel('False Positive Rate', fontsize=14, fontweight='bold')\n",
        "ax2.set_ylabel('True Positive Rate', fontsize=14, fontweight='bold')\n",
        "ax2.set_title('ROC Curves - Training vs Testing Sets\\nNaive Bayes (Tuned)',\n",
        "              fontsize=16, fontweight='bold', pad=20)\n",
        "ax2.legend(loc='lower right', fontsize=12, framealpha=0.9)\n",
        "ax2.grid(True, alpha=0.3, linestyle='--')\n",
        "ax2.set_xlim([0.0, 1.0])\n",
        "ax2.set_ylim([0.0, 1.05])\n",
        "\n",
        "# 3. Confusion Matrix - Training\n",
        "im1 = ax3.imshow(train_cm, cmap='Blues', interpolation='nearest', alpha=0.8)\n",
        "ax3.set_xlabel('Predicted Label', fontsize=14, fontweight='bold')\n",
        "ax3.set_ylabel('True Label', fontsize=14, fontweight='bold')\n",
        "ax3.set_title('Training Set - Confusion Matrix\\n(10-Fold CV Average)',\n",
        "              fontsize=16, fontweight='bold', pad=20)\n",
        "ax3.set_xticks([0, 1])\n",
        "ax3.set_yticks([0, 1])\n",
        "ax3.set_xticklabels(class_names)\n",
        "ax3.set_yticklabels(class_names)\n",
        "\n",
        "# Add text annotations for training CM\n",
        "for i in range(train_cm.shape[0]):\n",
        "    for j in range(train_cm.shape[1]):\n",
        "        ax3.text(j, i, f'{train_cm[i, j]}\\n({train_cm[i, j]/np.sum(train_cm):.1%})',\n",
        "                ha='center', va='center', fontsize=12, fontweight='bold',\n",
        "                color='white' if train_cm[i, j] > np.max(train_cm)/2 else 'black')\n",
        "\n",
        "# 4. Confusion Matrix - Testing\n",
        "im2 = ax4.imshow(test_cm, cmap='Reds', interpolation='nearest', alpha=0.8)\n",
        "ax4.set_xlabel('Predicted Label', fontsize=14, fontweight='bold')\n",
        "ax4.set_ylabel('True Label', fontsize=14, fontweight='bold')\n",
        "ax4.set_title('Testing Set - Confusion Matrix\\n(Held-Out Dataset)',\n",
        "              fontsize=16, fontweight='bold', pad=20)\n",
        "ax4.set_xticks([0, 1])\n",
        "ax4.set_yticks([0, 1])\n",
        "ax4.set_xticklabels(class_names)\n",
        "ax4.set_yticklabels(class_names)\n",
        "\n",
        "# Add text annotations for testing CM\n",
        "for i in range(test_cm.shape[0]):\n",
        "    for j in range(test_cm.shape[1]):\n",
        "        ax4.text(j, i, f'{test_cm[i, j]}\\n({test_cm[i, j]/np.sum(test_cm):.1%})',\n",
        "                ha='center', va='center', fontsize=12, fontweight='bold',\n",
        "                color='white' if test_cm[i, j] > np.max(test_cm)/2 else 'black')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('naive_bayes_tuned_comprehensive_performance.png', dpi=300,\n",
        "            bbox_inches='tight', facecolor='white')\n",
        "plt.show()\n",
        "\n",
        "# STEP 9: Classification Reports\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìã DETAILED CLASSIFICATION REPORTS FOR BOTH CLASSES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nTRAINING SET CLASSIFICATION REPORT (10-Fold CV Average):\")\n",
        "training_report = classification_report(y_train_encoded, cv_predictions,\n",
        "                                      target_names=class_names, digits=4)\n",
        "print(training_report)\n",
        "\n",
        "print(\"\\nTESTING SET CLASSIFICATION REPORT (Held-Out):\")\n",
        "testing_report = classification_report(y_test_encoded, y_test_pred,\n",
        "                                     target_names=class_names, digits=4)\n",
        "print(testing_report)\n",
        "\n",
        "# STEP 10: Individual Class Performance Analysis\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéØ INDIVIDUAL CLASS PERFORMANCE ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for i, class_name in enumerate(class_names):\n",
        "    print(f\"\\n{class_name.upper()} CLASS PERFORMANCE:\")\n",
        "    print(f\"  Training (10-Fold CV):\")\n",
        "    print(f\"    Precision: {train_precision[i]:.4f}\")\n",
        "    print(f\"    Recall:    {train_recall[i]:.4f}\")\n",
        "    print(f\"    F1-Score:  {train_f1[i]:.4f}\")\n",
        "    print(f\"  Testing (Held-Out):\")\n",
        "    print(f\"    Precision: {test_precision[i]:.4f}\")\n",
        "    print(f\"    Recall:    {test_recall[i]:.4f}\")\n",
        "    print(f\"    F1-Score:  {test_f1[i]:.4f}\")\n",
        "    print(f\"  Performance Gap:\")\n",
        "    print(f\"    Precision: {train_precision[i]-test_precision[i]:+.4f}\")\n",
        "    print(f\"    Recall:    {train_recall[i]-test_recall[i]:+.4f}\")\n",
        "    print(f\"    F1-Score:  {train_f1[i]-test_f1[i]:+.4f}\")\n",
        "\n",
        "# STEP 11: Tuning Results Summary\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üî¨ HYPERPARAMETER TUNING SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"üéØ BEST PARAMETERS FOUND:\")\n",
        "for param, value in best_params.items():\n",
        "    print(f\"   {param}: {value}\")\n",
        "\n",
        "print(f\"\\nüìä PERFORMANCE COMPARISON:\")\n",
        "print(f\"   Default Model Accuracy: {accuracy_default:.4f}\")\n",
        "print(f\"   Tuned Model Accuracy:   {accuracy_tuned:.4f}\")\n",
        "print(f\"   Improvement:            {improvement:+.4f}\")\n",
        "\n",
        "print(f\"\\nüöÄ FINAL TUNED MODEL PERFORMANCE:\")\n",
        "print(f\"   Training CV Accuracy: {train_accuracy:.4f}\")\n",
        "print(f\"   Test Accuracy:        {test_accuracy:.4f}\")\n",
        "print(f\"   Test ROC-AUC:         {test_roc_auc:.4f}\")\n",
        "print(f\"   Test MCC:             {test_mcc:.4f}\")\n",
        "\n",
        "# STEP 12: SAVE ALL ROC DATA FOR LATER USE\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üíæ SAVING ROC DATA FOR COMBINED VISUALIZATIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Save the tuned model\n",
        "joblib.dump(best_nb_model, 'naive_bayes_tuned_model.pkl')\n",
        "joblib.dump(label_encoder, 'naive_bayes_tuned_label_encoder.pkl')\n",
        "\n",
        "# ============== SAVE ROC PREDICTION DATA ==============\n",
        "print(\"\\nüíæ SAVING ROC PREDICTION DATA FILES...\")\n",
        "\n",
        "# File 1: Save NPZ file for ROC plotting\n",
        "np.savez('naive_bayes_roc_predictions.npz',\n",
        "         # Training set data (from cross-validation)\n",
        "         y_train_true=y_train_encoded,\n",
        "         y_train_prob=cv_probabilities[:, 1],  # Probability for positive class\n",
        "\n",
        "         # Testing set data\n",
        "         y_test_true=y_test_encoded,\n",
        "         y_test_prob=y_test_proba[:, 1],  # Probability for positive class\n",
        "\n",
        "         # Precomputed ROC curve points\n",
        "         fpr_train=fpr_train,\n",
        "         tpr_train=tpr_train,\n",
        "         fpr_test=fpr_test,\n",
        "         tpr_test=tpr_test,\n",
        "\n",
        "         # AUC values\n",
        "         train_auc=train_roc_auc,\n",
        "         test_auc=test_roc_auc,\n",
        "\n",
        "         # Metadata\n",
        "         model_name='Naive Bayes',\n",
        "         class_names=class_names,\n",
        "         feature_count=len(feature_columns))\n",
        "print(\"‚úÖ Saved: naive_bayes_roc_predictions.npz\")\n",
        "\n",
        "# File 2: Save detailed predictions CSV\n",
        "predictions_df = pd.DataFrame({\n",
        "    'true_label': y_test_encoded,\n",
        "    'predicted_label': y_test_pred,\n",
        "    'probability_class_0': y_test_proba[:, 0],\n",
        "    'probability_class_1': y_test_proba[:, 1],\n",
        "    'correct': (y_test_encoded == y_test_pred)\n",
        "})\n",
        "predictions_df.to_csv('naive_bayes_predictions.csv', index=False)\n",
        "print(\"‚úÖ Saved: naive_bayes_predictions.csv\")\n",
        "\n",
        "# File 3: Save comprehensive metrics CSV\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Model': ['Naive Bayes'],\n",
        "    'Train_Accuracy': [train_accuracy],\n",
        "    'Test_Accuracy': [test_accuracy],\n",
        "    'Train_AUC': [train_roc_auc],\n",
        "    'Test_AUC': [test_roc_auc],\n",
        "    'Train_Precision_0': [train_precision[0]],\n",
        "    'Train_Precision_1': [train_precision[1]],\n",
        "    'Test_Precision_0': [test_precision[0]],\n",
        "    'Test_Precision_1': [test_precision[1]],\n",
        "    'Train_Recall_0': [train_recall[0]],\n",
        "    'Train_Recall_1': [train_recall[1]],\n",
        "    'Test_Recall_0': [test_recall[0]],\n",
        "    'Test_Recall_1': [test_recall[1]],\n",
        "    'Train_F1_0': [train_f1[0]],\n",
        "    'Train_F1_1': [train_f1[1]],\n",
        "    'Test_F1_0': [test_f1[0]],\n",
        "    'Test_F1_1': [test_f1[1]],\n",
        "    'Train_MCC': [train_mcc],\n",
        "    'Test_MCC': [test_mcc],\n",
        "    'Num_Features': [len(feature_columns)]\n",
        "})\n",
        "metrics_df.to_csv('naive_bayes_metrics.csv', index=False)\n",
        "print(\"‚úÖ Saved: naive_bayes_metrics.csv\")\n",
        "\n",
        "# File 4: Save all predictions (training + testing) for reference\n",
        "all_predictions_df = pd.DataFrame({\n",
        "    'dataset': ['train'] * len(y_train_encoded) + ['test'] * len(y_test_encoded),\n",
        "    'true_label': np.concatenate([y_train_encoded, y_test_encoded]),\n",
        "    'predicted_label': np.concatenate([cv_predictions, y_test_pred]),\n",
        "    'probability_class_1': np.concatenate([cv_probabilities[:, 1], y_test_proba[:, 1]])\n",
        "})\n",
        "all_predictions_df.to_csv('naive_bayes_all_predictions.csv', index=False)\n",
        "print(\"‚úÖ Saved: naive_bayes_all_predictions.csv\")\n",
        "\n",
        "# File 5: Save grid search results\n",
        "grid_results_df = pd.DataFrame(grid_search.cv_results_)\n",
        "grid_results_df.to_excel('naive_bayes_grid_search_results.xlsx', index=False)\n",
        "print(\"‚úÖ Saved: naive_bayes_grid_search_results.xlsx\")\n",
        "\n",
        "print(f\"\\nüéØ NAIVE BAYES WITH HYPERPARAMETER TUNING COMPLETED!\")\n",
        "print(f\"üìä Training (10-Fold CV) Accuracy: {train_accuracy:.1%}\")\n",
        "print(f\"üìä Testing Accuracy: {test_accuracy:.1%}\")\n",
        "print(f\"üìà Training ROC-AUC: {train_roc_auc:.3f}\")\n",
        "print(f\"üìà Testing ROC-AUC: {test_roc_auc:.3f}\")\n",
        "print(f\"‚ö° Improvement over default: {improvement:+.2%}\")\n",
        "print(f\"üîç Performance metrics shown for BOTH classes: {class_names}\")\n",
        "\n",
        "print(f\"\\nüìÅ FILES GENERATED FOR ROC CURVES:\")\n",
        "print(f\"   1. naive_bayes_roc_predictions.npz     - Main ROC data file\")\n",
        "print(f\"   2. naive_bayes_predictions.csv         - Detailed test predictions\")\n",
        "print(f\"   3. naive_bayes_metrics.csv             - Performance metrics\")\n",
        "print(f\"   4. naive_bayes_all_predictions.csv     - All predictions (train+test)\")\n",
        "print(f\"   5. naive_bayes_tuned_model.pkl         - Trained model\")\n",
        "print(f\"   6. naive_bayes_grid_search_results.xlsx - Grid search results\")\n",
        "\n",
        "print(f\"\\nüéØ Use 'naive_bayes_roc_predictions.npz' for creating combined ROC plots!\")\n",
        "print(\"üöÄ TUNED MODEL READY FOR DEPLOYMENT!\")"
      ],
      "metadata": {
        "id": "7vvMFMjEKxcI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}