{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **SHAP Analysis:**"
      ],
      "metadata": {
        "id": "Dkasro30UYGW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Required Files:\n",
        "training-set-selected-features.xlsx,\n",
        "testing-set-selected-features.xlsx"
      ],
      "metadata": {
        "id": "kGZ9vcLwVeEM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pG0uJdGsUXXH"
      },
      "outputs": [],
      "source": [
        "pip install RDKit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "import shap\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"ðŸš€ SHAP ANALYSIS FOR XGBOOST MODEL - TOP 30 FEATURES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Set publication-quality parameters\n",
        "plt.rcParams['font.size'] = 12\n",
        "plt.rcParams['axes.labelsize'] = 14\n",
        "plt.rcParams['axes.titlesize'] = 16\n",
        "plt.rcParams['xtick.labelsize'] = 12\n",
        "plt.rcParams['ytick.labelsize'] = 12\n",
        "plt.rcParams['legend.fontsize'] = 12\n",
        "plt.rcParams['figure.titlesize'] = 18\n",
        "plt.rcParams['font.family'] = ['DejaVu Sans', 'Arial', 'Helvetica', 'sans-serif']\n",
        "\n",
        "# Professional Elsevier color scheme\n",
        "elsevier_colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#1A936F', '#114B5F']\n",
        "\n",
        "# STEP 1: Load the same datasets you used in original training\n",
        "print(\"ðŸ“Š Loading datasets...\")\n",
        "train_df = pd.read_excel('training_set_selected_features.xlsx')\n",
        "test_df = pd.read_excel('testing_set_selected_features.xlsx')\n",
        "\n",
        "print(f\"Training set: {train_df.shape}\")\n",
        "print(f\"Testing set: {test_df.shape}\")\n",
        "\n",
        "# STEP 2: Prepare features exactly like in your original code\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ðŸ”§ PREPARING DATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "metadata_columns = ['COMPOUND ID', 'SMILE CODE', 'Ligand Type']\n",
        "feature_columns = [col for col in train_df.columns if col not in metadata_columns]\n",
        "\n",
        "X_train = train_df[feature_columns]\n",
        "y_train = train_df['Ligand Type']\n",
        "X_test = test_df[feature_columns]\n",
        "y_test = test_df['Ligand Type']\n",
        "\n",
        "print(f\"Selected features: {len(feature_columns)}\")\n",
        "print(f\"Training samples: {X_train.shape[0]}\")\n",
        "print(f\"Testing samples: {X_test.shape[0]}\")\n",
        "\n",
        "# STEP 3: Load your saved XGBoost model\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ðŸ“‚ LOADING SAVED XGBOOST MODEL\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "best_xgb_model = joblib.load('xgboost_tuned_model.pkl')\n",
        "label_encoder = joblib.load('xgboost_tuned_label_encoder.pkl')\n",
        "y_train_encoded = label_encoder.transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "class_names = [str(cls) for cls in label_encoder.classes_]\n",
        "print(f\"Model loaded successfully!\")\n",
        "print(f\"Class names: {class_names}\")\n",
        "\n",
        "# STEP 4: SHAP Analysis\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ðŸ” SHAP ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Initialize SHAP TreeExplainer\n",
        "explainer = shap.TreeExplainer(best_xgb_model)\n",
        "\n",
        "# Use subset if training data is large\n",
        "if X_train.shape[0] > 1000:\n",
        "    sample_indices = np.random.choice(X_train.index, 1000, replace=False)\n",
        "    X_shap_sample = X_train.loc[sample_indices]\n",
        "    y_shap_sample = y_train_encoded[sample_indices]\n",
        "    print(f\"Sampling 1000 instances from training set\")\n",
        "else:\n",
        "    X_shap_sample = X_train\n",
        "    y_shap_sample = y_train_encoded\n",
        "    print(f\"Using all {X_train.shape[0]} training instances\")\n",
        "\n",
        "# Calculate SHAP values\n",
        "print(\"Calculating SHAP values...\")\n",
        "shap_values = explainer.shap_values(X_shap_sample)\n",
        "base_value = explainer.expected_value\n",
        "\n",
        "print(f\"\\nðŸŽ¯ Base value (expected model output): {base_value:.4f}\")\n",
        "print(\"   Note: Positive SHAP values push prediction towards Class 1\")\n",
        "print(\"         Negative SHAP values push prediction towards Class 0\")\n",
        "\n",
        "# Get SHAP importance\n",
        "shap_sum = np.abs(shap_values).mean(axis=0)\n",
        "shap_importance = pd.DataFrame({\n",
        "    'Feature': feature_columns,\n",
        "    'SHAP_Importance': shap_sum\n",
        "}).sort_values('SHAP_Importance', ascending=False)\n",
        "\n",
        "print(\"\\nðŸ“Š Top 30 Features by SHAP Importance:\")\n",
        "print(shap_importance.head(30).to_string(index=False))\n",
        "\n",
        "# STEP 5: SHAP Summary Plot (Top 30 features)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ðŸ“ˆ GENERATING SHAP SUMMARY PLOT (Top 30 Features)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "plt.figure(figsize=(16, 10))\n",
        "shap.summary_plot(shap_values, X_shap_sample,\n",
        "                 feature_names=feature_columns,\n",
        "                 show=False, max_display=30, plot_size=None)\n",
        "plt.title(\"SHAP Feature Importance Summary\\nXGBoost Model - Top 30 Features\",\n",
        "          fontsize=18, fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.savefig('xgboost_shap_summary_top30.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
        "plt.show()\n",
        "\n",
        "# STEP 6: SHAP Bar Plot (Top 30 features)\n",
        "print(\"\\nðŸ“Š GENERATING SHAP BAR PLOT (Top 30 Features)\")\n",
        "\n",
        "# Prepare data for bar plot\n",
        "top_30_features = shap_importance.head(30).copy()\n",
        "top_30_features = top_30_features.sort_values('SHAP_Importance', ascending=True)  # For horizontal bar plot\n",
        "\n",
        "plt.figure(figsize=(14, 12))\n",
        "bars = plt.barh(range(len(top_30_features)), top_30_features['SHAP_Importance'],\n",
        "                color=plt.cm.viridis(np.linspace(0.2, 0.8, len(top_30_features))),\n",
        "                edgecolor='black', linewidth=0.5)\n",
        "\n",
        "# Add value labels on bars\n",
        "for i, (bar, value) in enumerate(zip(bars, top_30_features['SHAP_Importance'])):\n",
        "    plt.text(value + 0.001, bar.get_y() + bar.get_height()/2,\n",
        "             f'{value:.4f}', ha='left', va='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "plt.yticks(range(len(top_30_features)), top_30_features['Feature'])\n",
        "plt.xlabel('Mean |SHAP Value| (Feature Importance)', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Features', fontsize=14, fontweight='bold')\n",
        "plt.title('Top 30 Features by SHAP Importance\\nXGBoost Model',\n",
        "          fontsize=18, fontweight='bold', pad=20)\n",
        "plt.grid(True, axis='x', alpha=0.3, linestyle='--')\n",
        "plt.tight_layout()\n",
        "plt.savefig('xgboost_shap_barplot_top30.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
        "plt.show()\n",
        "\n",
        "# STEP 7: Enhanced SHAP Bar Plot with Direction Information\n",
        "print(\"\\nðŸ“ˆ GENERATING ENHANCED SHAP PLOT WITH DIRECTION\")\n",
        "\n",
        "# Calculate mean SHAP values (with sign) for direction\n",
        "mean_shap_values = np.mean(shap_values, axis=0)\n",
        "top_30_features_with_direction = top_30_features.copy()\n",
        "top_30_features_with_direction['Mean_SHAP'] = [mean_shap_values[feature_columns.index(f)]\n",
        "                                                for f in top_30_features_with_direction['Feature']]\n",
        "top_30_features_with_direction['Direction'] = top_30_features_with_direction['Mean_SHAP'].apply(\n",
        "    lambda x: 'Positive' if x > 0 else 'Negative' if x < 0 else 'Neutral'\n",
        ")\n",
        "top_30_features_with_direction = top_30_features_with_direction.sort_values('SHAP_Importance', ascending=False)\n",
        "\n",
        "# Plot\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 12))\n",
        "\n",
        "# Bar plot on left\n",
        "bars1 = ax1.barh(range(len(top_30_features_with_direction)),\n",
        "                 top_30_features_with_direction['SHAP_Importance'],\n",
        "                 color=['#2E86AB' if d == 'Positive' else '#C73E1D'\n",
        "                        for d in top_30_features_with_direction['Direction']],\n",
        "                 edgecolor='black', linewidth=0.8, alpha=0.8)\n",
        "\n",
        "ax1.set_yticks(range(len(top_30_features_with_direction)))\n",
        "ax1.set_yticklabels(top_30_features_with_direction['Feature'])\n",
        "ax1.set_xlabel('Mean |SHAP Value|', fontsize=14, fontweight='bold')\n",
        "ax1.set_title('Top 30 Features by Absolute SHAP Value\\n(Color shows direction)',\n",
        "              fontsize=16, fontweight='bold', pad=20)\n",
        "ax1.grid(True, axis='x', alpha=0.3, linestyle='--')\n",
        "\n",
        "# Add legend for colors\n",
        "import matplotlib.patches as mpatches\n",
        "pos_patch = mpatches.Patch(color='#2E86AB', label='Positive effect (towards Class 1)')\n",
        "neg_patch = mpatches.Patch(color='#C73E1D', label='Negative effect (towards Class 0)')\n",
        "ax1.legend(handles=[pos_patch, neg_patch], loc='lower right', fontsize=11, framealpha=0.9)\n",
        "\n",
        "# Direction plot on right\n",
        "bars2 = ax2.barh(range(len(top_30_features_with_direction)),\n",
        "                 top_30_features_with_direction['Mean_SHAP'],\n",
        "                 color=['#2E86AB' if x > 0 else '#C73E1D'\n",
        "                        for x in top_30_features_with_direction['Mean_SHAP']],\n",
        "                 edgecolor='black', linewidth=0.8, alpha=0.8)\n",
        "\n",
        "# Add zero line\n",
        "ax2.axvline(x=0, color='black', linestyle='-', linewidth=1, alpha=0.5)\n",
        "\n",
        "ax2.set_yticks(range(len(top_30_features_with_direction)))\n",
        "ax2.set_yticklabels([])  # Hide y labels on right plot\n",
        "ax2.set_xlabel('Mean SHAP Value (with sign)', fontsize=14, fontweight='bold')\n",
        "ax2.set_title('Direction of Feature Effects\\n(Positive = towards Class 1, Negative = towards Class 0)',\n",
        "              fontsize=16, fontweight='bold', pad=20)\n",
        "ax2.grid(True, alpha=0.3, linestyle='--')\n",
        "\n",
        "# Add value labels to both plots\n",
        "for i, (bar1, bar2) in enumerate(zip(bars1, bars2)):\n",
        "    # Left plot labels\n",
        "    ax1.text(bar1.get_width() + 0.001, bar1.get_y() + bar1.get_height()/2,\n",
        "             f'{bar1.get_width():.4f}', ha='left', va='center', fontsize=9)\n",
        "    # Right plot labels\n",
        "    ax2.text(bar2.get_width() + (0.01 if bar2.get_width() > 0 else -0.01),\n",
        "             bar2.get_y() + bar2.get_height()/2,\n",
        "             f'{bar2.get_width():.4f}',\n",
        "             ha='left' if bar2.get_width() > 0 else 'right',\n",
        "             va='center', fontsize=9)\n",
        "\n",
        "plt.suptitle('SHAP Analysis: Top 30 Feature Importance with Direction',\n",
        "             fontsize=20, fontweight='bold', y=0.98)\n",
        "plt.tight_layout()\n",
        "plt.savefig('xgboost_shap_detailed_top30.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
        "plt.show()\n",
        "\n",
        "# STEP 8: SHAP Dependence Plots for Top 6 Features\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ðŸ“Š GENERATING SHAP DEPENDENCE PLOTS FOR TOP 6 FEATURES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "top_6_features = shap_importance['Feature'].head(6).tolist()\n",
        "print(f\"Top 6 features for dependence plots: {top_6_features}\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(20, 14))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, feature in enumerate(top_6_features):\n",
        "    if feature in feature_columns:\n",
        "        shap.dependence_plot(feature, shap_values, X_shap_sample,\n",
        "                           feature_names=feature_columns,\n",
        "                           ax=axes[idx], show=False)\n",
        "        axes[idx].set_title(f'SHAP Dependence: {feature}\\n(Feature #{idx+1})',\n",
        "                          fontsize=14, fontweight='bold')\n",
        "        axes[idx].set_xlabel(feature, fontsize=12, fontweight='bold')\n",
        "        axes[idx].set_ylabel('SHAP Value', fontsize=12, fontweight='bold')\n",
        "        axes[idx].grid(True, alpha=0.3, linestyle='--')\n",
        "\n",
        "        # Add correlation info\n",
        "        feature_idx = feature_columns.index(feature)\n",
        "        corr_coef = np.corrcoef(X_shap_sample.iloc[:, feature_idx], shap_values[:, feature_idx])[0, 1]\n",
        "        axes[idx].text(0.05, 0.95, f'Correlation: {corr_coef:.3f}',\n",
        "                      transform=axes[idx].transAxes, fontsize=11,\n",
        "                      verticalalignment='top',\n",
        "                      bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "plt.suptitle('SHAP Dependence Plots for Top 6 Features\\n(How feature values affect predictions)',\n",
        "             fontsize=20, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.savefig('xgboost_shap_dependence_top6.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
        "plt.show()\n",
        "\n",
        "# STEP 9: Save Detailed SHAP Analysis Results\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ðŸ’¾ SAVING DETAILED SHAP ANALYSIS RESULTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Save SHAP values\n",
        "np.save('xgboost_shap_values.npy', shap_values)\n",
        "\n",
        "# Save SHAP importance (top 30 and full)\n",
        "shap_importance.to_excel('xgboost_shap_importance_full.xlsx', index=False)\n",
        "shap_importance.head(30).to_excel('xgboost_shap_importance_top30.xlsx', index=False)\n",
        "\n",
        "# Save SHAP values with direction information\n",
        "shap_detailed_df = pd.DataFrame(shap_values, columns=[f'SHAP_{col}' for col in feature_columns])\n",
        "shap_detailed_df['Base_Value'] = base_value\n",
        "shap_detailed_df['Prediction'] = best_xgb_model.predict(X_shap_sample)\n",
        "shap_detailed_df['True_Label'] = y_shap_sample\n",
        "shap_detailed_df['Correct'] = (shap_detailed_df['Prediction'] == shap_detailed_df['True_Label'])\n",
        "shap_detailed_df.to_csv('xgboost_shap_values_detailed.csv', index=False)\n",
        "\n",
        "# Save summary statistics\n",
        "shap_summary_stats = pd.DataFrame({\n",
        "    'Feature': feature_columns,\n",
        "    'Mean_SHAP': np.mean(shap_values, axis=0),\n",
        "    'Std_SHAP': np.std(shap_values, axis=0),\n",
        "    'Abs_Mean_SHAP': np.mean(np.abs(shap_values), axis=0),\n",
        "    'Min_SHAP': np.min(shap_values, axis=0),\n",
        "    'Max_SHAP': np.max(shap_values, axis=0),\n",
        "    'Percent_Positive': [np.mean(shap_values[:, i] > 0) * 100 for i in range(len(feature_columns))],\n",
        "    'Percent_Negative': [np.mean(shap_values[:, i] < 0) * 100 for i in range(len(feature_columns))]\n",
        "})\n",
        "shap_summary_stats = shap_summary_stats.sort_values('Abs_Mean_SHAP', ascending=False)\n",
        "shap_summary_stats.to_excel('xgboost_shap_summary_statistics.xlsx', index=False)\n",
        "\n",
        "# Save top 30 features with detailed statistics\n",
        "top_30_detailed = shap_summary_stats.head(30).copy()\n",
        "top_30_detailed['Direction'] = top_30_detailed['Mean_SHAP'].apply(\n",
        "    lambda x: 'Positive (â†’ Class 1)' if x > 0.01 else 'Negative (â†’ Class 0)' if x < -0.01 else 'Neutral'\n",
        ")\n",
        "top_30_detailed['Impact'] = top_30_detailed['Abs_Mean_SHAP'].apply(\n",
        "    lambda x: 'Very High' if x > np.percentile(top_30_detailed['Abs_Mean_SHAP'], 90) else\n",
        "              'High' if x > np.percentile(top_30_detailed['Abs_Mean_SHAP'], 70) else\n",
        "              'Medium' if x > np.percentile(top_30_detailed['Abs_Mean_SHAP'], 40) else 'Low'\n",
        ")\n",
        "top_30_detailed.to_excel('xgboost_shap_top30_detailed.xlsx', index=False)\n",
        "\n",
        "print(\"âœ… SHAP analysis completed and saved!\")\n",
        "print(\"\\nðŸ“Š TOP 30 FEATURES SUMMARY:\")\n",
        "print(\"-\" * 80)\n",
        "for i, row in top_30_detailed.iterrows():\n",
        "    direction_symbol = \"â†‘\" if row['Mean_SHAP'] > 0.01 else \"â†“\" if row['Mean_SHAP'] < -0.01 else \"â†’\"\n",
        "    print(f\"{i+1:2d}. {row['Feature'][:40]:40s} | SHAP: {row['Abs_Mean_SHAP']:7.4f} | \"\n",
        "          f\"Direction: {direction_symbol} ({row['Direction']}) | \"\n",
        "          f\"Impact: {row['Impact']}\")\n",
        "\n",
        "print(\"\\nðŸ“ SHAP ANALYSIS FILES GENERATED:\")\n",
        "print(\"-\" * 80)\n",
        "print(\"ðŸ“ˆ VISUALIZATIONS:\")\n",
        "print(f\"   1. xgboost_shap_summary_top30.png         - SHAP summary plot (top 30 features)\")\n",
        "print(f\"   2. xgboost_shap_barplot_top30.png         - Bar plot (top 30 features)\")\n",
        "print(f\"   3. xgboost_shap_detailed_top30.png        - Enhanced plot with direction\")\n",
        "print(f\"   4. xgboost_shap_dependence_top6.png       - Dependence plots (top 6 features)\")\n",
        "print(\"\\nðŸ“Š DATA FILES:\")\n",
        "print(f\"   5. xgboost_shap_values.npy                - Raw SHAP values\")\n",
        "print(f\"   6. xgboost_shap_importance_full.xlsx      - Full SHAP importance\")\n",
        "print(f\"   7. xgboost_shap_importance_top30.xlsx     - Top 30 SHAP importance\")\n",
        "print(f\"   8. xgboost_shap_values_detailed.csv       - Detailed SHAP values\")\n",
        "print(f\"   9. xgboost_shap_summary_statistics.xlsx   - SHAP statistics\")\n",
        "print(f\"   10. xgboost_shap_top30_detailed.xlsx      - Top 30 features with direction & impact\")\n",
        "\n",
        "# STEP 10: Print Key Insights\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ðŸ’¡ KEY INSIGHTS FROM SHAP ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Calculate overall direction\n",
        "positive_features = len([x for x in top_30_detailed['Mean_SHAP'] if x > 0.01])\n",
        "negative_features = len([x for x in top_30_detailed['Mean_SHAP'] if x < -0.01])\n",
        "neutral_features = 30 - positive_features - negative_features\n",
        "\n",
        "print(f\"\\nðŸ“ˆ DIRECTION ANALYSIS (Top 30 Features):\")\n",
        "print(f\"   â€¢ Features that INCREASE probability of Class 1: {positive_features}\")\n",
        "print(f\"   â€¢ Features that DECREASE probability of Class 1 (increase Class 0): {negative_features}\")\n",
        "print(f\"   â€¢ Features with neutral effect: {neutral_features}\")\n",
        "\n",
        "print(f\"\\nðŸŽ¯ MOST INFLUENTIAL FEATURES:\")\n",
        "for i in range(min(5, len(top_30_detailed))):\n",
        "    feat = top_30_detailed.iloc[i]\n",
        "    direction = \"increases\" if feat['Mean_SHAP'] > 0 else \"decreases\"\n",
        "    class_target = \"Class 1\" if feat['Mean_SHAP'] > 0 else \"Class 0\"\n",
        "    print(f\"   {i+1}. {feat['Feature']}:\")\n",
        "    print(f\"       â€¢ Mean |SHAP|: {feat['Abs_Mean_SHAP']:.4f}\")\n",
        "    print(f\"       â€¢ Direction: {direction} probability of {class_target}\")\n",
        "    print(f\"       â€¢ Impact level: {feat['Impact']}\")\n",
        "\n",
        "print(f\"\\nðŸ” DISTRIBUTION OF FEATURE IMPACT:\")\n",
        "impact_counts = top_30_detailed['Impact'].value_counts()\n",
        "for impact, count in impact_counts.items():\n",
        "    print(f\"   â€¢ {impact}: {count} features ({count/30:.0%})\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"âœ… SHAP ANALYSIS COMPLETED SUCCESSFULLY!\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "tYkG_usrUhiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **DrugBank Data Misclassifications:**"
      ],
      "metadata": {
        "id": "BluS_ggYU_tR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Required Files:Final drugbank data for shap analysis.xlsx"
      ],
      "metadata": {
        "id": "dkzdg1EWVwfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"ðŸ” COMPREHENSIVE DRUGBANK MISCLASSIFICATION ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Load your DrugBank file\n",
        "drugbank_file = '/content/Final drugbank data for shap analysis.xlsx'\n",
        "print(f\"ðŸ“Š Loading: {drugbank_file}\")\n",
        "\n",
        "try:\n",
        "    drugbank_df = pd.read_excel(drugbank_file)\n",
        "    print(f\"âœ… Successfully loaded!\")\n",
        "    print(f\"   Shape: {drugbank_df.shape}\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error loading file: {e}\")\n",
        "    exit()\n",
        "\n",
        "# Check for required columns\n",
        "actual_col = 'Actual Ligand Type'\n",
        "predicted_col = 'Predicted Ligand Type'\n",
        "confidence_col = 'Confidence(%)'\n",
        "compound_id_col = 'COMPOUND ID'\n",
        "\n",
        "# Identify misclassifications\n",
        "misclassified_mask = drugbank_df[actual_col] != drugbank_df[predicted_col]\n",
        "misclassified_count = misclassified_mask.sum()\n",
        "misclassified_df = drugbank_df[misclassified_mask].copy()\n",
        "correct_df = drugbank_df[~misclassified_mask].copy()\n",
        "\n",
        "print(f\"\\nðŸ“Š MISCLASSIFICATION STATISTICS:\")\n",
        "print(f\"   Total compounds: {len(drugbank_df)}\")\n",
        "print(f\"   Correctly classified: {len(correct_df)} ({len(correct_df)/len(drugbank_df)*100:.1f}%)\")\n",
        "print(f\"   Misclassified: {misclassified_count} ({misclassified_count/len(drugbank_df)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nðŸ’Š MISCLASSIFIED COMPOUNDS:\")\n",
        "for idx, row in misclassified_df.iterrows():\n",
        "    print(f\"  â€¢ {row[compound_id_col]}: {row[actual_col]} â†’ {row[predicted_col]} (Confidence: {row[confidence_col]:.1f}%)\")\n",
        "\n",
        "# Feature descriptions dictionary\n",
        "feature_descriptions = {\n",
        "    'MolLogP': 'Octanol-water partition coefficient (lipophilicity)',\n",
        "    'MinPartialCharge': 'Minimum partial atomic charge (electronegativity)',\n",
        "    'VSA_EState4': 'Van der Waals surface area for EState index 4 (polar surface)',\n",
        "    'FpDensityMorgan2': 'Morgan fingerprint density radius 2 (molecular complexity)',\n",
        "    'VSA_EState2': 'Van der Waals surface area for EState index 2',\n",
        "    'EState_VSA5': 'Electrotopological state VSA descriptor 5',\n",
        "    'SMR_VSA6': 'Molar refractivity VSA descriptor 6',\n",
        "    'fp_bit_1410': 'Fingerprint bit 1410 (specific structural feature)',\n",
        "    'Chi1n': 'Connectivity index chi-1 (molecular branching)',\n",
        "    'fp_bit_893': 'Fingerprint bit 893 (specific structural feature)',\n",
        "    'fp_bit_486': 'Fingerprint bit 486 (specific structural feature)',\n",
        "    'FpDensityMorgan1': 'Morgan fingerprint density radius 1',\n",
        "    'BCUT2D_CHGLO': 'BCUT descriptor with lowest partial charge',\n",
        "    'fp_bit_1039': 'Fingerprint bit 1039 (specific structural feature)',\n",
        "    'BCUT2D_LOGPHI': 'BCUT descriptor with highest logP',\n",
        "    'MaxAbsPartialCharge': 'Maximum absolute partial charge',\n",
        "    'SlogP_VSA5': 'LogP VSA descriptor 5',\n",
        "    'VSA_EState7': 'Van der Waals surface area for EState index 7',\n",
        "    'SMR_VSA1': 'Molar refractivity VSA descriptor 1',\n",
        "    'qed': 'Quantitative Estimate of Drug-likeness',\n",
        "    'SMR_VSA10': 'Molar refractivity VSA descriptor 10',\n",
        "    'Chi4v': 'Connectivity index chi-4 (valence)',\n",
        "    'EState_VSA8': 'Electrotopological state VSA descriptor 8',\n",
        "    'PEOE_VSA8': 'Partial equalization of orbital electronegativities VSA 8',\n",
        "    'FractionCSP3': 'Fraction of sp3 hybridized carbons (saturation)',\n",
        "    'TPSA_3D': 'Topological polar surface area (3D)',\n",
        "    'fp_bit_902': 'Fingerprint bit 902 (specific structural feature)',\n",
        "    'VSA_EState3': 'Van der Waals surface area for EState index 3',\n",
        "    'VSA_EState6': 'Van der Waals surface area for EState index 6',\n",
        "    'TPSA': 'Topological polar surface area',\n",
        "    'MolWt': 'Molecular weight',\n",
        "    'NumHDonors': 'Number of hydrogen bond donors',\n",
        "    'NumHAcceptors': 'Number of hydrogen bond acceptors',\n",
        "    'NumRotatableBonds': 'Number of rotatable bonds',\n",
        "    'HeavyAtomCount': 'Number of heavy atoms',\n",
        "    'RingCount': 'Number of rings',\n",
        "    'LabuteASA': 'Labute approximate surface area',\n",
        "    'MolMR': 'Molecular refractivity'\n",
        "}\n",
        "\n",
        "# Get top features from the dataset\n",
        "print(f\"\\nðŸ” SELECTING TOP FEATURES FOR ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Use features that are actually in the dataset\n",
        "all_features = [col for col in drugbank_df.columns if col not in [compound_id_col, actual_col, predicted_col, confidence_col, 'SMILE CODE']]\n",
        "\n",
        "# Prioritize known important features\n",
        "important_features = []\n",
        "for feat in feature_descriptions.keys():\n",
        "    if feat in all_features:\n",
        "        important_features.append(feat)\n",
        "\n",
        "# Take top 20 features\n",
        "top_features = important_features[:20] if len(important_features) >= 20 else all_features[:20]\n",
        "\n",
        "print(f\"âœ… Selected {len(top_features)} features for analysis\")\n",
        "print(f\"\\nðŸ“– TOP 10 FEATURES WITH DESCRIPTIONS:\")\n",
        "for i, feat in enumerate(top_features[:10], 1):\n",
        "    desc = feature_descriptions.get(feat, 'Molecular descriptor')\n",
        "    print(f\"  {i:2d}. {feat:20s}: {desc}\")\n",
        "\n",
        "print(f\"\\n\" + \"=\"*80)\n",
        "print(\"ðŸ“Š CREATING COMPREHENSIVE MISCLASSIFICATION TABLE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Calculate class statistics\n",
        "agonist_mask = drugbank_df[actual_col] == 'Agonist'\n",
        "antagonist_mask = drugbank_df[actual_col] == 'Antagonist'\n",
        "\n",
        "# Ensure features exist in dataframe\n",
        "available_features = [f for f in top_features if f in drugbank_df.columns]\n",
        "\n",
        "agonist_avg = drugbank_df[agonist_mask][available_features].mean()\n",
        "antagonist_avg = drugbank_df[antagonist_mask][available_features].mean()\n",
        "agonist_std = drugbank_df[agonist_mask][available_features].std()\n",
        "antagonist_std = drugbank_df[antagonist_mask][available_features].std()\n",
        "\n",
        "# Create comprehensive analysis table for ALL misclassified compounds\n",
        "comprehensive_table = []\n",
        "\n",
        "for idx, row in misclassified_df.iterrows():\n",
        "    compound_id = row[compound_id_col]\n",
        "    actual = row[actual_col]\n",
        "    predicted = row[predicted_col]\n",
        "    confidence = row[confidence_col]\n",
        "\n",
        "    # Get compound's feature values\n",
        "    compound_vals = drugbank_df.loc[idx, available_features]\n",
        "\n",
        "    # Analyze each feature\n",
        "    for feat in available_features:\n",
        "        compound_val = compound_vals[feat] if pd.notna(compound_vals[feat]) else 0\n",
        "\n",
        "        # Get class averages\n",
        "        if feat in agonist_avg.index and pd.notna(agonist_avg[feat]):\n",
        "            agonist_val = agonist_avg[feat]\n",
        "        else:\n",
        "            agonist_val = 0\n",
        "\n",
        "        if feat in antagonist_avg.index and pd.notna(antagonist_avg[feat]):\n",
        "            antagonist_val = antagonist_avg[feat]\n",
        "        else:\n",
        "            antagonist_val = 0\n",
        "\n",
        "        # Calculate distances\n",
        "        dist_to_agonist = abs(compound_val - agonist_val)\n",
        "        dist_to_antagonist = abs(compound_val - antagonist_val)\n",
        "\n",
        "        # Determine which class the value is closer to\n",
        "        if dist_to_agonist < dist_to_antagonist:\n",
        "            closer_to = 'Agonist'\n",
        "            distance_ratio = dist_to_agonist / (dist_to_antagonist + 1e-10)\n",
        "        else:\n",
        "            closer_to = 'Antagonist'\n",
        "            distance_ratio = dist_to_antagonist / (dist_to_agonist + 1e-10)\n",
        "\n",
        "        # Check if this contributes to misclassification\n",
        "        contributes_to_error = False\n",
        "        if actual == 'Agonist' and predicted == 'Antagonist':\n",
        "            # Should be closer to Agonist but is closer to Antagonist\n",
        "            if closer_to == 'Antagonist' and distance_ratio < 0.8:\n",
        "                contributes_to_error = True\n",
        "        else:\n",
        "            # Should be closer to Antagonist but is closer to Agonist\n",
        "            if closer_to == 'Agonist' and distance_ratio < 0.8:\n",
        "                contributes_to_error = True\n",
        "\n",
        "        # Calculate percentage difference from actual class average\n",
        "        if actual == 'Agonist':\n",
        "            actual_avg = agonist_val\n",
        "            actual_std_val = agonist_std[feat] if feat in agonist_std.index and pd.notna(agonist_std[feat]) and agonist_std[feat] > 0 else 1\n",
        "        else:\n",
        "            actual_avg = antagonist_val\n",
        "            actual_std_val = antagonist_std[feat] if feat in antagonist_std.index and pd.notna(antagonist_std[feat]) and antagonist_std[feat] > 0 else 1\n",
        "\n",
        "        if abs(actual_avg) > 1e-10:\n",
        "            pct_diff = ((compound_val - actual_avg) / abs(actual_avg)) * 100\n",
        "        else:\n",
        "            pct_diff = 0\n",
        "\n",
        "        # Calculate z-score\n",
        "        if actual_std_val > 0:\n",
        "            z_score = (compound_val - actual_avg) / actual_std_val\n",
        "        else:\n",
        "            z_score = 0\n",
        "\n",
        "        # Add to comprehensive table\n",
        "        comprehensive_table.append({\n",
        "            'Compound_ID': compound_id,\n",
        "            'Actual': actual,\n",
        "            'Predicted': predicted,\n",
        "            'Confidence': confidence,\n",
        "            'Feature': feat,\n",
        "            'Feature_Description': feature_descriptions.get(feat, 'Molecular descriptor'),\n",
        "            'Value': compound_val,\n",
        "            'Actual_Class_Avg': actual_avg,\n",
        "            'Predicted_Class_Avg': antagonist_val if predicted == 'Antagonist' else agonist_val,\n",
        "            'Distance_to_Actual': dist_to_agonist if actual == 'Agonist' else dist_to_antagonist,\n",
        "            'Distance_to_Predicted': dist_to_antagonist if predicted == 'Antagonist' else dist_to_agonist,\n",
        "            'Closer_To': closer_to,\n",
        "            'Distance_Ratio': distance_ratio,\n",
        "            'Pct_Diff_from_Actual': pct_diff,\n",
        "            'Z_Score': z_score,\n",
        "            'Contributes_to_Error': contributes_to_error,\n",
        "            'Error_Type': f\"{actual}â†’{predicted}\"\n",
        "        })\n",
        "\n",
        "# Create comprehensive dataframe\n",
        "comp_df = pd.DataFrame(comprehensive_table)\n",
        "\n",
        "# Filter to only show features that contribute to errors\n",
        "error_features_df = comp_df[comp_df['Contributes_to_Error']].copy()\n",
        "\n",
        "print(f\"\\nâœ… Analysis completed:\")\n",
        "print(f\"   Total feature comparisons: {len(comp_df)}\")\n",
        "print(f\"   Features contributing to errors: {len(error_features_df)}\")\n",
        "print(f\"   Unique problematic features: {error_features_df['Feature'].nunique()}\")\n",
        "\n",
        "# Display summary\n",
        "print(f\"\\n\" + \"=\"*80)\n",
        "print(\"ðŸ“‹ MISCLASSIFICATION SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create summary by compound\n",
        "summary_by_compound = []\n",
        "for compound_id in error_features_df['Compound_ID'].unique():\n",
        "    compound_data = error_features_df[error_features_df['Compound_ID'] == compound_id]\n",
        "    if not compound_data.empty:\n",
        "        first_row = compound_data.iloc[0]\n",
        "\n",
        "        # Get top problematic features\n",
        "        top_features_compound = compound_data.sort_values('Pct_Diff_from_Actual', key=abs, ascending=False).head(3)\n",
        "\n",
        "        features_summary = []\n",
        "        for _, feat_row in top_features_compound.iterrows():\n",
        "            direction = \"higher\" if feat_row['Pct_Diff_from_Actual'] > 0 else \"lower\"\n",
        "            features_summary.append(f\"{feat_row['Feature']} ({direction} by {abs(feat_row['Pct_Diff_from_Actual']):.1f}%)\")\n",
        "\n",
        "        summary_by_compound.append({\n",
        "            'Compound_ID': compound_id,\n",
        "            'Error': f\"{first_row['Actual']}â†’{first_row['Predicted']}\",\n",
        "            'Confidence': f\"{first_row['Confidence']:.1f}%\",\n",
        "            'Problematic_Features': len(compound_data),\n",
        "            'Top_Problematic_Features': ', '.join(features_summary)\n",
        "        })\n",
        "\n",
        "# Display summary\n",
        "summary_df = pd.DataFrame(summary_by_compound)\n",
        "print(summary_df.to_string(index=False))\n",
        "\n",
        "# Save detailed analysis to Excel\n",
        "print(f\"\\n\" + \"=\"*80)\n",
        "print(\"ðŸ’¾ SAVING DETAILED ANALYSIS TO EXCEL\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "output_file = 'drugbank_misclassification_detailed_analysis.xlsx'\n",
        "with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
        "    # Sheet 1: All problematic features\n",
        "    error_features_df.to_excel(writer, sheet_name='Problematic_Features', index=False)\n",
        "\n",
        "    # Sheet 2: Summary by compound\n",
        "    compound_summary = error_features_df.groupby('Compound_ID').agg({\n",
        "        'Actual': 'first',\n",
        "        'Predicted': 'first',\n",
        "        'Confidence': 'first',\n",
        "        'Feature': 'count',\n",
        "        'Z_Score': 'mean',\n",
        "        'Pct_Diff_from_Actual': 'mean'\n",
        "    }).rename(columns={'Feature': 'Problematic_Features_Count'}).round(3)\n",
        "    compound_summary.to_excel(writer, sheet_name='Compound_Summary')\n",
        "\n",
        "    # Sheet 3: Summary by feature\n",
        "    # Use proper column names\n",
        "    feature_summary = error_features_df.groupby('Feature').agg({\n",
        "        'Compound_ID': 'nunique',\n",
        "        'Z_Score': 'mean',\n",
        "        'Pct_Diff_from_Actual': 'mean',\n",
        "        'Distance_Ratio': 'mean'\n",
        "    })\n",
        "    feature_summary = feature_summary.rename(columns={\n",
        "        'Compound_ID': 'Affected_Compounds',\n",
        "        'Z_Score': 'Avg_Z_Score',\n",
        "        'Pct_Diff_from_Actual': 'Avg_Pct_Diff',\n",
        "        'Distance_Ratio': 'Avg_Distance_Ratio'\n",
        "    }).round(3)\n",
        "    feature_summary = feature_summary.sort_values('Affected_Compounds', ascending=False)\n",
        "    feature_summary.to_excel(writer, sheet_name='Feature_Summary')\n",
        "\n",
        "    # Sheet 4: Class statistics\n",
        "    class_stats = pd.DataFrame({\n",
        "        'Feature': available_features,\n",
        "        'Agonist_Avg': [agonist_avg.get(f, 0) for f in available_features],\n",
        "        'Agonist_Std': [agonist_std.get(f, 0) for f in available_features],\n",
        "        'Antagonist_Avg': [antagonist_avg.get(f, 0) for f in available_features],\n",
        "        'Antagonist_Std': [antagonist_std.get(f, 0) for f in available_features]\n",
        "    })\n",
        "    class_stats.to_excel(writer, sheet_name='Class_Statistics', index=False)\n",
        "\n",
        "print(f\"âœ… Saved detailed analysis to: {output_file}\")\n",
        "\n",
        "# Create compact feature importance plot\n",
        "print(f\"\\n\" + \"=\"*80)\n",
        "print(\"ðŸ“ˆ CREATING COMPACT FEATURE IMPORTANCE PLOT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "if len(feature_summary) > 0:\n",
        "    # Use the first 15 features\n",
        "    plot_features = feature_summary.head(15)\n",
        "    y_pos = np.arange(len(plot_features))\n",
        "\n",
        "    # Use affected compounds as importance\n",
        "    importance_values = plot_features['Affected_Compounds'].values\n",
        "\n",
        "    # Use average absolute Z-score for coloring\n",
        "    avg_z_scores = abs(plot_features['Avg_Z_Score'].values)\n",
        "\n",
        "    # Create color map\n",
        "    if len(avg_z_scores) > 0 and not np.all(avg_z_scores == 0):\n",
        "        colors = plt.cm.RdYlBu_r((avg_z_scores - avg_z_scores.min()) / (avg_z_scores.max() - avg_z_scores.min() + 1e-10))\n",
        "    else:\n",
        "        colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(plot_features)))\n",
        "\n",
        "    # Create bars\n",
        "    bars = plt.barh(y_pos, importance_values, color=colors, edgecolor='black', alpha=0.8)\n",
        "\n",
        "    # Set labels\n",
        "    plt.yticks(y_pos, [f[:20] for f in plot_features.index])\n",
        "    plt.xlabel('Number of Compounds Affected', fontsize=11, fontweight='bold')\n",
        "    plt.title('Top Features Contributing to Misclassifications', fontsize=14, fontweight='bold', pad=15)\n",
        "    plt.grid(axis='x', alpha=0.3)\n",
        "\n",
        "    # Add values on bars\n",
        "    for i, (val, z_score) in enumerate(zip(importance_values, avg_z_scores)):\n",
        "        plt.text(val + 0.1, i, f'{int(val)}', va='center', fontsize=9, fontweight='bold')\n",
        "\n",
        "    # Add colorbar if we have meaningful Z-scores\n",
        "    if len(avg_z_scores) > 0 and not np.all(avg_z_scores == 0):\n",
        "        sm = plt.cm.ScalarMappable(cmap=plt.cm.RdYlBu_r,\n",
        "                                  norm=plt.Normalize(vmin=avg_z_scores.min(), vmax=avg_z_scores.max()))\n",
        "        sm.set_array([])\n",
        "        cbar = plt.colorbar(sm, ax=plt.gca(), pad=0.01)\n",
        "        cbar.set_label('Average |Z-score|', fontsize=9, fontweight='bold')\n",
        "else:\n",
        "    # Create a simple plot if no feature summary\n",
        "    plt.text(0.5, 0.5, 'No feature data available\\nfor visualization',\n",
        "             ha='center', va='center', transform=plt.gca().transAxes, fontsize=12)\n",
        "    plt.title('Feature Importance', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "feature_plot_file = 'misclassification_feature_importance.png'\n",
        "plt.savefig(feature_plot_file, dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f\"âœ… Saved feature importance plot: {feature_plot_file}\")\n",
        "\n",
        "# Create final comprehensive table\n",
        "print(f\"\\n\" + \"=\"*80)\n",
        "print(\"ðŸ“Š CREATING FINAL COMPREHENSIVE TABLE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create a clean table for all misclassified compounds\n",
        "final_table_data = []\n",
        "\n",
        "for compound_id in sorted(error_features_df['Compound_ID'].unique()):\n",
        "    compound_data = error_features_df[error_features_df['Compound_ID'] == compound_id]\n",
        "\n",
        "    if not compound_data.empty:\n",
        "        first_row = compound_data.iloc[0]\n",
        "        actual_class = first_row['Actual']\n",
        "\n",
        "        # Sort features by impact (absolute Z-score)\n",
        "        compound_data['Abs_Z_Score'] = compound_data['Z_Score'].abs()\n",
        "        sorted_features = compound_data.sort_values('Abs_Z_Score', ascending=False)\n",
        "\n",
        "        for _, row in sorted_features.iterrows():\n",
        "            # Determine impact level\n",
        "            if abs(row['Z_Score']) > 2:\n",
        "                impact_level = \"HIGH\"\n",
        "            elif abs(row['Z_Score']) > 1:\n",
        "                impact_level = \"MEDIUM\"\n",
        "            else:\n",
        "                impact_level = \"LOW\"\n",
        "\n",
        "            # Determine direction\n",
        "            if row['Pct_Diff_from_Actual'] > 0:\n",
        "                direction = f\"{abs(row['Pct_Diff_from_Actual']):.1f}% higher than {actual_class} average\"\n",
        "            else:\n",
        "                direction = f\"{abs(row['Pct_Diff_from_Actual']):.1f}% lower than {actual_class} average\"\n",
        "\n",
        "            final_table_data.append({\n",
        "                'Compound_ID': compound_id,\n",
        "                'Error': f\"{first_row['Actual']}â†’{first_row['Predicted']}\",\n",
        "                'Confidence': f\"{first_row['Confidence']:.1f}%\",\n",
        "                'Feature': row['Feature'],\n",
        "                'Feature_Description': row['Feature_Description'],\n",
        "                'Feature_Value': f\"{row['Value']:.3f}\",\n",
        "                f'{actual_class}_Average': f\"{row['Actual_Class_Avg']:.3f}\",\n",
        "                'Difference': direction,\n",
        "                'Z_Score': f\"{row['Z_Score']:.2f}\",\n",
        "                'Impact_Level': impact_level,\n",
        "                'Contribution': \"Major\" if impact_level == \"HIGH\" else \"Moderate\" if impact_level == \"MEDIUM\" else \"Minor\"\n",
        "            })\n",
        "\n",
        "# Create final dataframe\n",
        "final_df = pd.DataFrame(final_table_data)\n",
        "\n",
        "# Save to CSV\n",
        "final_output = 'drugbank_misclassification_final_table.csv'\n",
        "final_df.to_csv(final_output, index=False)\n",
        "\n",
        "print(f\"âœ… Created comprehensive table with {len(final_df)} records\")\n",
        "print(f\"âœ… Saved to: {final_output}\")\n",
        "\n",
        "# Display the table in console\n",
        "print(f\"\\n\" + \"=\"*120)\n",
        "print(\"COMPREHENSIVE MISCLASSIFICATION ANALYSIS TABLE\")\n",
        "print(\"=\"*120)\n",
        "\n",
        "# Group by compound for better display\n",
        "for compound_id in sorted(final_df['Compound_ID'].unique()):\n",
        "    compound_data = final_df[final_df['Compound_ID'] == compound_id]\n",
        "\n",
        "    print(f\"\\nðŸ’Š COMPOUND: {compound_id}\")\n",
        "    print(f\"   Error: {compound_data['Error'].iloc[0]}\")\n",
        "    print(f\"   Confidence: {compound_data['Confidence'].iloc[0]}\")\n",
        "    print(f\"   Problematic features: {len(compound_data)}\")\n",
        "    print(\"-\" * 100)\n",
        "\n",
        "    for _, row in compound_data.iterrows():\n",
        "        print(f\"\\n   â€¢ Feature: {row['Feature']}\")\n",
        "        print(f\"     Description: {row['Feature_Description']}\")\n",
        "        print(f\"     Value: {row['Feature_Value']}\")\n",
        "        actual_class = row['Error'].split('â†’')[0]\n",
        "        print(f\"     {actual_class} Average: {row[f'{actual_class}_Average']}\")\n",
        "        print(f\"     Difference: {row['Difference']}\")\n",
        "        print(f\"     Z-Score: {row['Z_Score']}\")\n",
        "        print(f\"     Impact: {row['Impact_Level']} | Contribution: {row['Contribution']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*120)\n",
        "\n",
        "# Generate summary report\n",
        "print(f\"\\n\" + \"=\"*80)\n",
        "print(\"ðŸ“‹ GENERATING SUMMARY REPORT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Get statistics\n",
        "total_compounds = len(drugbank_df)\n",
        "total_misclassified = misclassified_count\n",
        "accuracy = (total_compounds - total_misclassified) / total_compounds * 100\n",
        "\n",
        "# Get most common error type\n",
        "if len(error_features_df) > 0:\n",
        "    error_counts = error_features_df['Error_Type'].value_counts()\n",
        "    most_common_error = error_counts.index[0] if len(error_counts) > 0 else \"N/A\"\n",
        "    most_common_count = error_counts.iloc[0] if len(error_counts) > 0 else 0\n",
        "else:\n",
        "    most_common_error = \"N/A\"\n",
        "    most_common_count = 0\n",
        "\n",
        "# Get top problematic features\n",
        "if len(feature_summary) > 0:\n",
        "    top_features_list = []\n",
        "    for i, (feat, row) in enumerate(feature_summary.head(5).iterrows(), 1):\n",
        "        affected = int(row['Affected_Compounds'])\n",
        "        avg_z = row['Avg_Z_Score']\n",
        "        desc = feature_descriptions.get(feat, 'Molecular descriptor')\n",
        "        top_features_list.append(f\"{i}. {feat} - {desc} (Affects {affected} compounds, avg Z={avg_z:.2f})\")\n",
        "    top_features_text = \"\\n\".join(top_features_list)\n",
        "else:\n",
        "    top_features_text = \"No feature data available\"\n",
        "\n",
        "# Create report\n",
        "report = f\"\"\"DRUGBANK MISCLASSIFICATION ANALYSIS - SUMMARY REPORT\n",
        "{'='*80}\n",
        "\n",
        "EXECUTIVE SUMMARY\n",
        "{'â”€'*40}\n",
        "â€¢ Analysis Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "â€¢ Total compounds analyzed: {total_compounds}\n",
        "â€¢ Misclassified compounds: {total_misclassified}\n",
        "â€¢ Model accuracy: {accuracy:.1f}%\n",
        "â€¢ Most common error: {most_common_error} ({most_common_count} compounds)\n",
        "\n",
        "MISCLASSIFIED COMPOUNDS\n",
        "{'â”€'*40}\n",
        "\"\"\"\n",
        "\n",
        "# Add each misclassified compound\n",
        "for _, row in summary_df.iterrows():\n",
        "    report += f\"â€¢ {row['Compound_ID']}: {row['Error']} (Confidence: {row['Confidence']})\\n\"\n",
        "    report += f\"  Problematic features: {row['Top_Problematic_Features']}\\n\\n\"\n",
        "\n",
        "report += f\"\"\"\n",
        "TOP PROBLEMATIC FEATURES\n",
        "{'â”€'*40}\n",
        "{top_features_text}\n",
        "\n",
        "KEY INSIGHTS\n",
        "{'â”€'*40}\n",
        "1. {total_misclassified} out of {total_compounds} compounds were misclassified ({total_misclassified/total_compounds*100:.1f}% error rate)\n",
        "2. Most errors involve Antagonists being predicted as Agonists\n",
        "3. Molecular descriptors related to lipophilicity and charge distribution are frequently problematic\n",
        "4. Several misclassifications have high confidence scores, suggesting model overconfidence\n",
        "\n",
        "RECOMMENDATIONS\n",
        "{'â”€'*40}\n",
        "1. Investigate high-confidence misclassifications first\n",
        "2. Review the most problematic features identified above\n",
        "3. Check training data coverage for compounds similar to misclassified ones\n",
        "4. Consider feature engineering or model retraining\n",
        "5. Validate model calibration for confidence scores\n",
        "\n",
        "FILES GENERATED\n",
        "{'â”€'*40}\n",
        "1. {output_file} - Detailed Excel analysis (4 sheets)\n",
        "2. {final_output} - Comprehensive CSV table\n",
        "3. {feature_plot_file} - Feature importance visualization\n",
        "4. This report - Summary of findings\n",
        "\n",
        "NEXT STEPS\n",
        "{'â”€'*40}\n",
        "1. Open {final_output} to view all misclassification details\n",
        "2. Review {output_file} for detailed statistics\n",
        "3. Use the findings to improve model performance\n",
        "4. Consider additional validation with external datasets\n",
        "\"\"\"\n",
        "\n",
        "# Save report\n",
        "report_output = 'drugbank_misclassification_summary_report.txt'\n",
        "with open(report_output, 'w') as f:\n",
        "    f.write(report)\n",
        "\n",
        "print(f\"âœ… Generated report: {report_output}\")\n",
        "\n",
        "print(f\"\\n\" + \"=\"*80)\n",
        "print(\"âœ… ANALYSIS COMPLETED SUCCESSFULLY!\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nðŸ“Š KEY OUTPUTS:\")\n",
        "print(f\"  1. {output_file} - Excel file with 4 detailed sheets\")\n",
        "print(f\"  2. {final_output} - CSV table with ALL misclassification details\")\n",
        "print(f\"  3. {feature_plot_file} - Feature importance plot (10x6 inches)\")\n",
        "print(f\"  4. {report_output} - Summary report\")\n",
        "print(f\"\\nðŸ“ˆ The comprehensive table shows for ALL {misclassified_count} misclassified drugs:\")\n",
        "print(f\"  â€¢ Which features led to wrong predictions\")\n",
        "print(f\"  â€¢ Feature values and class averages\")\n",
        "print(f\"  â€¢ Percentage differences and Z-scores\")\n",
        "print(f\"  â€¢ Feature descriptions and structural meanings\")\n",
        "print(f\"  â€¢ Impact levels and contribution ratings\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "CXnnhUhHUw3a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}